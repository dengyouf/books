{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Material for MkDocs","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"k8s/harbor/","title":"\u4f01\u4e1a\u7ea7Harbor\u955c\u50cf\u4ed3\u5e93","text":""},{"location":"k8s/harbor/#_1","title":"\u4e3b\u673a\u73af\u5883\u51c6\u5907","text":"<p>\u4f7f\u7528Docker\u5feb\u901f\u90e8\u7f72Harbor\u955c\u50cf\u4ed3\u5e93\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3aUbuntu 24.04.2 LTS(noble)\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b:</p>"},{"location":"k8s/harbor/#1","title":"1.\u4e3b\u673a\u540d\u89e3\u6790","text":"<pre><code>hostnamectl  set-hostname harbor.devops.io\necho \"192.168.1.250 harbor.devops.io harbor\" &gt;&gt; /etc/hosts\n</code></pre>"},{"location":"k8s/harbor/#2selinux","title":"2.\u5173\u95ed\u9632\u706b\u5899\u548cselinux","text":"<pre><code>systemctl disable --now ufw\n</code></pre>"},{"location":"k8s/harbor/#3","title":"3.\u65f6\u95f4\u540c\u6b65","text":"<pre><code>timedatectl  set-timezone Asia/Shanghai\napt install -y chrony\ncat  &gt; /etc/chrony/chrony.conf &lt;&lt; 'EOF'\npool ntp.aliyun.com       iburst maxsources 4\nkeyfile /etc/chrony/chrony.keys\ndriftfile /var/lib/chrony/chrony.drift\nlogdir /var/log/chrony\nmaxupdateskew 100.0\nrtcsync\nmakestep 1 3\nEOF\nsystemctl restart chrony.service &amp;&amp; systemctl  enable chrony.service\nchronyc sources\n</code></pre>"},{"location":"k8s/harbor/#4","title":"4.\u5f00\u542f\u5185\u6838\u8f6c\u53d1","text":"<pre><code>cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf &lt;&lt;EOF\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl  --system\n</code></pre>"},{"location":"k8s/harbor/#docker","title":"\u5b89\u88c5docker","text":""},{"location":"k8s/harbor/#1docker","title":"1.\u5b89\u88c5docker","text":"<pre><code>sudo apt-get remove docker docker-engine docker.io\nsudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL https://mirrors.huaweicloud.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\nsudo add-apt-repository \"deb [arch=amd64] https://mirrors.huaweicloud.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\"\nsudo apt-get update\nsudo apt-get install -y docker-ce\n docker version\nClient: Docker Engine - Community\n Version:           28.5.1\n API version:       1.51\n Go version:        go1.24.8\n Git commit:        e180ab8\n Built:             Wed Oct  8 12:17:26 2025\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          28.5.1\n  API version:      1.51 (minimum version 1.24)\n  Go version:       go1.24.8\n  Git commit:       f8215cc\n  Built:            Wed Oct  8 12:17:26 2025\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          v1.7.28\n  GitCommit:        b98a3aace656320842a23f4a392a33f46af97866\n runc:\n  Version:          1.3.0\n  GitCommit:        v1.3.0-0-g4ca628d1\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre>"},{"location":"k8s/harbor/#2docker","title":"2.\u914d\u7f6edocker\u6570\u636e\u76ee\u5f55","text":"<pre><code>systemctl stop docker docker.socket\nmkdir /data\nmv /var/lib/docker /data/\nln -sv /data/docker /var/lib/docker\nsystemctl  enable docker --now\n</code></pre>"},{"location":"k8s/harbor/#3_1","title":"3.\u914d\u7f6e\u52a0\u901f\u5668","text":"<pre><code>tee  /etc/docker/daemon.json &lt;&lt; 'EOF'\n{\n    \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n    \"registry-mirrors\": [\n         \"https://o4uba187.mirror.aliyuncs.com\",\n         \"https://docker.1ms.run\",\n         \"https://docker.1panel.live\"\n    ]\n}\nEOF\nsystemctl daemon-reload\n</code></pre>"},{"location":"k8s/harbor/#harbor_1","title":"\u5b89\u88c5Harbor","text":""},{"location":"k8s/harbor/#1_1","title":"1.\u83b7\u53d6\u79bb\u7ebf\u5305","text":"<pre><code>wget https://ghfast.top/https://github.com/goharbor/harbor/releases/download/v2.14.0/harbor-offline-installer-v2.14.0.tgz\ntar -xf harbor-offline-installer-v2.14.0.tgz  -C /data/\n</code></pre>"},{"location":"k8s/harbor/#2","title":"2.\u751f\u6210\u8bc1\u4e66","text":"<pre><code>mkdir /data/harbor/ssl\ncd /data/harbor/ssl\n# 1. Generate a CA certificate private key.\nopenssl genrsa -out ca.key 4096\n# 2. Generate the CA certificate.\nopenssl req -x509 -new -nodes -sha512 -days 3650 \\\n -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.devops.io\" \\\n -key ca.key \\\n -out ca.crt\n# 3. Generate a private key for harbor server.\nopenssl genrsa -out harbor.devops.io.key 4096\n# 4. Generate a certificate signing request (CSR).\nopenssl req -sha512 -new \\\n    -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.devops.io\" \\\n    -key harbor.devops.io.key \\\n    -out harbor.devops.io.csr\n# 5. Generate an x509 v3 extension file.\ncat &gt; v3.ext &lt;&lt;-EOF\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1=harbor.devops.io\nDNS.2=harbor.devops.io\nDNS.3=192.168.1.250\nEOF\n# 6. Use the v3.ext file to generate a certificate for your Harbor host.\nopenssl x509 -req -sha512 -days 3650 \\\n    -extfile v3.ext \\\n    -CA ca.crt -CAkey ca.key -CAcreateserial \\\n    -in harbor.devops.io.csr \\\n    -out harbor.devops.io.crt\nCertificate request self-signature ok\nsubject=C = CN, ST = Beijing, L = Beijing, O = example, OU = Personal, CN = harbor.devops.io\n</code></pre>"},{"location":"k8s/harbor/#3harboryml","title":"3.\u914d\u7f6eharbor.yml","text":"<pre><code>cd /data/harbor/\ncp harbor.yml.tmpl harbor.yml\nvim /data/harbor/harbor.yml\nhostname: harbor.devops.io\n\n# http related config\nhttp:\n  # port for http, default is 80. If https enabled, this port will redirect to https port\n  port: 80\n\n# https related config\nhttps:\n  # https port for harbor, default is 443\n  port: 443\n  # The path of cert and key files for nginx\n  certificate: /data/harbor/ssl/harbor.devops.io.crt\n  private_key: /data/harbor/ssl/harbor.devops.io.key\n</code></pre>"},{"location":"k8s/harbor/#4prepare","title":"4.\u6267\u884cprepare\u811a\u672c","text":"<pre><code>docker load -i /data/harbor/harbor.v2.14.0.tar.gz\ncd /data/harbor/ &amp;&amp; ./prepare\n</code></pre>"},{"location":"k8s/harbor/#5install","title":"5.\u6267\u884cinstall\u811a\u672c","text":"<pre><code>./install.sh\n</code></pre>"},{"location":"k8s/harbor/#6ui","title":"6.\u8bbf\u95eeUI","text":""},{"location":"k8s/harbor/#docker_1","title":"\u914d\u7f6edocker\u4f7f\u7528","text":""},{"location":"k8s/harbor/#1_2","title":"1.\u914d\u7f6e\u8ba4\u8bc1\u8bc1\u4e66","text":"<pre><code># \u8f6c\u6362\u8bc1\u4e66\ncd /data/harbor/ssl\nopenssl x509 -inform PEM -in harbor.devops.io.crt -out harbor.devops.io.cert\n\nmkdir -pv  /etc/docker/certs.d/harbor.devops.io/\ncp /data/harbor/ssl/{harbor.devops.io.cert,harbor.devops.io.key,ca.crt} /etc/docker/certs.d/harbor.devops.io/\n</code></pre>"},{"location":"k8s/harbor/#2_1","title":"2.\u4e0a\u4f20\u955c\u50cf","text":"<pre><code>docker pull ikubernetes/myapp:v1\ndocker tag  ikubernetes/myapp:v1 harbor.devops.io/library/myapp:v1\ndocker login harbor.devops.io -u admin -p Harbor12345\ndocker push harbor.devops.io/library/myapp:v1\n</code></pre>"},{"location":"k8s/harbor/#containerd","title":"\u914d\u7f6econtainerd\u4f7f\u7528","text":"<pre><code># \u914d\u7f6econfig_path\u8def\u5f84\ngrep -B 2 config_path /etc/containerd/config.toml\n    [plugins.\"io.containerd.grpc.v1.cri\".registry]\n      config_path = \"/etc/containerd/certs.d\"\nsystemctl restart containerd\nmkdir -pv /etc/containerd/certs.d/harbor.devops.io\nscp harbor.devops.io:/data/harbor/ssl/ca.crt /etc/containerd/certs.d/harbor.devops.io\ncat &gt; /etc/containerd/certs.d/harbor.devops.io/hosts.toml  &lt;&lt; 'EOF'\nserver = \"https://harbor.devops.io\"\n[host.\"https://harbor.devops.io\"]\n  capabilities = [\"pull\", \"resolve\",\"push\"]\n  skip_verify = false\n  ca = [\"ca.crt\"]\nEOF\n</code></pre>"},{"location":"k8s/harbor/#1podsecret","title":"1.\u914d\u7f6ePod\u62c9\u53bbSecret","text":"<pre><code>kubectl create secret docker-registry harbor-secret \\\n  --docker-server=harbor.devops.io \\\n  --docker-username=admin \\\n  --docker-password=Harbor12345 \\\n  --docker-email=harbro@devops.com \\\n  -n default\n\ncat myapp.yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: myapp\n  name: myapp\n  namespace: default\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: myapp\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      imagePullSecrets:\n        - name: harbor-secret\n      containers:\n      - image: harbor.devops.io/baseimages/myapp:v1\n        imagePullPolicy: IfNotPresent\n        name: myapp\n      restartPolicy: Always\n</code></pre>"},{"location":"k8s/helm/","title":"Helm","text":"<p>Helm \u662f Kubernetes \u7684\u5305\u7ba1\u7406\u5668\u3002\u5176\u901a\u8fc7Helm Charts\u5b9a\u4e49\u3001\u5b89\u88c5\u548c\u5347\u7ea7\u8d1f\u8f7d\u7684Kubernetes\u5e94\u7528\u7a0b\u5e8f\uff0cCharts\u662fKubernetes\u8d44\u6e90\u6e05\u5355\u7684\u6a21\u7248\u6587\u4ef6\uff0c\u901a\u8fc7Charts\u53ef\u4ee5\u7b80\u5316\u7ef4\u62a4\u5e94\u7528\u7a0b\u5e8f\u7684YAML\u8d44\u6e90\u6e05\u5355\u6587\u4ef6\u3002\u6b64\u5916\uff0cCharts\u652f\u6301\u7248\u672c\u7ba1\u7406\u3001\u5206\u4eab\u3001\u53d1\u5e03\u3002</p> <ul> <li>Helm: CLI\u5de5\u5177</li> <li>Charts\uff1a\u7ec4\u7ec7\u90e8\u7f72\u6e05\u5355\u7684\u6a21\u7248\u6587\u4ef6\u7ec4\u5408\uff0c\u9759\u6001\u6587\u4ef6</li> <li>Release: \u5e94\u7528Charts\u751f\u6210\u7684\u5bf9\u8c61\u8d44\u6e90\uff0c\u4f8b\u5982\u8fd0\u884c\u7684Deployment\uff0cSVC\u8d44\u6e90\u7b49</li> </ul>"},{"location":"k8s/helm/#helm_1","title":"Helm\u5b89\u88c5","text":"<pre><code>~# wget https://get.helm.sh/helm-v3.18.3-linux-amd64.tar.gz\n~# tar -xf helm-v3.18.3-linux-amd64.tar.gz\n~# cp linux-amd64/helm /usr/bin/\n~# helm version\nversion.BuildInfo{Version:\"v3.18.3\", GitCommit:\"6838ebcf265a3842d1433956e8a622e3290cf324\", GitTreeState:\"clean\", GoVersion:\"go1.24.4\"}\n</code></pre>"},{"location":"k8s/helm/#helm_2","title":"helm\u57fa\u7840\u4f7f\u7528","text":""},{"location":"k8s/helm/#1charts","title":"1.\u6dfb\u52a0charts\u4ed3\u5e93","text":"<pre><code>~# helm repo add stable http://mirror.azure.cn/kubernetes/charts/\n~# helm repo add bitnami https://charts.bitnami.com/bitnami\n~# helm repo ls\nNAME    URL\nstable  http://mirror.azure.cn/kubernetes/charts/\nbitnami https://charts.bitnami.com/bitnami\n</code></pre>"},{"location":"k8s/helm/#2charts","title":"2.\u641c\u7d22charts","text":"<pre><code>~# helm search repo bitnami\n</code></pre>"},{"location":"k8s/helm/#3chart","title":"3.\u5b89\u88c5Chart","text":"<p><pre><code>~# helm repo update\n~# helm install stable/mysql --generate-name --set persistence.enabled=false\n</code></pre> <pre><code>~# kubectl  get all -l release=mysql-1761113274\n~# helm list\n</code></pre></p>"},{"location":"k8s/helm/#4release","title":"4.\u5378\u8f7dRelease","text":"<pre><code>~# helm uninstall mysql-1761113274\n~# helm del mysql-1761113274 -n default\n</code></pre>"},{"location":"k8s/helm/#5","title":"5.\u67e5\u770b\u5305\u4fe1\u606f","text":"<pre><code>~# helm show chart stable/mysql\n~# helm show values stable/mysql\n~# helm show all stable/mysql\n</code></pre>"},{"location":"k8s/helm/#6release","title":"6.\u83b7\u53d6release\u7684\u90e8\u7f72\u6e05\u5355\u4fe1\u606f","text":"<pre><code>~# helm get manifest mmysql-1761113274\n</code></pre>"},{"location":"k8s/helm/#7","title":"7.\u5b9a\u5236\u5b89\u88c5","text":"<ul> <li>\u5b9a\u5236values.yaml</li> </ul> <pre><code>~# helm search repo stable/mysql\nNAME                CHART VERSION   APP VERSION DESCRIPTION\nstable/mysql        1.6.9           5.7.30      DEPRECATED - Fast, reliable, scalable, and easy...\nstable/mysqldump    2.6.2           2.4.1       DEPRECATED! - A Helm chart to help backup MySQL...\n# \u83b7\u53d6charts\n~# helm pull stable/mysql --version 1.6.9 --untar\n~# helm fetch stable/mysql --version 1.6.9 --untar\n~# cd mysql\n# \u5b9a\u5236values.yaml\n~# vim mysql/values.yaml\n...\npersistence:\n  enabled: false\n...\nnodeSelector:\n  kubernetes.io/hostname: k8s-worker02\n~/mysql# helm install mysql57 . -f mysql/values.yaml\n# \u5f3a\u5236\u66f4\u65b0,\u91cd\u65b0\u90e8\u7f72 Pod\n~/mysql# helm upgrade --install mysql57 . -f values.yaml --force --recreate-pods\n</code></pre> <ul> <li><code>--set\u5b9a\u5236\u5b89\u88c5</code></li> </ul> <pre><code>~# helm install stable/mysql --generate-name --set persistence.enabled=false --set nodeSelector.\"kubernetes\\.io/hostname\"=k8s-worker02\n</code></pre>"},{"location":"k8s/helm/#8","title":"8.\u56de\u6eda","text":"<pre><code>~# helm install mysql57 stable/mysql --set persistence.enabled=false\n~# helm upgrade mysql57 stable/mysql --set persistence.enabled=false --set imageTag=5.7.38\n~# helm history mysql57\nREVISION    UPDATED                     STATUS      CHART       APP VERSION DESCRIPTION\n1           Wed Oct 22 15:00:46 2025    superseded  mysql-1.6.9 5.7.30      Install complete\n2           Wed Oct 22 15:02:34 2025    deployed    mysql-1.6.9 5.7.30      Upgrade complete\n~# helm rollback mysql57 1\nRollback was a success! Happy Helming!\n</code></pre>"},{"location":"k8s/helm/#9","title":"9.\u6e32\u67d3\u8f93\u51fa\u6e05\u5355","text":"<pre><code>~# helm template mysql57 stable/mysql --set persistence.enabled=false --set imageTag=5.7.38\n</code></pre>"},{"location":"k8s/helm/#charts","title":"Charts\u89e3\u6790","text":"<p>Helm \u4f7f\u7528\u4e00\u79cd\u540d\u4e3acharts\u7684\u5305\u683c\u5f0f\u3002\u4e00\u4e2aCharts\u662f\u63cf\u8ff0\u4e00\u7ec4\u76f8\u5173\u7684Kubernets\u90e8\u7f72\u8d44\u6e90\u6e05\u5355\u6587\u4ef6\u96c6\u5408\u3002Charts\u662f\u521b\u5efa\u5728\u7279\u5b9a\u76ee\u5f55\u4e0b\u9762\u7684\u6587\u4ef6\u96c6\u5408\uff0c\u7136\u540e\u53ef\u4ee5\u5c06\u5b83\u4eec\u6253\u5305\u5230\u4e00\u4e2a\u7248\u672c\u5316\u7684\u5b58\u6863\u4e2d\u6765\u90e8\u7f72\u3002</p> <ul> <li>Go\u6a21\u7248\u51fd\u6570</li> </ul> <pre><code>~# tree -L 2 mysql\nmysql\n\u251c\u2500\u2500 Chart.yaml      # \u5305\u542b\u5f53\u524d chart \u4fe1\u606f\u7684 YAML \u6587\u4ef6\n\u251c\u2500\u2500 README.md       # \u53ef\u9009\uff1a\u4e00\u4e2a\u53ef\u8bfb\u6027\u9ad8\u7684 README \u6587\u4ef6\n\u251c\u2500\u2500 templates       # \u6a21\u677f\u76ee\u5f55\uff0c\u4e0e values \u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5c06\u6e32\u67d3\u751f\u6210 Kubernetes \u8d44\u6e90\u6e05\u5355\u6587\u4ef6\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 configurationFiles-configmap.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 deployment.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _helpers.tpl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 initializationFiles-configmap.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NOTES.txt   # \u53ef\u9009: \u5305\u542b\u7b80\u77ed\u4f7f\u7528\u4f7f\u7528\u7684\u6587\u672c\u6587\u4ef6\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pvc.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 secrets.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serviceaccount.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 servicemonitor.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 svc.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u2514\u2500\u2500 values.yaml\n</code></pre>"},{"location":"k8s/helm/#helm_3","title":"Helm\u5b9e\u8df5","text":""},{"location":"k8s/helm/#1-charts","title":"1. \u521d\u59cb\u5316charts\u6a21\u7248","text":"<pre><code>~# helm create myapp\n</code></pre>"},{"location":"k8s/helm/#2-valuesyaml","title":"2. \u5b9a\u5236values.yaml","text":"<pre><code>replicaCount: 3\nimage:\n  repository: harbor.devops.io/baseimages/myapp\n  pullPolicy: IfNotPresent\n  tag: \"v1\"\nimagePullSecrets:\n  - name: harbor-secret\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n</code></pre>"},{"location":"k8s/helm/#3","title":"3.\u6d4b\u8bd5\u5b89\u88c5","text":"<pre><code>~# helm upgrade --install  myapp myapp\n~# helm ls\nNAME    NAMESPACE   REVISION    UPDATED                                 STATUS      CHART       APP VERSION\nmyapp   default     1           2025-10-23 14:44:59.809403967 +0800 CST deployed    myapp-0.1.0 0.1.0\n</code></pre>"},{"location":"k8s/helm/#4","title":"4.\u5347\u7ea7\u5b89\u88c5","text":"<pre><code>~# helm upgrade --install  myapp myapp --set image.tag=\"v2\"\n</code></pre>"},{"location":"k8s/helm/#6","title":"6.\u56de\u6eda","text":"<pre><code>~# helm history myapp\nREVISION    UPDATED                     STATUS      CHART       APP VERSION DESCRIPTION\n1           Thu Oct 23 14:44:59 2025    superseded  myapp-0.1.0 0.1.0           Install complete\n2           Thu Oct 23 14:49:46 2025    deployed    myapp-0.1.0 0.1.0           Upgrade complete\n#\n~# helm rollback myapp 1\n</code></pre>"},{"location":"k8s/helm/#7_1","title":"7.\u6253\u5305","text":"<pre><code>~# helm package myapp --version 0.0.1\nSuccessfully packaged chart and saved it to: /root/myapp-0.0.1.tgz\n</code></pre>"},{"location":"k8s/helm/#8harbor","title":"8.\u63a8\u9001\u5230harbor\u4ed3\u5e93","text":"<pre><code>cp  /etc/containerd/certs.d/harbor.devops.io/ca.crt  /usr/local/share/ca-certificates/harbor.devops.io.crt\nupdate-ca-certificates\nexport HELM_EXPERIMENTAL_OCI=1\nhelm registry login harbor.devops.io -u admin -p Harbor12345\n~# helm push myapp-0.0.1.tgz  oci://harbor.devops.io/charts\n</code></pre>"},{"location":"k8s/helm/#9charts","title":"9.\u4f7f\u7528charts","text":"<pre><code>~# helm install myapp oci://harbor.devops.io/charts/myapp  --version 0.0.1\n~# helm pull oci://harbor.devops.io/charts/myapp --version 0.0.1 --untar\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/","title":"Kubeadm \u5b89\u88c5 Kubernetes v1.30.12(Containerd)","text":""},{"location":"k8s/kubeadm-v1.30.12/#_1","title":"\u4e3b\u673a\u73af\u5883\u51c6\u5907","text":"<p>\u4f7f\u7528Kubeadm\u5feb\u901f\u90e8\u7f72Kubernetes\u96c6\u7fa4\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3aUbuntu 24.04.2 LTS(noble)\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b:</p> <ul> <li>Kubernetes: v1.30.12</li> <li>Contianerd: v1.7.28</li> <li>Calico: v3.29.6</li> </ul> \u4e3b\u673a\u540d IP \u89d2\u8272 k8s-master01 192.168.1.111 master k8s-worker01 192.168.1.121 worker k8s-worker02 192.168.1.122 worker k8s-worker03 192.168.1.123 worker"},{"location":"k8s/kubeadm-v1.30.12/#1","title":"1.\u914d\u7f6e\u56fd\u5185\u6e90","text":"<ul> <li>Apt\u6e90\u7801</li> </ul> <pre><code>cat &gt; /etc/apt/sources.list.d/ubuntu.list &lt;&lt; 'EOF'\ndeb https://mirrors.aliyun.com/ubuntu/ noble main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ noble main restricted universe multiverse\n\ndeb https://mirrors.aliyun.com/ubuntu/ noble-security main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ noble-security main restricted universe multiverse\n\ndeb https://mirrors.aliyun.com/ubuntu/ noble-updates main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ noble-updates main restricted universe multiverse\n\n# deb https://mirrors.aliyun.com/ubuntu/ noble-proposed main restricted universe multiverse\n# deb-src https://mirrors.aliyun.com/ubuntu/ noble-proposed main restricted universe multiverse\n\ndeb https://mirrors.aliyun.com/ubuntu/ noble-backports main restricted universe multiverse\ndeb-src https://mirrors.aliyun.com/ubuntu/ noble-backports main restricted universe multiverse\nEOF\n</code></pre> <ul> <li>Docker\u6e90</li> </ul> <pre><code># step 1: \u5b89\u88c5\u5fc5\u8981\u7684\u4e00\u4e9b\u7cfb\u7edf\u5de5\u5177\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n\n# step 2: \u4fe1\u4efb Docker \u7684 GPG \u516c\u94a5\nsudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n\n# Step 3: \u5199\u5165\u8f6f\u4ef6\u6e90\u4fe1\u606f\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Step 4: \u5b89\u88c5Docker\nsudo apt-get update\n# \u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684Docker-CE:\n# Step 1: \u67e5\u627eDocker-CE\u7684\u7248\u672c:\n# apt-cache madison docker-ce\n#   docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages\n#   docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages\n# Step 2: \u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684Docker-CE: (VERSION\u4f8b\u5982\u4e0a\u9762\u768417.03.1~ce-0~ubuntu-xenial)\n# sudo apt-get -y install docker-ce=[VERSION]\n</code></pre> <ul> <li>Kubernetes\u6e90</li> </ul> <pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https\ncurl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key |\n    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /\" |\n    tee /etc/apt/sources.list.d/kubernetes.list\napt-get update\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2","title":"2.\u4e3b\u673a\u540d\u89e3\u6790","text":"<pre><code>cat &gt;&gt; /etc/hosts &lt;&lt; 'EOF'\n192.168.1.111 k8s-master01\n192.168.1.121 k8s-worker01\n192.168.1.122 k8s-worker02\n192.168.1.123 k8s-worker03\n# k8s-vip \u9884\u7559\u540e\u671f\u6269\u5c55\u9ad8\u53ef\u7528\u96c6\u7fa4\n192.168.1.111 k8s-vip\nEOF\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#3selinuxubuntuselinux","title":"3.\u5173\u95ed\u9632\u706b\u5899\u548cselinux(Ubuntu\u4e0d\u7528\u7ba1SELinux)","text":"<pre><code>sed -i 's/.*swap.*/#&amp;/' /etc/fstab\nswapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsystemctl  mask swap.target\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#4swap","title":"4.\u5173\u95edswap\u5206\u533a","text":"<pre><code>sed -i 's/.*swap.*/#&amp;/' /etc/fstab\nswapoff -a &amp;&amp; sysctl -w vm.swappiness=0\nsystemctl  mask swap.target\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#5","title":"5.\u65f6\u533a\u8ddf\u65f6\u95f4\u540c\u6b65","text":"<pre><code>timedatectl  set-timezone Asia/Shanghai\napt install -y chrony\ncat  &gt; /etc/chrony/chrony.conf &lt;&lt; 'EOF'\npool ntp.aliyun.com       iburst maxsources 4\nkeyfile /etc/chrony/chrony.keys\ndriftfile /var/lib/chrony/chrony.drift\nlogdir /var/log/chrony\nmaxupdateskew 100.0\nrtcsync\nmakestep 1 3\nEOF\nsystemctl restart chrony.service &amp;&amp; systemctl  enable chrony.service\nchronyc sources\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n^* 203.107.6.88                  2   6     7     1  -4176us[ +806us] +/-   29ms\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#6ipvs","title":"6.\u52a0\u8f7dIPVS\u6a21\u5757","text":"<pre><code>apt install ipset ipvsadm -y\ncat &gt;&gt; /etc/modules-load.d/k8s.conf &lt;&lt; 'EOF'\nbr_netfilter\nip_vs\nip_vs_rr\nip_vs_wrr\nip_vs_sh\nnf_conntrack\nnf_nat\noverlay\nvxlan\niptable_nat\nxt_MASQUERADE\nEOF\nsystemctl restart systemd-modules-load\nlsmod | grep -E 'br_netfilter|ip_vs|nf_conntrack|overlay|vxlan'\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#7","title":"7.\u5185\u6838\u53c2\u6570\u4f18\u5316","text":"<pre><code>cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; 'EOF'\n# \u5f00\u542fIPv4\u8f6c\u53d1\nnet.ipv4.ip_forward = 1\n# \u5141\u8bb8\u6865\u63a5\u7684\u6d41\u91cf\u8fdb\u5165iptables/netfilter\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\n# \u4f18\u5316\u8fde\u63a5\u8ddf\u8e2a\u8868\u5927\u5c0f\uff0c\u9632\u6b62\u5927\u89c4\u6a21\u8fde\u63a5\u7206\u6389\nnet.netfilter.nf_conntrack_max = 2310720\n# TCP\u4f18\u5316\uff08\u7f29\u77ed TIME_WAIT\uff0c\u5feb\u901f\u56de\u6536\u8fde\u63a5\uff09\nnet.ipv4.tcp_tw_reuse = 1\nnet.ipv4.tcp_fin_timeout = 15\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 5\n# \u5185\u5b58\u76f8\u5173\u4f18\u5316\uff08\u9632\u6b62OOM\uff09\nvm.swappiness = 0\nvm.overcommit_memory = 1\nvm.panic_on_oom = 0\n# \u6587\u4ef6\u53e5\u67c4\u9650\u5236\nfs.file-max = 52706963\n# \u7f51\u7edc\u5c42\u9762\u4f18\u5316\nnet.core.somaxconn = 32768\nnet.core.netdev_max_backlog = 16384\nnet.ipv4.tcp_max_syn_backlog = 16384\nEOF\nsysctl --system\ncat &gt;&gt; /etc/security/limits.conf &lt;&lt; 'EOF'\n* soft nofile 1048576\n* hard nofile 1048576\n* soft nproc 1048576\n* hard nproc 1048576\n* soft memlock unlimited\n* hard memlock unlimited\nEOF\nulimit  -n 64435\nulimit  -s 10240\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#_2","title":"\u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6","text":""},{"location":"k8s/kubeadm-v1.30.12/#1_1","title":"1.\u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684\u5bb9\u5668\u8fd0\u884c\u65f6","text":"<pre><code>apt-cache madison containerd\napt install -y containerd=1.7.28-0ubuntu1~24.04.1\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2contianerd","title":"2.\u914d\u7f6econtianerd","text":"<pre><code>mkdir -pv /etc/containerd\ncontainerd config default | sudo tee /etc/containerd/config.toml\n# \u4fee\u6539cgroup Driver\u4e3asystemd\nsed -ri 's@SystemdCgroup = false@SystemdCgroup = true@' /etc/containerd/config.toml\n# \u66f4\u6539sandbox_image\nsed -ri 's@registry.k8s.io\\/pause:3.8@registry.cn-hangzhou.aliyuncs.com\\/google_containers\\/pause:3.9@' /etc/containerd/config.toml\n# \u914d\u7f6econtainerd \u7684\u955c\u50cf\u5b58\u50a8\u76ee\u5f55\nsed -i 's@root = \"/var/lib/containerd\"@root = \"/data/containerd\"@g' /etc/containerd/config.toml\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#3","title":"3.\u914d\u7f6e\u52a0\u901f\u5668","text":"<pre><code>sed -i 's@config_path = \"\"@config_path = \"\\/etc\\/containerd\\/certs.d\\/\"@g' /etc/containerd/config.toml\n# docker.io \u955c\u50cf\u52a0\u901f\nmkdir -p /etc/containerd/certs.d/docker.io\ncat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt; 'EOF'\nserver = \"https://docker.io\" # \u6e90\u955c\u50cf\u5730\u5740\n\n[host.\"https://docker.1ms.run\"]\n  capabilities = [\"pull\",\"resolve\"]\n\n[host.\"https://docker.m.daocloud.io\"] # \u9053\u5ba2-\u955c\u50cf\u52a0\u901f\u5730\u5740\n  capabilities = [\"pull\",\"resolve\"]\n\n[host.\"https://dockerproxy.com\"] # \u955c\u50cf\u52a0\u901f\u5730\u5740\n  capabilities = [\"pull\", \"resolve\"]\n\n[host.\"https://docker.mirrors.sjtug.sjtu.edu.cn\"] # \u4e0a\u6d77\u4ea4\u5927-\u955c\u50cf\u52a0\u901f\u5730\u5740\n  capabilities = [\"pull\",\"resolve\"]\n\n[host.\"https://docker.mirrors.ustc.edu.cn\"] # \u4e2d\u79d1\u5927-\u955c\u50cf\u52a0\u901f\u5730\u5740\n  capabilities = [\"pull\",\"resolve\"]\n\n[host.\"https://docker.nju.edu.cn\"] # \u5357\u4eac\u5927\u5b66-\u955c\u50cf\u52a0\u901f\u5730\u5740\n  capabilities = [\"pull\",\"resolve\"]\n\n[host.\"https://registry-1.docker.io\"]\n  capabilities = [\"pull\",\"resolve\",\"push\"]\nEOF\n\n# registry.k8s.io \u955c\u50cf\u52a0\u901f\nmkdir -p /etc/containerd/certs.d/registry.k8s.io\ncat &gt; /etc/containerd/certs.d/registry.k8s.io/hosts.toml &lt;&lt; 'EOF'\nserver = \"https://registry.k8s.io\"\n\n[host.\"https://k8s.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\", \"push\"]\nEOF\n\n# quay.io \u955c\u50cf\u52a0\u901f\nmkdir -p /etc/containerd/certs.d/quay.io\ncat &gt; /etc/containerd/certs.d/quay.io/hosts.toml &lt;&lt; 'EOF'\nserver = \"https://quay.io\"\n\n[host.\"https://quay.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\", \"push\"]\nEOF\n\n# docker.elastic.co\u955c\u50cf\u52a0\u901f\nmkdir -p /etc/containerd/certs.d/docker.elastic.co\ntee /etc/containerd/certs.d/docker.elastic.co/hosts.toml &lt;&lt; 'EOF'\nserver = \"https://docker.elastic.co\"\n\n[host.\"https://elastic.m.daocloud.io\"]\n  capabilities = [\"pull\", \"resolve\", \"push\"]\nEOF\nsystemctl daemon-reload &amp;&amp;  systemctl restart containerd &amp;&amp; systemctl enable containerd\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#4crictl","title":"4.\u5b89\u88c5crictl\u5ba2\u6237\u7aef","text":"<pre><code>wget https://ghfast.top/https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.0/crictl-v1.24.0-linux-amd64.tar.gz\ntar -zxvf crictl-v1.24.0-linux-amd64.tar.gz -C /usr/local/bin\ncat &gt; /etc/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\ndebug: false\npull-image-on-create: false\nEOF\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#kubernetes","title":"\u5b89\u88c5Kubernetes\u96c6\u7fa4","text":""},{"location":"k8s/kubeadm-v1.30.12/#1kubeadm","title":"1.\u5b89\u88c5kubeadm\u8f6f\u4ef6\u5305","text":"<pre><code>apt-cache madison kubeadm\napt install kubeadm=1.30.12-1.1 kubelet=1.30.12-1.1 kubectl=1.30.12-1.1\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2kubelet","title":"2.\u914d\u7f6ekubelet\u5f00\u673a\u81ea\u542f","text":"<pre><code>systemctl enable --now kubelet\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#3_1","title":"3.\u96c6\u7fa4\u5b89\u88c5","text":""},{"location":"k8s/kubeadm-v1.30.12/#31master","title":"3.1.Master \u8282\u70b9\u521d\u59cb\u5316","text":"<pre><code># \u63d0\u524d\u62c9\u53d6\u955c\u50cf\nkubeadm config images pull --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --kubernetes-version=1.30.12\nkubeadm init \\\n  --kubernetes-version=1.30.12 \\\n  --control-plane-endpoint=\"k8s-vip\" \\\n  --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\\n  --pod-network-cidr=10.244.0.0/16 \\\n  --service-cidr=10.96.0.0/12 \\\n  --token-ttl=0 \\\n  --upload-certs|tee kubeadm.log\n[init] Using Kubernetes version: v1.30.12\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [k8s-master01 k8s-vip kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.111]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.1.111 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.1.111 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"super-admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\"\n[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s\n[kubelet-check] The kubelet is healthy after 1.002218069s\n[api-check] Waiting for a healthy API server. This can take up to 4m0s\n[api-check] The API server is healthy after 5.504002718s\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace\n[upload-certs] Using certificate key:\nb02bcd4ca23ffb9ea4aaf72cd7fecb745592efd7acb50ce49d651b0c676780c5\n[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: 2zzmat.b1omkhqhuns4s30x\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n  kubeadm join k8s-vip:6443 --token 2zzmat.b1omkhqhuns4s30x \\\n    --discovery-token-ca-cert-hash sha256:66d67a3c65fb6a32e5e67b1194ec9a49fc826f1ae1d9ab92b3a2932e49a82d5e \\\n    --control-plane --certificate-key b02bcd4ca23ffb9ea4aaf72cd7fecb745592efd7acb50ce49d651b0c676780c5\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join k8s-vip:6443 --token 2zzmat.b1omkhqhuns4s30x \\\n    --discovery-token-ca-cert-hash sha256:66d67a3c65fb6a32e5e67b1194ec9a49fc826f1ae1d9ab92b3a2932e49a82d5e\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#32kubectl","title":"3.2.\u914d\u7f6ekubectl\u547d\u4ee4","text":"<pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nkubectl  get nodes\nNAME           STATUS     ROLES           AGE    VERSION\nk8s-master01   NotReady   control-plane   107s   v1.30.12\nkubectl  get pods -A\nNAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6d58d46f65-2fg6x               0/1     Pending   0          93s\nkube-system   coredns-6d58d46f65-p66nm               0/1     Pending   0          93s\nkube-system   etcd-k8s-master01                      1/1     Running   0          107s\nkube-system   kube-apiserver-k8s-master01            1/1     Running   0          107s\nkube-system   kube-controller-manager-k8s-master01   1/1     Running   0          110s\nkube-system   kube-proxy-m2wnr                       1/1     Running   0          93s\nkube-system   kube-scheduler-k8s-master01            1/1     Running   0          109s\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#33worker","title":"3.3.\u52a0\u5165worker\u8282\u70b9","text":"<pre><code>kubeadm join k8s-vip:6443 --token 2zzmat.b1omkhqhuns4s30x \\\n    --discovery-token-ca-cert-hash sha256:66d67a3c65fb6a32e5e67b1194ec9a49fc826f1ae1d9ab92b3a2932e49a82d5e\nkubectl  get nodes -owide\nNAME           STATUS     ROLES           AGE     VERSION    INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nk8s-master01   NotReady   control-plane   3m58s   v1.30.12   192.168.1.111   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-85-generic   containerd://1.7.28\nk8s-worker01   NotReady   &lt;none&gt;          52s     v1.30.12   192.168.1.121   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-85-generic   containerd://1.7.28\nk8s-worker02   NotReady   &lt;none&gt;          27s     v1.30.12   192.168.1.122   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-85-generic   containerd://1.7.28\nk8s-worker03   NotReady   &lt;none&gt;          23s     v1.30.12   192.168.1.123   &lt;none&gt;        Ubuntu 24.04.2 LTS   6.8.0-85-generic   containerd://1.7.28\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#4","title":"4.\u90e8\u7f72\u7f51\u7edc\u63d2\u4ef6","text":"<p>\u7248\u672c\u5bf9\u5e94\u5173\u7cfb:<code>https://docs.tigera.io/calico/3.29/getting-started/kubernetes/requirements</code></p>"},{"location":"k8s/kubeadm-v1.30.12/#41","title":"4.1.\u83b7\u53d6\u90e8\u7f72\u6587\u4ef6","text":"<pre><code>curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.6/manifests/calico.yaml -O\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#42calico","title":"4.2.\u914d\u7f6ecalico","text":"<pre><code># \u914d\u7f6ePod\u7f51\u7edc\nsed -i 's@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g' calico.yaml\nsed -i 's@#   value: \"192.168.0.0/16\"@  value: \"10.244.0.0/16\"@g' calico.yaml\n# \u9ed8\u8ba4\u4e3a26\u4f4d\u63a9\u7801\uff0c\u6539\u621024\uff0c\u53ef\u7528\u5730\u5740\u6709\u591a\u5c11\u4e2a2^8 -2  = 254 \u4e2a\nsed -i '/value: \"10\\.244\\.0\\.0\\/16\"/a\\            - name: CALICO_IPV4POOL_BLOK_SIZE\\n              value: \"24\"'  calico.yaml\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#43calico","title":"4.3.\u90e8\u7f72calico","text":"<pre><code>kubectl apply -f calico.yaml\nkubectl  get pod -l k8s-app=calico-node -n kube-system\nNAME                READY   STATUS    RESTARTS   AGE\ncalico-node-l95lq   1/1     Running   0          5m46s\ncalico-node-lz5s9   1/1     Running   0          5m46s\ncalico-node-n6mh5   1/1     Running   0          5m46s\ncalico-node-v2q6d   1/1     Running   0          5m46s\nroot@k8s-master01:~/v1.30.12/calico# kubectl  get nodes\nNAME           STATUS   ROLES           AGE    VERSION\nk8s-master01   Ready    control-plane   169m   v1.30.12\nk8s-worker01   Ready    &lt;none&gt;          166m   v1.30.12\nk8s-worker02   Ready    &lt;none&gt;          166m   v1.30.12\nk8s-worker03   Ready    &lt;none&gt;          166m   v1.30.12\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#5ipvs","title":"5.\u4fee\u6539\u96c6\u7fa4\u7f51\u7edc\u4e3aIPVS","text":"<pre><code>kubectl  edit cm/kube-proxy -n kube-system -o yaml\n  ipvs:\n    ...\n    strictARP: true\n  mode: \"ipvs\"\nkubectl -n kube-system rollout restart daemonset kube-proxy\nipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.96.0.1:443 rr\n  -&gt; 192.168.1.111:6443           Masq    1      1          0\nTCP  10.96.0.10:53 rr\n  -&gt; 10.244.69.193:53             Masq    1      0          0\n  -&gt; 10.244.69.194:53             Masq    1      0          0\nTCP  10.96.0.10:9153 rr\n  -&gt; 10.244.69.193:9153           Masq    1      0          0\n  -&gt; 10.244.69.194:9153           Masq    1      0          0\nUDP  10.96.0.10:53 rr\n  -&gt; 10.244.69.193:53             Masq    1      0          0\n  -&gt; 10.244.69.194:53             Masq    1      0          0\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#_3","title":"\u9a8c\u8bc1\u96c6\u7fa4","text":""},{"location":"k8s/kubeadm-v1.30.12/#1deploysvc","title":"1.\u521b\u5efadeploy\u548csvc","text":"<pre><code>kubectl  create deployment myapp --image=ikubernetes/myapp:v1 --replicas=5\nkubectl expose deployment myapp --port=80 --target-port=80 --type=\"NodePort\"\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2podsvc","title":"2.\u67e5\u770bpod\u8ddfsvc","text":"<pre><code>kubectl  get pod -A -o wide -l app=myapp\nNAMESPACE   NAME                    READY   STATUS    RESTARTS   AGE     IP              NODE           NOMINATED NODE   READINESS GATES\ndefault     myapp-64b9c4c64-bsszd   1/1     Running   0          18s     10.244.39.196   k8s-worker03   &lt;none&gt;           &lt;none&gt;\ndefault     myapp-64b9c4c64-dnkvt   1/1     Running   0          17s     10.244.69.197   k8s-worker02   &lt;none&gt;           &lt;none&gt;\ndefault     myapp-64b9c4c64-sltln   1/1     Running   0          5m5s    10.244.79.67    k8s-worker01   &lt;none&gt;           &lt;none&gt;\ndefault     myapp-64b9c4c64-x9cn6   1/1     Running   0          2m25s   10.244.79.68    k8s-worker01   &lt;none&gt;           &lt;none&gt;\ndefault     myapp-64b9c4c64-zcm96   1/1     Running   0          17s     10.244.39.197   k8s-worker03   &lt;none&gt;           &lt;none&gt;\nkubectl  get svc myapp\nNAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nmyapp   NodePort   10.100.179.119   &lt;none&gt;        80:32203/TCP   2s\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#3svcpod","title":"3.\u901a\u8fc7svc\u8bbf\u95eepod","text":"<pre><code>for i in `seq 5`;do curl 10.100.179.119/hostname.html;done\nmyapp-64b9c4c64-x9cn6\nmyapp-64b9c4c64-sltln\nmyapp-64b9c4c64-dnkvt\nmyapp-64b9c4c64-zcm96\nmyapp-64b9c4c64-bsszd\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#4-dns","title":"4. \u9a8c\u8bc1dns","text":"<p><pre><code>apt install dnsutils\nkubectl get svc -n kube-system\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   3h7m\ndig -t A myapp.default.svc.cluster.local @10.96.0.10\n\n; &lt;&lt;&gt;&gt; DiG 9.18.39-0ubuntu0.24.04.1-Ubuntu &lt;&lt;&gt;&gt; -t A myapp.default.svc.cluster.local @10.96.0.10\n;; global options: +cmd\n;; Got answer:\n;; WARNING: .local is reserved for Multicast DNS\n;; You are currently testing what happens when an mDNS query is leaked to DNS\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 10731\n;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n;; WARNING: recursion requested but not available\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 1232\n; COOKIE: 3f502025a920150a (echoed)\n;; QUESTION SECTION:\n;myapp.default.svc.cluster.local. IN    A\n\n;; ANSWER SECTION:\nmyapp.default.svc.cluster.local. 30 IN  A   10.100.179.119\n\n;; Query time: 2 msec\n;; SERVER: 10.96.0.10#53(10.96.0.10) (UDP)\n;; WHEN: Tue Oct 21 01:04:18 CST 2025\n;; MSG SIZE  rcvd: 119\n</code></pre> <pre><code>kubectl  exec -it myapp-64b9c4c64-bsszd -- sh\n/ # wget -O - -q myapp\nHello MyApp | Version: v1 | &lt;a href=\"hostname.html\"&gt;Pod Name&lt;/a&gt;\n/ # wget -O - -q myapp.default\nHello MyApp | Version: v1 | &lt;a href=\"hostname.html\"&gt;Pod Name&lt;/a&gt;\n/ # wget -O - -q myapp.default.svc.cluster.local.\nHello MyApp | Version: v1 | &lt;a href=\"hostname.html\"&gt;Pod Name&lt;/a&gt;\n</code></pre></p>"},{"location":"k8s/kubeadm-v1.30.12/#add-on","title":"\u5b89\u88c5\u63d2\u4ef6Add-On","text":""},{"location":"k8s/kubeadm-v1.30.12/#1metrics-server","title":"1.metrics-server","text":"<pre><code>wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.2/components.yaml\nvim commponents.yaml\n    spec:\n      containers:\n      - args:\n        - --cert-dir=/tmp\n        - --secure-port=10250\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        - --kubelet-insecure-tls\nkubectl apply -f commponents.yaml\nkubectl  top nodes\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   226m         11%    889Mi           11%\nk8s-worker01   98m          4%     699Mi           8%\nk8s-worker02   98m          4%     1442Mi          18%\nk8s-worker03   101m         5%     544Mi           6%\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#dashboard","title":"dashboard","text":""},{"location":"k8s/kubeadm-v1.30.12/#1dashboard","title":"1.\u5b89\u88c5dashboard","text":"<pre><code>wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\nkubectl apply -f recommended.yaml\nkubectl  get pod  -n kubernetes-dashboard\nNAME                                         READY   STATUS    RESTARTS   AGE\ndashboard-metrics-scraper-795895d745-z468j   1/1     Running   0          61s\nkubernetes-dashboard-56cf4b97c5-s6ndg        1/1     Running   0          61s\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2svcnodeport","title":"2.\u4fee\u6539svc\u7c7b\u578b\u4e3anodeport","text":"<pre><code>kubectl patch svc kubernetes-dashboard -n kubernetes-dashboard -p '{\"spec\": {\"type\": \"NodePort\"}}'\nkubectl  get svc -n kubernetes-dashboard\nNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\ndashboard-metrics-scraper   ClusterIP   10.104.234.188   &lt;none&gt;        8000/TCP        3m11s\nkubernetes-dashboard        NodePort    10.107.61.90     &lt;none&gt;        443:30403/TCP   3m11s\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#3_2","title":"3.\u521b\u5efa\u7528\u6237","text":"<pre><code>cat &gt; admin-user.yaml &lt;&lt; 'EOF'\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: \"admin-user\"\ntype: kubernetes.io/service-account-token\nEOF\nkubectl apply -f admin-user.yaml\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#4token","title":"4.\u83b7\u53d6token","text":"<pre><code>kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath=\"{.data.token}\" | base64 -d\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#_4","title":"\u5378\u8f7d\u96c6\u7fa4","text":""},{"location":"k8s/kubeadm-v1.30.12/#1_2","title":"1.\u5378\u8f7d\u6574\u4e2a\u96c6\u7fa4","text":"<p>\u5148\u62c6\u9664\u5404\u4e2a\u5de5\u4f5c\u8282\u70b9\uff0c\u5728\u62c6\u9664\u63a7\u5236\u5e73\u9762</p> <pre><code>kubeadm reset\nrm -rf /etc/kubernetes /var/lib/kubelet/ /var/lib/cni/ /etc/cni/net.d/ /var/lib/etcd/\n</code></pre>"},{"location":"k8s/kubeadm-v1.30.12/#2_1","title":"2.\u62c6\u9664\u5355\u4e2a\u5de5\u4f5c\u8282\u70b9","text":"<pre><code># 1. \u7981\u6b62\u8c03\u5ea6\nkubectl cordon k8s-worker03\n# 2. \u6392\u7a7a\u8282\u70b9\nkubectl drain k8s-worker03\n# 3. \u5220\u9664\u8282\u70b9\nkubectl delete node  k8s-worker03\n# 4. \u6267\u884creset\u8ddf\u540e\u7eed\u7684\u6e05\u7406\u5de5\u4f5c\nkubeadm reset.\nrm -rf /etc/kubernetes /var/lib/kubelet/ /var/lib/cni/ /etc/cni/net.d/ /var/lib/etcd/\n</code></pre>"},{"location":"k8s/kustomize/","title":"Kuberentes\u539f\u751f\u914d\u7f6e\u7ba1\u7406","text":"<p>Kustomize \u5f15\u5165\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u677f\u5373\u53ef\u81ea\u5b9a\u4e49\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u73b0\u6210\u5e94\u7528\u7a0b\u5e8f\u7684\u4f7f\u7528\u3002</p>"},{"location":"k8s/kustomize/#_1","title":"\u57fa\u7840\u5165\u95e8","text":"<ul> <li>\u7ec4\u7ec7\u76ee\u5f55\u7ed3\u6784</li> </ul> <p><pre><code>~]# tree base\nbase\n\u251c\u2500\u2500 deployment.yaml\n\u251c\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 service.yaml\n</code></pre> - deployment.yaml</p> <p><pre><code>cat &gt; deployment.yaml &lt;&lt; 'EOF'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: ikubernetes/myapp:v1\n        ports:\n        - name: http-80\n          containerPort: 80\n          protocol: TCP\nEOF\n</code></pre> - service.yaml</p> <p><pre><code>cat &gt; service.yaml &lt;&lt; 'EOF'\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp\nspec:\n  type: ClusterIP\n  selector:\n    app: myapp\n  ports:\n  - name: http-80\n    port: 80\n    targetPort: 80\nEOF\n</code></pre> - kustomization.yaml</p> <p><pre><code>cat &gt; kustomization.yaml &lt;&lt; 'EOF'\n---\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - deployment.yaml\n  - service.yaml\nEOF\n</code></pre> - \u67e5\u770b\u6e32\u67d3\u6548\u679c</p> <p><pre><code># \u5b89\u88c5 kustomize\n~# curl -s \"https://ghfast.top/https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\nv5.7.1\n~# mv kustomize  /usr/local/bin/\n~# kustomize version\nv5.7.1\n~# kustomize build .\n</code></pre> - \u5e94\u7528\u8d44\u6e90\u6e05\u5355</p> <pre><code>base# kubectl  apply -k .\nservice/myapp created\ndeployment.apps/myapp created\n</code></pre>"},{"location":"k8s/kustomize/#_2","title":"Kustomize","text":""},{"location":"storage/auth/","title":"\u8ba4\u8bc1\u7ba1\u7406","text":"<p>ceph \u4f5c\u4e3a\u4e00\u4e2a\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u8c61\u5b58\u50a8\u3001\u5feb\u8bbe\u5907\u548c\u6587\u4ef6\u7cfb\u7edf\uff0c\u4e3a\u4e86\u5728\u7f51\u7edc\u4f20\u8f93\u4e2d\u7e41\u6b96\u6570\u636e\u88ab\u7be1\u6539\uff0c\u505a\u5230\u8f83\u9ad8\u7a0b\u5ea6\u7684\u5b89\u5168\u6027\uff0c\u52a0\u5165CephX\u52a0\u5bc6\u8ba4\u8bc1\u534f\u8bae\u3002\u76ee\u7684\u662f\u4e3a\u4e86\u5728\u5ba2\u6237\u7aef\u548c\u7ba1\u7406\u7aef\u4e4b\u524d\u5b9e\u73b0\u8eab\u4efd\u8bc6\u522b\uff0c\u6570\u636e\u52a0\u5bc6\u3001\u9a8c\u8bc1\u7b49\u3002ceph\u96c6\u7fa4\u9ed8\u8ba4\u5f00\u542f\u4e86cephX\u534f\u8bae</p> <pre><code>ceph-cluster]$ cat ceph.conf\n[global]\n...\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n</code></pre>"},{"location":"storage/auth/#_2","title":"\u8ba4\u8bc1\u548c\u6388\u6743","text":"<p>ceph\u7cfb\u7edf\u4e2d\uff0c\u6240\u6709\u7684\u5143\u6570\u636e\u90fd\u4fdd\u5b58\u5728mon\u8282\u70b9\u7684ceph-mon\u8fdb\u7a0b\u4e2d\uff0cmon\u4fdd\u5b58\u4e86\u7cfb\u7edf\u4e2d\u91cd\u8981\u7684\u8ba4\u8bc1\u76f8\u5173\u5143\u6570\u636e\uff0c\u4e0b\u9762\u662f\u6bcf\u4e2a\u7528\u6237key\u4ee5\u53ca\u6743\u9650\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>~]$ ceph auth ls\nclient.admin\n    key: AQD0KfdoGLBlDhAARnjOcCCD2+J+ZHhJU8F1Aw==\n    caps: [mds] allow *\n    caps: [mgr] allow *\n    caps: [mon] allow *\n    caps: [osd] allow *\n</code></pre> \u5bf9\u4e8eceph\u8ba4\u8bc1\u548c\u6388\u6743\u6765\u8bf4\uff0c\u4e3b\u8981\u6d89\u53ca\u4e09\u4e2a\u5185\u5bb9\uff0cceph\u7528\u6237\uff0c\u8d44\u6e90\u6743\u9650\uff0c\u7528\u6237\u6388\u6743 - ceph\u7528\u6237\u5fc5\u987b\u62e5\u6709\u6267\u884c\u6743\u9650\u624d\u80fd\u6267\u884cceph\u7684\u7ba1\u7406\u547d\u4ee4 - ceph\u7528\u6237\u9700\u8981\u62e5\u6709\u5b58\u50a8\u6c60\u7684\u8bbf\u95ee\u6743\u9650\u624d\u80fd\u5230ceph\u4e2d\u8bfb\u5199\u6570\u636e</p> <p>1.\u7528\u6237 ceph\u521b\u5efa\u51fa\u6765\u7684\u7528\u6237</p> <p>2.\u6388\u6743 \u5c06\u67d0\u4e9b\u8d44\u6e90\u7684\u4f7f\u7528\u6743\u5229\u53eb\u4e2a\u7279\u5b9a\u7684\u7528\u6237<code>allow</code></p> <p>3.\u6743\u9650 - r - w - x - class-read - class-write - profile osd</p>"},{"location":"storage/auth/#_3","title":"\u7528\u6237\u7ba1\u7406","text":""},{"location":"storage/auth/#1","title":"1.\u5217\u51fa\u7528\u6237","text":"<pre><code>~]$ ceph auth list\n</code></pre>"},{"location":"storage/auth/#2","title":"2.\u68c0\u7d22\u7528\u6237","text":"<pre><code>~]$ ceph auth get client.admin\n[client.admin]\n    key = AQD0KfdoGLBlDhAARnjOcC4X2+J+ZHhJU8F1Aw==\n    caps mds = \"allow *\"\n    caps mgr = \"allow *\"\n    caps mon = \"allow *\"\n    caps osd = \"allow *\"\nexported keyring for client.admin\n~]$ ceph auth export client.admin\n[client.admin]\n    key = AQD0KfdoGLBlDhAARnjOcC4X2+J+ZHhJU8F1Aw==\n    caps mds = \"allow *\"\n    caps mgr = \"allow *\"\n    caps mon = \"allow *\"\n    caps osd = \"allow *\"\nexport auth(key=AQD0KfdoGLBlDhAARnjOcC4X2+J+ZHhJU8F1Aw==)\n</code></pre>"},{"location":"storage/auth/#3","title":"3.\u5217\u51fa\u7528\u6237\u79c1\u94a5","text":"<pre><code>~]$ ceph auth print-key client.admin\nAQD0KfdoGLBlDhAARnjOcC4X2+J+ZHhJU8F1Aw==\n</code></pre>"},{"location":"storage/auth/#4","title":"4.\u6dfb\u52a0\u7528\u6237","text":"<pre><code># ceph auth add: \u521b\u5efa\u7528\u6237\u3001\u751f\u6210\u5bc6\u94a5\u5e76\u6dfb\u52a0\u6307\u5b9acaps\n~]$ ceph auth add client.testuser mon 'allow r' osd 'allow rw pool=rbdpool'\nadded key for client.testuser\n~]$ ceph auth get client.testuser\n[client.testuser]\n    key = AQA2XPdodzX7DRAAkXjpN2u/pUlLqrGMIG2r7w==\n    caps mon = \"allow r\"\n    caps osd = \"allow rw pool=rbdpool\"\nexported keyring for client.testuser\n#ceph auth get-or-create: \u521b\u5efa\u7528\u6237\u5e76\u8fd4\u56de\u5bc6\u94a5\u6587\u4ef6\u683c\u5f0f\u7684\u5bc6\u94a5\u4fe1\u606f\uff0c\u7528\u6237\u5b58\u5728\u65f6\u8fd4\u56de\u5bc6\u94a5\u4fe1\u606f\n~]$ ceph auth get-or-create  client.testuser mon 'allow rw' osd 'allow rw pool=rbdpool'\n[client.testuser]\n    key = AQA2XPdodzX7DRAAkXjpN2u/pUlLqrGMIG2r7w==\n#ceph auth get-or-create-key: \u521b\u5efa\u7528\u6237\u5e76\u8fd4\u56de\u5bc6\u94a5\u4fe1\u606f\uff0c\u7528\u6237\u5b58\u5728\u65f6\u8fd4\u56de\u5bc6\u94a5\u4fe1\u606f\n~]$ ceph auth get-or-create-key  client.testuser mon 'allow rw' osd 'allow rw pool=rbdpool'\nAQA2XPdodzX7DRAAkXjpN2u/pUlLqrGMIG2r7w==\n</code></pre>"},{"location":"storage/auth/#5","title":"5.\u5bfc\u5165/\u5bfc\u51fa\u7528\u6237","text":"<pre><code># \u5bfc\u51fa\n~]$ ceph auth export client.testuser &gt; testuser.file\n# \u5bfc\u5165\n~]$ ceph auth import -i testuser.file\nimported keyring\n</code></pre>"},{"location":"storage/auth/#6","title":"6.\u66f4\u65b0\u7528\u6237","text":"<pre><code># \u66ff\u6362\u7528\u6237\u6743\u9650,\u4e0d\u662f\u8ffd\u52a0\uff0c\u662f\u6574\u4f53\u66ff\u6362\n~]$ ceph auth caps client.testuser mon 'allow rw' osd 'allow rw pool=rbdpool'\nupdated caps for client.testuser\n</code></pre>"},{"location":"storage/auth/#7","title":"7.\u5220\u9664\u7528\u6237","text":"<pre><code>~]$ ceph auth del client.testuser\n</code></pre>"},{"location":"storage/auth/#keyring","title":"Keyring","text":"<p>\u5bc6\u94a5\u73af\u6587\u4ef6\u662f\u5b58\u50a8\u673a\u5bc6\u3001\u5bc6\u7801\u3001\u5bc6\u94a5\u3001\u8bc1\u4e66\u5e76\u4f7f\u4ed6\u4eec\u53ef\u5e94\u7528\u5e94\u7528\u7a0b\u5e8f\u7684\u7ec4\u4ef6\u7684\u96c6\u5408\u3002\u5bc6\u94a5\u73af\u6587\u4ef6\u5b58\u50a8\u4e00\u4e2a\u6216\u591a\u4e2aCeph\u8eab\u4efd\u9a8c\u8bc1\u5bc6\u94a5\u4ee5\u53ca\u53ef\u80fd\u7684\u76f8\u5173\u529f\u80fd\u89c4\u8303</p> <p>\u8bbf\u95eeCeph\u96c6\u7fa4\u65f6\uff0c\u5ba2\u6237\u7aef\u9ed8\u8ba4\u4f1a\u4e8e\u672c\u5730\u67e5\u770b\u5bc6\u94a5\u73af\uff0c\u53ea\u6709\u8ba4\u8bc1\u6210\u529f\u7684\u5bf9\u8c61\u624d\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\uff0c\u9ed8\u8ba4\u7684\u5bc6\u94a5\u73af\u6709\u4e0b\u9762\u56db\u4e2a\uff1a</p> <ul> <li><code>/etc/ceph/cluster-name.user-name.keyring</code>: \u5355\u7528\u6237</li> <li><code>/etc/ceph/cluster.keyring</code>\uff1a\u591a\u7528\u6237</li> <li><code>/etc/ceph/keyring</code></li> <li><code>/etc/ceph/keyring.bin</code> <pre><code>~]$ ls /etc/ceph/ -l\n\u603b\u7528\u91cf 12\n-rw-r-----+ 1 root root 151 10\u6708 21 14:45 ceph.client.admin.keyring\n-rw-r--r--  1 root root 323 10\u6708 21 14:45 ceph.conf\n-rw-r--r--  1 root root  92 6\u6708  30 2021 rbdmap\n-rw-------  1 root root   0 10\u6708 21 14:45 tmpklVbo9\n</code></pre></li> </ul>"},{"location":"storage/auth/#keyring_1","title":"keyring\u7ba1\u7406","text":""},{"location":"storage/auth/#keyring_2","title":"\u6dfb\u52a0keyring","text":"<ul> <li>\u65b9\u5f0f1:\u5148\u521b\u5efa\u7528\u6237\uff0c\u5728\u5bfc\u51fakeyring\u6587\u4ef6 <pre><code># ceph auth add\n~]$ ceph  auth get-or-create client.kube mon 'allow r' osd 'allow * pool=kube'\n[client.kube]\n    key = AQDGYfdoWGEBFxAAZCGy/1RktgAWmuyYKTkhow==\n# ceph auth get -o\n~]$ ceph auth get client.kube -o ceph.client.kube.keyring\nexported keyring for client.kube\n~]$ cat ceph.client.kube.keyring\n[client.kube]\n    key = AQDGYfdoWGEBFxAAZCGy/1RktgAWmuyYKTkhow==\n    caps mon = \"allow r\"\n    caps osd = \"allow * pool=kube\"\n</code></pre></li> <li>\u65b9\u5f0f2:\u521b\u5efakeyring\uff0c\u7136\u540e\u5bfc\u5165\u96c6\u7fa4 <pre><code>#ceph-authtool\n~]$ ceph-authtool --create-keyring ceph.cluster.keyring\n#ceph auth add -i \u5408\u5e76\u591a\u7528\u7528\u6237\u5bc6\u94a5\u73af\u6587\u4ef6\n~]$ ceph-authtool ceph.cluster.keyring --import-keyring ceph.client.kube.keyring\n~]$ ceph-authtool ceph.cluster.keyring --import-keyring ceph-cluster/ceph.client.admin.keyring\n~]$ cat ceph.cluster.keyring\n[client.admin]\n    key = AQD0KfdoGLBlDhAARnjOcC4X2+J+ZHhJU8F1Aw==\n    caps mds = \"allow *\"\n    caps mgr = \"allow *\"\n    caps mon = \"allow *\"\n    caps osd = \"allow *\"\n[client.kube]\n    key = AQDGYfdoWGEBFxAAZCGy/1RktgAWmuyYKTkhow==\n    caps mon = \"allow r\"\n    caps osd = \"allow * pool=kube\"\n</code></pre></li> </ul>"},{"location":"storage/ceph/","title":"\u4f7f\u7528Ceph-Deploy\u90e8\u7f72Ceph\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf","text":""},{"location":"storage/ceph/#ceph","title":"ceph\u96c6\u7fa4\u89c4\u5212","text":"<p>\u4f7f\u7528deph-deploy\u5feb\u901f\u90e8\u7f72ceph\u96c6\u7fa4\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e3aCentOS Linux release 7.5.1804 (Core)\uff0c\u7528\u5230\u7684\u5404\u76f8\u5173\u7a0b\u5e8f\u7248\u672c\u5982\u4e0b:</p> <ul> <li>ceph-deploy: 2.0.1</li> <li>ceph: 14.2.22(nautilus)</li> <li>\u516c\u5171\u7f51\u7edc: 192.168.1.0/24</li> <li>\u96c6\u7fa4\u7f51\u7edc: 192.168.122.0/24</li> </ul> \u4e3b\u673a\u540d IP \u673a\u5668\u914d\u7f6e \u89d2\u8272 \u5907\u6ce8 ceph-admin 192.168.1.100192.168.122.100 2c4g/60G admin\u7ba1\u7406\u8282\u70b9 ceph-node01 192.168.1.101192.168.122.101 2c4g/60G/30G/50G mon\u3001mgr\u3001osd\u3001mds \u5176\u4e2dvdb,vdc\u9884\u7559\u7ed9osd\u4f7f\u7528 ceph-node02 192.168.1.102192.168.122.102 2c4g/60G/30G/50G mon\u3001mgr\u3001osd\u3001mds \u5176\u4e2dvdb,vdc\u9884\u7559\u7ed9osd\u4f7f\u7528 ceph-node03 192.168.1.103192.168.122.103 2c4g/60G/30G/50G mon\u3001rgw\u3001osd \u5176\u4e2dvdb,vdc\u9884\u7559\u7ed9osd\u4f7f\u7528 <ul> <li>ceph-mgr(Manager)\u7ba1\u7406\u5668: \u96c6\u7fa4\u6027\u80fd\u6307\u6807\u624b\u673a\uff0c\u7ba1\u7406\u63d2\u4ef6\uff0c\u8d1f\u8f7d\u5747\u8861\uff0c\u63d0\u4f9bAPI\u670d\u52a1</li> <li>ceph-mon(Monitor)\u76d1\u63a7\u5668: \u96c6\u7fa4\u72b6\u6001\u7ba1\u7406\uff0c\u4f7f\u7528 Paxos \u7b97\u6cd5\u4fdd\u8bc1\u5143\u6570\u636e\u4e00\u81f4\u6027\uff0c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u4ef2\u88c1\u7ba1\u7406\uff0c\u751f\u4ea7\u73af\u5883\u6700\u5c113\u4e2a\u8282\u70b9</li> <li>ceph-osd(Object Storage Daemon)\u5bf9\u8c61\u5b58\u50a8\u5b88\u62a4\u8fdb\u7a0b: \u4e00\u5757\u78c1\u76d8\u5c31\u662f\u4e00\u4e2aOSD\uff0c\u63d0\u4f9b\u6570\u636e\u5b58\u50a8\uff0c\u6570\u636e\u590d\u5236\uff0c\u5fc3\u8df3\u68c0\u6d4b\uff0c\u6570\u636e\u518d\u5e73\u8861</li> <li>ceph-mds(Metadata Server)\u5143\u6570\u636e\u670d\u52a1: \u5143\u6570\u636e\u7ba1\u7406(\u7ef4\u62a4\u6587\u4ef6\u548c\u76ee\u5f55\u7684\u5c42\u6b21\u7ed3\u6784\u3001\u7ba1\u7406\u6587\u4ef6\u6743\u9650\u3001\u5927\u5c0f\u3001\u65f6\u95f4\u6233\u7b49 inode \u4fe1\u606f\u3001\u7ef4\u62a4\u5b8c\u6574\u7684\u6587\u4ef6\u7cfb\u7edf\u547d\u540d\u7a7a\u95f4)\uff0c\u5ba2\u6237\u7aef\u534f\u8c03(\u5c06\u6587\u4ef6\u8def\u5f84\u8f6c\u6362\u4e3a\u5bf9\u8c61\u5b58\u50a8\u4f4d\u7f6e\u3001\u5904\u7406\u6587\u4ef6\u9501\u548c\u76ee\u5f55\u9501\uff0c\u786e\u4fdd\u591a\u4e2a\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u7f13\u5b58\u4e00\u81f4\u6027)</li> <li>ceph-rgw(RADOS Gateway)\u5bf9\u8c61\u5b58\u50a8\u7f51\u5173: \u4f9b\u4e86\u4e0e Amazon S3 \u548c Swift \u517c\u5bb9\u7684 RESTful API \u63a5\u53e3\uff0c\u5c06 Ceph \u5b58\u50a8\u96c6\u7fa4\u66b4\u9732\u4e3a\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1</li> <li>PG(Placement Groups): \u4e00\u4e2aPG\u5305\u542b\u591a\u4e2aOSD\uff0c\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u914d\u6570\u636e\u548c\u6570\u636e\u5b9a\u4f4d\uff0c\uff0c\u5199\u6570\u636e\u65f6\uff0c\u5148\u5199\u5165\u4e3bOSD\uff0c\u5728\u5197\u4f59\u5230\u526f\u672cOSD\u8282\u70b9</li> </ul>"},{"location":"storage/ceph/#_1","title":"\u7cfb\u7edf\u73af\u5883\u51c6\u5907","text":""},{"location":"storage/ceph/#1","title":"1.\u4e3b\u673a\u540d\u89e3\u6790","text":"<pre><code>cat &gt;&gt; /etc/hosts &lt;&lt; 'EOF'\n192.168.1.100 ceph-admin\n192.168.1.101 ceph-node01\n192.168.1.102 ceph-node02\n192.168.1.103 ceph-node03\nEOF\n</code></pre>"},{"location":"storage/ceph/#2selinux","title":"2.\u5173\u95ed\u9632\u706b\u5899\u548cSelinux","text":"<pre><code>for i in stop disable;do systemctl $i firewalld; done\nsed -i 's/enforcing/disabled/' /etc/selinux/config &amp;&amp; setenforce 0\n</code></pre>"},{"location":"storage/ceph/#3networkmanager","title":"3.\u5173\u95edNetworkManager","text":"<pre><code>systemctl disable NetworkManager &amp;&amp; systemctl stop NetworkManager\n</code></pre>"},{"location":"storage/ceph/#4","title":"4.\u5185\u6838\u53c2\u6570\u4f18\u5316","text":"<pre><code>echo \"ulimit -SHn 102400\" &gt;&gt; /etc/rc.local\ncat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF\n* soft nofile 65535\n* hard nofile 65535\nEOF\ncat &gt;&gt; /etc/sysctl.conf &lt;&lt; EOF\nkernel.pid_max = 4194303\nEOF\necho \"vm.swappiness = 0\" &gt;&gt; /etc/sysctl.conf\nsysctl -p\n</code></pre>"},{"location":"storage/ceph/#5","title":"5.\u521b\u5efa\u666e\u901a\u7528\u6237","text":"<p>ceph-deploy \u5fc5\u987b\u4ee5\u666e\u901a\u7528\u6237\u767b\u5f55\u5230Ceph\u96c6\u7fa4\u7684\u5404\u76ee\u6807\u8282\u70b9\uff0c\u4e14\u6b64\u7528\u6237\u9700\u8981\u62e5\u6709\u65e0\u5bc6\u7801\u4f7f\u7528sudo\u547d\u4ee4\u7684\u6743\u9650\uff0c\u4ee5\u4fbf\u5728\u5b89\u88c5\u8f6f\u4ef6\u53ca\u751f\u6210\u914d\u7f6e\u6587\u4ef6\u7684\u8fc7\u7a0b\u4e2d\u65e0\u9700\u4e2d\u65ad\u914d\u7f6e\u8fc7\u7a0b <pre><code>useradd  cephadm\necho \"cephadm\" | passwd --stdin cephadm\necho \"cephadm ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/cephadm\nchmod 0440 /etc/sudoers.d/cephadm\n</code></pre></p>"},{"location":"storage/ceph/#6","title":"6.\u4e3b\u673a\u4e92\u4fe1","text":"<pre><code>cat &gt; ssh_trust_setup.sh &lt;&lt; 'EOF'\n#!/bin/bash\n#\nset -e\nHOSTS=(\n192.168.1.100\n192.168.1.101\n192.168.1.102\n192.168.1.103\n)\n\nUSER=cephadm\nPORT=22\n\nif ! command -v sshpass &gt;/dev/null 2&gt;&amp;1; then\n  echo \"[INFO] \u672a\u68c0\u6d4b\u5230 sshpass\uff0c\u6b63\u5728\u5b89\u88c5...\"\n  if command -v apt &gt;/dev/null 2&gt;&amp;1; then\n    sudo apt update -y &amp;&amp; apt install -y sshpass\n  elif command -v yum &gt;/dev/null 2&gt;&amp;1; then\n    sudo yum install -y epel-release &amp;&amp; sudo yum install -y sshpass\n  else\n    echo \"[ERROR] \u672a\u627e\u5230 apt \u6216 yum\uff0c\u8bf7\u624b\u52a8\u5b89\u88c5 sshpass\"\n    exit 1\n  fi\nfi\n\nif [ ! -f ~/.ssh/id_rsa ]; then\n  echo \"[INFO] \u672a\u68c0\u6d4b\u5230 SSH \u5bc6\u94a5\uff0c\u6b63\u5728\u751f\u6210...\"\n  ssh-keygen -t rsa -b 4096 -N \"\" -f ~/.ssh/id_rsa\nfi\n\necho\nread -s -p \"\u8bf7\u8f93\u5165\u8fdc\u7a0b\u4e3b\u673a (${USER}) \u767b\u5f55\u5bc6\u7801: \" PASSWORD\necho\necho \"[INFO] \u5f00\u59cb\u914d\u7f6e\u4e92\u4fe1...\"\n\nfor host in \"${HOSTS[@]}\"; do\n  echo \"-----&gt; \u6b63\u5728\u914d\u7f6e $host\"\n  sshpass -p \"$PASSWORD\" ssh-copy-id -o StrictHostKeyChecking=no -p \"$PORT\" \"${USER}@${host}\" &gt;/dev/null 2&gt;&amp;1 \\\n    &amp;&amp; echo \"[OK] $host \u4e92\u4fe1\u5efa\u7acb\u6210\u529f\" \\\n    || echo \"[FAIL] $host \u4e92\u4fe1\u5931\u8d25\"\ndone\n\necho\necho \"[DONE] \u6240\u6709\u4e3b\u673a SSH \u4e92\u4fe1\u914d\u7f6e\u5b8c\u6210\u3002\"\necho \"\u53ef\u6d4b\u8bd5\uff1assh ${USER}@${HOSTS[0]}\"\nEOF\n</code></pre>"},{"location":"storage/ceph/#7ceph","title":"7.\u914d\u7f6eceph\u6e90","text":"<pre><code>sudo yum install https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm\n</code></pre>"},{"location":"storage/ceph/#ceph-deploy","title":"\u5b89\u88c5ceph-deploy","text":"<p>\u5728\u7ba1\u7406\u8282\u70b9\u5b89\u88c5ceph-deploy\u5de5\u5177\u5305</p> <pre><code>su - cephadm\nsudo yum install ceph-deploy python-setuptools python2-subprocess32 -y\nceph-deploy --version\n2.0.1\n</code></pre>"},{"location":"storage/ceph/#ceph_1","title":"\u90e8\u7f72Ceph\u96c6\u7fa4","text":""},{"location":"storage/ceph/#1_1","title":"1.\u96c6\u7fa4\u521d\u59cb\u5316","text":"<p>\u5728\u7ba1\u7406\u8282\u70b9\u4e0a\u4ee5cephadm\u7528\u6237\u521b\u5efa\u96c6\u7fa4\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55 <pre><code>su - cephadm\nmkdir ceph-cluster\ncd ceph-cluster\nceph-deploy new --public-network 192.168.1.0/24 --cluster-network 192.168.122.0/24 ceph-node01 ceph-node02 ceph-node03 --no-ssh-copykey\n ll\n\u603b\u7528\u91cf 16\n-rw-rw-r-- 1 cephadm cephadm  323 10\u6708 21 14:00 ceph.conf\n-rw-rw-r-- 1 cephadm cephadm 7567 10\u6708 21 14:00 ceph-deploy-ceph.log\n-rw------- 1 cephadm cephadm   73 10\u6708 21 14:00 ceph.mon.keyring\ncat ceph.conf\n[global]\nfsid = d901ea94-de1d-4814-9f97-6f7ebd4329dd\npublic_network = 192.168.1.0/24\ncluster_network = 192.168.122.0/24\nmon_initial_members = ceph-node01, ceph-node02, ceph-node03\nmon_host = 192.168.1.101,192.168.1.102,192.168.1.103\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n</code></pre></p>"},{"location":"storage/ceph/#2ceph","title":"2.\u5728\u6240\u6709\u8282\u70b9\u5b89\u88c5ceph\u8f6f\u4ef6","text":"<p>ceph-deploy\u547d\u4ee4\u80fd\u591f\u4ee5\u8fdc\u7a0b\u7684\u65b9\u5f0f\u8fde\u5165Ceph\u96c6\u7fa4\u5404\u8282\u70b9\u5b8c\u6210\u7a0b\u5e8f\u5305\u5b89\u88c5\u7b49\u64cd\u4f5c\uff0c\u547d\u4ee4\u683c\u5f0f\u5982\u4e0b\uff1aceph-deploy install {ceph-node} [{ceph-node} ...]\u56e0\u6b64\uff0c\u82e5\u8981\u5c06ceph-node01\u3001ceph-node02\u548cceph-node03\u914d\u7f6e\u4e3aCeph\u96c6\u7fa4\u8282\u70b9\uff0c\u5219\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5373\u53ef\uff1a <pre><code>ceph-deploy install --nogpgcheck ceph-node01 ceph-node02 ceph-node03\n</code></pre> \u6b64\u5904\u4e3a\u4e86\u52a0\u901f\u6211\u4eec\u5728\u96c6\u7fa4\u5404\u8282\u70b9\u624b\u52a8\u5b89\u88c5ceph\u7a0b\u5e8f\u5305<code>yum install -y ceph ceph-radosgw</code></p> <p>\u7b49\u5f85\u5404\u4e2a\u96c6\u7fa4\u8282\u70b9\u7a0b\u5e8f\u5305\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u914d\u7f6e ceph-node01\uff0c ceph-node02\uff0c ceph-node03 \u4e3a ceph \u96c6\u7fa4\u8282\u70b9\uff0c\u6b64\u5904\u4e3a\u4e86\u4e0d\u8ba9 ceph-deploy \u8282\u70b9 \u518d\u6b21\u91cd\u65b0\u5b89\u88c5 ceph \u7a0b\u5e8f\uff0c\u6211\u4eec\u9700\u8981\u6dfb\u52a0\u53c2\u6570 --no-adjust-repos <pre><code>ceph-deploy install --nogpgcheck --no-adjust-repos ceph-node01 ceph-node02 ceph-node03\n</code></pre></p>"},{"location":"storage/ceph/#3monitor","title":"3.\u521d\u59cb\u5316Monitor\u8282\u70b9","text":"<p>\u5728\u8282\u70b9\u4e0a\u4f1a\u542f\u52a8\u4e00\u4e2a ceph-mon \u8fdb\u7a0b\uff0c\u5e76\u4e14\u4ee5 ceph \u7528\u6237\u8fd0\u884c\u3002\u5728 /etc/ceph \u76ee\u5f55\u4f1a\u751f\u6210\u4e00\u4e9b\u5bf9\u5e94\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d ceph.conf \u6587\u4ef6\u5c31\u662f\u4ece\u524d\u9762 ceph-cluater \u6587\u4ef6\u76f4\u63a5copy\u8fc7\u53bb\u7684\uff0c\u6b64\u6587\u4ef6\u4e5f\u53ef\u4ee5\u76f4\u63a5\u8fdb\u884c\u4fee\u6539 <pre><code>ceph-deploy mon create-initial\nfor i in ceph-node01 ceph-node02 ceph-node03;do ssh $i \"ps aux|grep ceph-mon\"|grep -v grep ; done\nceph       63884  0.3  0.8 504340 34548 ?        Ssl  14:36   0:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-node01 --setuser ceph --setgroup ceph\nceph       63551  0.3  0.8 504340 33668 ?        Ssl  14:36   0:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-node02 --setuser ceph --setgroup ceph\nceph       63598  0.2  0.9 503316 35248 ?        Ssl  14:36   0:00 /usr/bin/ceph-mon -f --cluster ceph --id ceph-node03 --setuser ceph --setgroup ceph\n</code></pre></p>"},{"location":"storage/ceph/#4_1","title":"4.\u5206\u53d1\u914d\u7f6e\u6587\u4ef6\u5230\u96c6\u7fa4\u8282\u70b9","text":"<pre><code>ceph-deploy admin ceph-node01 ceph-node02 ceph-node03\n</code></pre>"},{"location":"storage/ceph/#4manager","title":"4.\u521b\u5efaManager\u8282\u70b9","text":"<p>\u5bf9\u4e8eLuminious+\u7248\u672c \u6216\u4ee5\u540e\u7684\u7248\u672c\uff0c\u5fc5\u987b\u914d\u7f6eManager\u8282\u70b9\uff0c\u542f\u52a8ceph-mgr\u8fdb\u7a0b\uff0c\u5426\u5219ceph\u662f\u4e0d\u5065\u5eb7\u7684\u4e0d\u5b8c\u6574\u7684\u3002Ceph Manager\u5b88\u62a4\u8fdb\u7a0b\u4ee5\u201cActive/Standby\u201d\u6a21\u5f0f\u8fd0\u884c\uff0c\u90e8\u7f72\u5176\u5b83ceph-mgr\u5b88\u62a4\u7a0b\u5e8f\u53ef\u786e\u4fdd\u5728Active\u8282\u70b9\u6216\u5176\u4e0a\u7684 ceph-mgr\u5b88\u62a4\u8fdb\u7a0b\u6545\u969c\u65f6\uff0c\u5176\u4e2d\u7684\u4e00\u4e2aStandby\u5b9e\u4f8b\u53ef\u4ee5\u5728\u4e0d\u4e2d\u65ad\u670d\u52a1\u7684\u60c5\u51b5\u4e0b\u63a5\u7ba1\u5176\u4efb\u52a1\u3002 Mgr \u662f\u4e00\u4e2a\u65e0\u72b6\u6001\u7684\u670d\u52a1\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u968f\u610f\u6dfb\u52a0\u5176\u4e2a\u6570\uff0c\u901a\u5e38\u800c\u8a00\uff0c\u4f7f\u7528 2 \u4e2a\u8282\u70b9\u5373\u53ef\u3002 <pre><code>ceph-deploy  mgr create ceph-node01 ceph-node02\nfor i in ceph-node01 ceph-node02;do ssh $i \"ps aux|grep ceph-mgr\"|grep -v grep ; done\nceph       64369  5.8  3.1 1038164 123764 ?      Ssl  14:43   0:01 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-node01 --setuser ceph --setgroup ceph\nceph       63799  5.3  3.0 740804 120220 ?       Ssl  14:43   0:01 /usr/bin/ceph-mgr -f --cluster ceph --id ceph-node02 --setuser ceph --setgroup ceph\n</code></pre></p>"},{"location":"storage/ceph/#5_1","title":"5.\u914d\u7f6e\u7ba1\u7406\u8282\u70b9\u4e5f\u80fd\u67e5\u770b\u96c6\u7fa4","text":"<p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 <code>ceph -s</code> \u547d\u4ee4\u9a8c\u8bc1\uff0c \u5982\u679c\u6ca1\u6709 ceph \u547d\u4ee4\u5219\u9700\u8981\u5b89\u88c5 ceph-common \uff0c\u4e3a\u4e86\u80fd\u8ba9 ceph-admin \u4e5f\u80fd\u6267\u884cceph -s\u547d\u4ee4\uff0c\u6211\u4eec\u9700\u8981\u5b89\u88c5 ceph-common \u547d\u4ee4\uff0c\u5e76\u4e14\u901a\u8fc7 ceph-deploy admin\u63a8\u9001\u914d\u7f6e\u6587\u4ef6\u7ed9 ceph-admin,\u5e76\u8bbe\u7f6ecephadm \u5bf9 \u914d\u7f6e\u6587\u4ef6\u6709\u53ef\u8bfb\u6743\u9650</p> <pre><code>sudo yum install -y ceph-common\nceph-deploy admin ceph-admin\nsudo setfacl -m u:cephadm:r /etc/ceph/ceph.client.admin.keyring\n ceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_WARN\n            OSD count 0 &lt; osd_pool_default_size 3\n            mons are allowing insecure global_id reclaim\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 9m)\n    mgr: ceph-node01(active, since 2m), standbys: ceph-node02\n    osd: 0 osds: 0 up, 0 in\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   0 B used, 0 B / 0 B avail\n    pgs:\n# \u53ef\u9009\u64cd\u4f5c\uff0c\u5ffd\u7565\u544a\u8b66\nceph health detail\nHEALTH_WARN OSD count 0 &lt; osd_pool_default_size 3; mons are allowing insecure global_id reclaim\nTOO_FEW_OSDS OSD count 0 &lt; osd_pool_default_size 3\nAUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED mons are allowing insecure global_id reclaim\n    mon.ceph-node01 has auth_allow_insecure_global_id_reclaim set to true\n    mon.ceph-node02 has auth_allow_insecure_global_id_reclaim set to true\n    mon.ceph-node03 has auth_allow_insecure_global_id_reclaim set to true\nceph config set mon auth_allow_insecure_global_id_reclaim false\n</code></pre>"},{"location":"storage/ceph/#6osd","title":"6.\u6dfb\u52a0OSD","text":"<p>\u5728\u6b64 ceph \u96c6\u7fa4\u4e2d\uff0c\u6211\u4eec\u6bcf\u53f0\u673a\u5668\u4f7f\u7528\u4e86\u4e09\u5757\u786c\u76d8 \uff0c/dev/vda\u3001/dev/vdb\u3001/dev/vdc\uff0c \u5176\u4e2d/dev/vda\u662f\u7cfb\u7edf\u76d8\uff0c/dev/vdb\u3001/dev/vdc,\u662f\u6211\u4eec\u63a5\u4e0b\u8981\u6dfb\u52a0\u4e3a OSD \u7684\u78c1\u76d8 <pre><code>lsblk\nNAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsr0              11:0    1  1024M  0 rom\nvda             252:0    0    60G  0 disk\n\u251c\u2500vda1          252:1    0     1G  0 part /boot\n\u2514\u2500vda2          252:2    0    60G  0 part\n  \u251c\u2500centos-root 253:0    0  65.1G  0 lvm  /\n  \u2514\u2500centos-swap 253:1    0   3.9G  0 lvm  [SWAP]\nvdb             252:16   0    30G  0 disk\nvdc             252:32   0    50G  0 disk\n</code></pre> \u65e9\u671f\u7248\u672c\u7684ceph-deploy\u547d\u4ee4\u652f\u6301\u5728\u5c06\u6dfb\u52a0OSD\u7684\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u51c6\u5907OSD\u548c\u6fc0\u6d3bOSD\uff0c\u4f46\u65b0\u7248\u672c\u4e2d\uff0c\u6b64\u79cd\u64cd\u4f5c\u65b9\u5f0f\u5df2\u7ecf\u88ab\u5e9f\u9664\u3002\u6dfb\u52a0OSD\u7684\u6b65\u9aa4\u53ea\u80fd\u7531\u547d\u4ee4\u201dceph-deploy osd create {node} --data {data-disk}\u201c\u4e00\u6b21\u5b8c\u6210\uff0c\u9ed8\u8ba4\u4f7f\u7528\u7684\u5b58\u50a8\u5f15\u64ce\u4e3abluestore</p> <p><pre><code>ceph-deploy --overwrite-conf osd create ceph-node01 --data /dev/vdb\nceph-deploy --overwrite-conf osd create ceph-node01 --data /dev/vdc\nceph-deploy --overwrite-conf osd create ceph-node02 --data /dev/vdb\nceph-deploy --overwrite-conf osd create ceph-node02 --data /dev/vdc\nceph-deploy --overwrite-conf osd create ceph-node03 --data /dev/vdb\nceph-deploy --overwrite-conf osd create ceph-node03 --data /dev/vdc\nfor i in ceph-node01 ceph-node02 ceph-node03;do ssh $i \"ps aux|grep ceph-osd\"|grep -v grep ; done\nceph       64857  0.4  0.9 874180 36668 ?        Ssl  14:54   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph\nceph       65311  0.4  1.0 874184 42636 ?        Ssl  14:54   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph\nceph       64246  0.5  0.9 874184 38040 ?        Ssl  14:55   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph\nceph       64698  0.5  1.0 874180 40384 ?        Ssl  14:55   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph\nceph       64113  0.6  0.9 874180 38532 ?        Ssl  14:55   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph\nceph       64566  0.7  1.0 874184 39960 ?        Ssl  14:56   0:00 /usr/bin/ceph-osd -f --cluster ceph --id 5 --setuser ceph --setgroup ceph\n</code></pre> <pre><code> ceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_WARN\n            mons are allowing insecure global_id reclaim\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 19m)\n    mgr: ceph-node01(active, since 12m), standbys: ceph-node02\n    osd: 6 osds: 6 up (since 7s), 6 in (since 7s)\n\n  task status:\n\n  data:\n    pools:   0 pools, 0 pgs\n    objects: 0 objects, 0 B\n    usage:   6.0 GiB used, 234 GiB / 240 GiB avail\n    pgs:\n</code></pre> <pre><code>ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre></p>"},{"location":"storage/ceph/#7rgw","title":"7.\u90e8\u7f72rgw\u7528\u6765\u63d0\u4f9b\u5bf9\u8c61\u5b58\u50a8","text":"<pre><code>ceph-deploy rgw create ceph-node03\n</code></pre>"},{"location":"storage/ceph/#8-mds-cephfs","title":"8.\u90e8\u7f72 MDS \u7528\u6765\u63d0\u4f9b CephFS","text":"<pre><code>ceph-deploy mds create ceph-node01 ceph-node02\n</code></pre>"},{"location":"storage/ceph/#8","title":"8.\u90e8\u7f72\u5b8c\u6210","text":"<pre><code>ceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_WARN\n            mons are allowing insecure global_id reclaim\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 26m)\n    mgr: ceph-node01(active, since 19m), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 7m), 6 in (since 7m)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   4 pools, 128 pgs\n    objects: 187 objects, 1.2 KiB\n    usage:   6.0 GiB used, 234 GiB / 240 GiB avail\n    pgs:     128 active+clean\n ceph health detail\nHEALTH_WARN mons are allowing insecure global_id reclaim\nAUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED mons are allowing insecure global_id reclaim\n    mon.ceph-node01 has auth_allow_insecure_global_id_reclaim set to true\n    mon.ceph-node02 has auth_allow_insecure_global_id_reclaim set to true\n    mon.ceph-node03 has auth_allow_insecure_global_id_reclaim set to true\nceph config set mon auth_allow_insecure_global_id_reclaim false\nceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 28m)\n    mgr: ceph-node01(active, since 21m), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 8m), 6 in (since 8m)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   4 pools, 128 pgs\n    objects: 187 objects, 1.2 KiB\n    usage:   6.0 GiB used, 234 GiB / 240 GiB avail\n    pgs:     128 active+clean\n</code></pre>"},{"location":"storage/cephfs/","title":"CephFS\u6587\u4ef6\u7cfb\u7edf","text":"<p>\u6587\u4ef6\u7cfb\u7edf\u4e3b\u8981\u7528\u4e8e\u591a\u5ba2\u6237\u7aef\u64cd\u4f5c\u540c\u4e00\u4e2a\u6587\u4ef6\uff0c</p>"},{"location":"storage/cephfs/#cephfs_1","title":"\u521d\u59cb\u5316 CephFS \u6587\u4ef6\u7cfb\u7edf","text":"<pre><code># \u521b\u5efa\u5143\u6570\u636e\u6c60\uff08Metadata Pool\uff09\n~]$ ceph osd pool create cephfs-metadata 64 64\npool 'cephfs-metadata' created\n# \u521b\u5efa\u6570\u636e\u6c60\uff08Data Pool\uff09\n~]$ ceph osd pool create cephfs-data  128 128\npool 'cephfs-data' created\n# \u521b\u5efa\u6587\u4ef6\u7cfb\u7edf\n~]$ ceph fs new cephfs cephfs-metadata cephfs-data\n# \u67e5\u770b\u6587\u4ef6\u7cfb\u7edf\u72b6\u6001\n~]$ ceph fs status\ncephfs - 0 clients\n======\n+------+--------+-------------+---------------+-------+-------+\n| Rank | State  |     MDS     |    Activity   |  dns  |  inos |\n+------+--------+-------------+---------------+-------+-------+\n|  0   | active | ceph-node01 | Reqs:    0 /s |   10  |   13  |\n+------+--------+-------------+---------------+-------+-------+\n+-----------------+----------+-------+-------+\n|       Pool      |   type   |  used | avail |\n+-----------------+----------+-------+-------+\n| cephfs-metadata | metadata | 1536k | 73.0G |\n|   cephfs-data   |   data   |    0  | 73.0G |\n+-----------------+----------+-------+-------+\n+-------------+\n| Standby MDS |\n+-------------+\n| ceph-node02 |\n+-------------+\nMDS version: ceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable)\n</code></pre>"},{"location":"storage/cephfs/#cephfs_2","title":"\u6302\u8f7dCephfs\u7cfb\u7edf","text":""},{"location":"storage/cephfs/#2ceph","title":"2.Ceph \u539f\u751f\u5ba2\u6237\u7aef","text":""},{"location":"storage/cephfs/#21","title":"2.1.\u51c6\u5907\u8ba4\u8bc1\u6587\u4ef6","text":"<pre><code>~]$ ceph auth get-or-create client.fsclient mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfs-data' -o ceph.client.fsclient.keyring\n~]$ ceph auth get-key client.fsclient -o fsclient.key\n#\u590d\u5236\u5230\u5ba2\u6237\u673a\u4e0a\n~]$ scp ceph.client.fsclient.keyring fsclient.key root@192.168.1.109:/etc/ceph\n</code></pre>"},{"location":"storage/cephfs/#22-cephfs","title":"2.2 \u6302\u8f7dcephFS","text":"<p><pre><code> ~]# mount -t ceph ceph-node01:6789,ceph-node02:6789,ceph-node03:6789:/ /mnt -o name=fsclient,secretfile=/etc/ceph/fsclient.key\n[root@ceph-client01 ~]# df -h\nFilesystem                                                  Size  Used Avail Use% Mounted on\n...\n192.168.1.101:6789,192.168.1.102:6789,192.168.1.103:6789:/   74G     0   74G   0% /mnt\n~]# cd /mnt/\nmnt]# touch 1111\n~]# umount /mnt\n~]# mkdir /cephdata\n~]# mount -t ceph ceph-node01:6789,ceph-node02:6789,ceph-node03:6789:/ /cephdata/ -o name=fsclient,secretfile=/etc/ceph/fsclient.key\n~]# ls /cephdata/\n</code></pre> <pre><code>vim /etc/fstab\nceph-node01,ceph-node02,ceph-node03:/  /cephdata  ceph  name=fsclient,secretfile=/etc/ceph/fsclient.key,_netdev,noatime  0  0\n</code></pre></p>"},{"location":"storage/cephfs/#3fuse","title":"3.FUSE\u5b9e\u8df5","text":"<p>\u5bf9\u4e8e\u67d0\u4e9b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u672c\u8eab\u6ca1\u6709\u652f\u6301ceph\u5185\u6838\u6a21\u5757\uff0c\u6b64\u65f6\u5982\u679c\u8fd8\u9700\u8981\u4f7f\u7528cephfs\uff0c\u53ef\u4ee5\u901a\u8fc7fuse\u65b9\u5f0f\u6765\u5b9e\u73b0\u3002FUSE\u5168\u79f0FileSystem in Usespace\u3002\u7528\u4e8e\u98de\u7279\u6743\u7528\u6237\u80fd\u591f\u65e0\u9700\u64cd\u4f5c\u5185\u6838\u800c\u521b\u5efa\u6587\u4ef6\u7cfb\u7edf</p>"},{"location":"storage/cephfs/#31ceh-fuse","title":"3.1\u3002\u5b89\u88c5ceh-fuse\u8f6f\u4ef6\u5305","text":"<pre><code>~]# yum install ceph-fuse ceph-common\n</code></pre>"},{"location":"storage/cephfs/#32","title":"3.2.\u83b7\u53d6\u6388\u6743\u6587\u4ef6","text":"<pre><code>~]$ ceph auth get-or-create client.fsclient mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfs-data' -o ceph.client.fsclient.keyring\n~]$ ceph auth get-key client.fsclient -o fsclient.key\n#\u590d\u5236\u5230\u5ba2\u6237\u673a\u4e0a\n~]$ scp ceph.client.fsclient.keyring fsclient.key root@192.168.1.109:/etc/ceph\n</code></pre>"},{"location":"storage/cephfs/#33cephfs","title":"3.3.\u6302\u8f7dcephfs","text":"<pre><code>~]# ceph-fuse -n client.fsclient -m ceph-node01:6789,ceph-node02:6789,ceph-node03:6789 /mnt\nceph-fuse[15672]: starting ceph client2025-10-22 11:23:26.513 7f96ae88ff80 -1 init, newargv = 0x557c4b0e6a70 newargc=9\n\nceph-fuse[15672]: starting fuse\n~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\n...\nceph-fuse                 74G     0   74G   0% /mnt\n</code></pre>"},{"location":"storage/cluster/","title":"Ceph Cluster\u96c6\u7fa4\u7ba1\u7406","text":""},{"location":"storage/cluster/#_1","title":"\u96c6\u7fa4\u72b6\u6001","text":""},{"location":"storage/cluster/#1","title":"1.\u68c0\u67e5\u96c6\u7fa4\u72b6\u6001","text":"<pre><code>~]$ ceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 20h)\n    mgr: ceph-node01(active, since 20h), standbys: ceph-node02\n    mds: cephfs:1 {0=ceph-node01=up:active} 1 up:standby\n    osd: 6 osds: 6 up (since 19h), 6 in (since 19h)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   10 pools, 416 pgs\n    objects: 336 objects, 168 MiB\n    usage:   6.5 GiB used, 233 GiB / 240 GiB avail\n    pgs:     416 active+clean\n</code></pre>"},{"location":"storage/cluster/#2pg","title":"2.pg\u72b6\u6001\u67e5\u770b","text":"<pre><code>~]$ ceph pg stat\n416 pgs: 416 active+clean; 168 MiB data, 549 MiB used, 233 GiB / 240 GiB avail\n</code></pre>"},{"location":"storage/cluster/#3","title":"3.\u5b58\u50a8\u7a7a\u95f4","text":"<pre><code>~]$ ceph df [detail]\nRAW STORAGE:\n    CLASS     SIZE        AVAIL       USED        RAW USED     %RAW USED\n    hdd       240 GiB     233 GiB     549 MiB      6.5 GiB          2.72\n    TOTAL     240 GiB     233 GiB     549 MiB      6.5 GiB          2.72\n\nPOOLS:\n    POOL                          ID     PGS     STORED      OBJECTS     USED        %USED     MAX AVAIL\n    .rgw.root                      1      32     1.2 KiB           4     768 KiB         0        73 GiB\n    default.rgw.control            2      32         0 B           8         0 B         0        73 GiB\n    default.rgw.meta               3      32     2.1 KiB          11     1.9 MiB         0        73 GiB\n    default.rgw.log                4      32         0 B         207         0 B         0        73 GiB\n    kube                           7      16     129 MiB          50     391 MiB      0.17        73 GiB\n    kvmpool                        8      16      13 MiB          18      48 MiB      0.02        73 GiB\n    default.rgw.buckets.index      9      32         0 B           4         0 B         0        73 GiB\n    default.rgw.buckets.data      10      32     5.7 KiB          11     2.1 MiB         0        73 GiB\n    cephfs-metadata               11      64      18 KiB          22     1.5 MiB         0        73 GiB\n    cephfs-data                   12     128         4 B           1     192 KiB         0        73 GiB\n</code></pre>"},{"location":"storage/cluster/#4osd","title":"4.OSD\u72b6\u6001","text":"<pre><code>~]$ ceph osd stat\n6 osds: 6 up (since 19h), 6 in (since 19h); epoch: e134\n~]$ ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre>"},{"location":"storage/cluster/#5mon","title":"5.MON\u72b6\u6001","text":"<pre><code>~]$ ceph mon stat\ne2: 3 mons at {ceph-node01=[v2:192.168.1.101:3300/0,v1:192.168.1.101:6789/0],ceph-node02=[v2:192.168.1.102:3300/0,v1:192.168.1.102:6789/0],ceph-node03=[v2:192.168.1.103:3300/0,v1:192.168.1.103:6789/0]}, election epoch 8, leader 0 ceph-node01, quorum 0,1,2 ceph-node01,ceph-node02,ceph-node03\n~]$ ceph mon dump\nepoch 2\nfsid d901ea94-de1d-4814-9f97-6f7ebd4329dd\nlast_changed 2025-10-21 14:36:37.597634\ncreated 2025-10-21 14:36:14.439192\nmin_mon_release 14 (nautilus)\n0: [v2:192.168.1.101:3300/0,v1:192.168.1.101:6789/0] mon.ceph-node01\n1: [v2:192.168.1.102:3300/0,v1:192.168.1.102:6789/0] mon.ceph-node02\n2: [v2:192.168.1.103:3300/0,v1:192.168.1.103:6789/0] mon.ceph-node03\ndumped monmap epoch 2\n# \u9009\u4e3e\u72b6\u6001\n~]$ ceph quorum_status [-f json-pretty]\n</code></pre>"},{"location":"storage/cluster/#_2","title":"\u7ba1\u7406\u5957\u63a5\u5b57","text":"<p>ceph\u7684\u7ba1\u7406\u5957\u63a5\u5b57\uff0c\u901a\u5e38\u7528\u4e8e\u67e5\u8be2\u5b88\u62a4\u8fdb\u7a0b\uff0c\u8d44\u6e90\u5bf9\u8c61\u6587\u4ef6\u4fdd\u5b58\u5728/var/lib/ceph\u76ee\u5f55\uff0c\u5957\u63a5\u5b57\u9ed8\u8ba4\u4fdd\u5b58\u5728/var/run/ceph/\u76ee\u5f55\uff0c\u53ea\u80fd\u672c\u5730\u4f7f\u7528\u3002</p> <pre><code>~]# ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok status\n{\n    \"cluster_fsid\": \"d901ea94-de1d-4814-9f97-6f7ebd4329dd\",\n    \"osd_fsid\": \"d04abe1d-abb7-4ef9-8235-ffbfea2529cf\",\n    \"whoami\": 0,\n    \"state\": \"active\",\n    \"oldest_map\": 1,\n    \"newest_map\": 134,\n    \"num_pgs\": 161\n}\n~]# ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-node01.asok config show\n~]# ceph --admin-daemon /var/run/ceph/ceph-mon.ceph-node01.asok config get xio_mp_max_page\n{\n    \"xio_mp_max_page\": \"4096\"\n}\n</code></pre>"},{"location":"storage/cluster/#_3","title":"\u914d\u7f6e\u6587\u4ef6","text":""},{"location":"storage/cluster/#1_1","title":"1.\u6587\u4ef6\u683c\u5f0f","text":"<pre><code>~]# cat /etc/ceph/ceph.conf\n[global]\nfsid = d901ea94-de1d-4814-9f97-6f7ebd4329dd\npublic_network = 192.168.1.0/24\ncluster_network = 192.168.122.0/24\nmon_initial_members = ceph-node01, ceph-node02, ceph-node03\nmon_host = 192.168.1.101,192.168.1.102,192.168.1.103\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n[mon]\nmon allow_pool_delete = true\n[mon.mon01]\n[osd]\n...\n[osd.1]\n...\n[mgr]\n..\n[client]\n...\n</code></pre>"},{"location":"storage/cluster/#2","title":"2.\u6587\u4ef6\u52a0\u8f7d","text":"<p>\u5168\u5c40\u914d\u7f6e\u6587\u4ef6</p> <ul> <li>/etc/ceph/ceph.conf</li> <li>export CEPH_CONF=xxx</li> <li>-c path/to/conf</li> </ul> <p>\u5c40\u90e8\u914d\u7f6e\u6587\u4ef6</p> <ul> <li>\uff5e/.ceph.config</li> <li>./ceph/conf</li> </ul>"},{"location":"storage/cluster/#3_1","title":"3.\u5b9e\u8df5","text":"<pre><code># \u83b7\u53d6\u6240\u6709\u5c5e\u6027\n~]# ceph daemon osd.0 config show\n# \u83b7\u53d6\u5355\u4e2a\u5c5e\u6027\n~]# ceph daemon osd.0 config get target_max_misplaced_ratio\n{\n    \"target_max_misplaced_ratio\": \"0.050000\"\n}\n# \u5220\u9664\u5b58\u50a8\u6c60\n~]$ ceph osd pool rm mypool mypool --yes-i-really-really-mean-it\nError EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool\n~]$ ceph config set mon   mon_allow_pool_delete true\n~]$ ceph osd pool rm mypool mypool --yes-i-really-really-mean-it\npool 'mypool' removed\n~]$ ceph config set mon   mon_allow_pool_delete false\n</code></pre>"},{"location":"storage/cluster/#_4","title":"\u78c1\u76d8\u7ba1\u7406","text":"<p>\u89c1OSD\u8282\u70b9\u64cd\u4f5c</p>"},{"location":"storage/cluster/#_5","title":"\u6027\u80fd\u8c03\u4f18","text":"<p>\u6027\u80fd\u8c03\u4f18\u5c31\u662f\u5728\u73b0\u6709\u4e3b\u673a\u8d44\u6e90\u524d\u63d0\u4e0b\uff0c\u53d1\u6325\u4e1a\u52a1\u4f5c\u7b54\u7684\u5904\u7406\u80fd\u529b\u3002\u901a\u8fc7\u7a7a\u95f4\u6362\u5b9e\u8df5\uff0c\u4f18\u5316\u4e1a\u52a1\u5904\u7406\u6548\u76ca</p> <ul> <li>\u6539\u5584\u4e1a\u52a1\u903b\u8f91\u5904\u7406\u901f\u5ea6</li> <li>\u63d0\u5347\u4e1a\u52a1\u541e\u5410\u91cf</li> <li>\u4f18\u5316\u4e3b\u673a\u6d88\u8017\u91cf</li> </ul> \u901a\u7528\u63aa\u65bd \u7c7b\u578b \u8bf4\u660e \u57fa\u7840\u8bbe\u65bd \u5408\u9002\u8bbe\u5907 \u591a\u4e91\u73af\u5883\u3001\u65b0\u65e7\u670d\u52a1\u5668\u7ed3\u5408\u3001\u5408\u9002\u7f51\u7edc \u7814\u53d1\u73af\u5883 \u5e94\u7528\u67b6\u6784\u3001\u7814\u53d1\u5e73\u53f0\u3001\u4ee3\u7801\u89c4\u8303 \u5e94\u7528\u8f6f\u4ef6 \u591a\u7ea7\u7f13\u5b58 \u6d4f\u89c8\u5668\u7f13\u5b58\u3001\u7f13\u5b58\u670d\u52a1\u5668\u3001\u670d\u52a1\u5668\u7f13\u5b58\uff0c\u5e94\u7528\u7f13\u5b58\u3001\u4ee3\u7801\u7f13\u5b58\u3001\u6570\u636e\u7f13\u5b58 \u6570\u636e\u538b\u7f29 \u6784\u5efa\u538b\u7f29\u3001\u4f20\u8f93\u96c5\u4fd7\u3001\u7f13\u5b58\u538b\u7f29\u3001\u5bf9\u8c61\u538b\u7f29\uff0c\u6e05\u7406\u65e0\u6548\u6570\u636e \u6570\u636e\u9884\u70ed \u7f13\u51b2\u6570\u636e\u3011\u9884\u53d6\u6570\u636e\u3001\u9884\u5236\u4e3b\u673a\u3001\u6570\u636e\u540c\u6b65 \u4e1a\u52a1\u5904\u7406 \u524a\u5cf0\u586b\u8c37 \u5ef6\u65f6\u52a0\u8f7d\u3001\u7684\u6279\u53d1\u90e8\u3001\u9650\u6d41\u63a7\u5236\u3001\u5f02\u6b65\u5904\u7406\u3001\u8d85\u65f6\u964d\u7ea7 \u4efb\u52a1\u6279\u5904\u7406 \u6570\u636e\u6253\u5305\u4f20\u8f93\u3001\u6279\u91cf\u4f20\u8f93\u3001\u5ef6\u8fdf\u6570\u636e\u5904\u7406"},{"location":"storage/cluster/#1_2","title":"1.\u5e38\u89c1\u7b56\u7565","text":"<p>ceph\u96c6\u7fa4\u5404\u4e2a\u670d\u52a1\u5b88\u62a4\u8fdb\u7a0b\u5728Linux\u4e3b\u673a\u8fd0\u884c\uff0c\u6240\u4ee5\u5b9e\u5f39\u4f18\u5316\u64cd\u4f5c\u7cfb\u7edf\u80fd\u591f\u5bf9ceph\u6027\u80fd\u4ea7\u751f\u79ef\u6781\u7684\u5f71\u54cd</p> <ul> <li>\u9009\u62e9\u5408\u9002\u7684CPU\u548c\u5185\u5b58: \u4e0d\u540c\u89d2\u8272\u5177\u6709\u4e0d\u540c\u7684CPU\u9700\u6c42\uff0c\u5185\u5b58\u9700\u6c42</li> <li>\u9009\u62e9\u5408\u9002\u7684\u78c1\u76d8\u5bb9\u91cf\uff1a\u8ba1\u7b97\u8282\u70b9\u6570\u91cf\u78c1\u76d8\u5bb9\u91cf\u9700\u6c42\uff0c\u9009\u62e9\u5408\u9002\u7684SAS\uff0cSATA\uff0cSSD\u5b9e\u73b0\u5206\u7ea7\u5b58\u50a8</li> <li>\u9009\u62e9\u5408\u9002\u7684\u7f51\u7edc\uff1aceph\u96c6\u7fa4\u5bf9\u7f51\u7edc\u4f20\u8f93\u80fd\u529b\u8981\u6c42\u9ad8\uff0c\u9700\u8981\u652f\u6301\u5de8\u578b\u6862\uff0c\u6240\u4ee5\u7f51\u7edc\u5e26\u5bbd\u5c3d\u91cf\u5927\u6700\u597d</li> <li>\u914d\u7f6e\u5408\u9002\u7684\u6587\u4ef6\u7cfb\u7edf</li> <li>\u642d\u5efa\u5185\u90e8\u65f6\u95f4\u670d\u52a1\u5668</li> <li>\u5408\u7406\u91c7\u7528\u591a\u4e91\u67b6\u6784\uff1a\u5c06\u5408\u9002\u7684\u4e1a\u52a1\u8fc1\u79fb\u5230\u516c\u6709\u4e91\u73af\u5883</li> </ul>"},{"location":"storage/cluster/#_6","title":"\u6027\u80fd\u6d4b\u8bd5","text":""},{"location":"storage/cluster/#_7","title":"\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"storage/cluster/#1_3","title":"1.\u78c1\u76d8\u6d4b\u8bd5","text":"<pre><code># \u6e05\u7406\u7f13\u5b58\n~]# echo 3 &gt; /proc/sys/vm/drop_caches\n# \u5199\u6027\u80fd\n~]# dd if=/dev/zero of=/var/lib/ceph/osd/ceph-0/write_test bs=1M count=1024\n\u8bb0\u5f55\u4e861024+0 \u7684\u8bfb\u5165\n\u8bb0\u5f55\u4e861024+0 \u7684\u5199\u51fa\n1073741824\u5b57\u8282(1.1 GB)\u5df2\u590d\u5236\uff0c2.74816 \u79d2\uff0c391 MB/\u79d2\n# \u8bfb\u6027\u80fd\n~]# dd if=/var/lib/ceph/osd/ceph-0/write_test of=/dev/null bs=1M count=1024\n\u8bb0\u5f55\u4e861024+0 \u7684\u8bfb\u5165\n\u8bb0\u5f55\u4e861024+0 \u7684\u5199\u51fa\n1073741824\u5b57\u8282(1.1 GB)\u5df2\u590d\u5236\uff0c0.566816 \u79d2\uff0c1.9 GB/\u79d2\n# \u6e05\u7406\u73af\u5883\n~]# rm -rf /var/lib/ceph/osd/ceph-0/write_test\n</code></pre>"},{"location":"storage/cluster/#2_1","title":"2.\u7f51\u7edc\u6d4b\u8bd5","text":"<pre><code>~]# yum install iperf -y\n# \u542f\u52a8\u670d\u52a1\u7aef\n~]# iperf -s -p 6090\n------------------------------------------------------------\nServer listening on TCP port 6090\nTCP window size: 85.3 KByte (default)\n------------------------------------------------------------\n# \u5ba2\u6237\u7aef\u6d4b\u8bd5\n~]# iperf -c ceph-node01 -p 6090\n------------------------------------------------------------\nClient connecting to ceph-node01, TCP port 6090\nTCP window size:  493 KByte (default)\n------------------------------------------------------------\n[  3] local 192.168.1.102 port 49794 connected with 192.168.1.101 port 6090\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0-10.0 sec  24.0 GBytes  20.7 Gbits/sec\n</code></pre>"},{"location":"storage/cluster/#rados","title":"rados\u6d4b\u8bd5","text":""},{"location":"storage/cluster/#1bench","title":"1.bench\u6027\u80fd\u5de5\u5177","text":"<pre><code>~]# ceph osd pool create testpool 32 32\n# \u5199\u6d4b\u8bd5\n~]# rados bench -p testpool 10 write --no-cleanup\nhints = 1\nMaintaining 16 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 10 seconds or 0 objects\nObject prefix: benchmark_data_ceph-node02_75493\n  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)\n    0       0         0         0         0         0           -           0\n    1      16        47        31   123.957       124    0.339039     0.26998\n    2      16        48        32    63.979         4    0.926563    0.290498\n    3      16        51        35   46.6536        12     2.37035    0.474669\n    4      16        62        46   45.9874        44     2.54365    0.970419\n    5      16        80        64   51.1822        72     1.86982     1.13235\n    6      16        95        79   52.6495        60    0.829198     1.11541\n    7      16       107        91   51.9836        48     1.70162     1.09511\n    8      16       123       107   53.4834        64     1.65955     1.11566\n    9      16       139       123   54.6504        64     1.26755     1.10312\n   10      16       155       139   55.5842        64     1.03061     1.08782\nTotal time run:         10.5233\nTotal writes made:      156\nWrite size:             4194304\nObject size:            4194304\nBandwidth (MB/sec):     59.2968\nStddev Bandwidth:       33.2238\nMax bandwidth (MB/sec): 124\nMin bandwidth (MB/sec): 4\nAverage IOPS:           14\nStddev IOPS:            8.30596\nMax IOPS:               31\nMin IOPS:               1\nAverage Latency(s):     1.07799\nStddev Latency(s):      0.676048\nMax latency(s):         3.48482\nMin latency(s):         0.0520137\n# \u8bfb\u6d4b\u8bd5\n~]# rados bench -p testpool 10 seq\nhints = 1\n  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)\n    0       1         1         0         0         0           -           0\nTotal time run:       0.854894\nTotal reads made:     156\nRead size:            4194304\nObject size:          4194304\nBandwidth (MB/sec):   729.915\nAverage IOPS:         182\nStddev IOPS:          0\nMax IOPS:             155\nMin IOPS:             155\nAverage Latency(s):   0.0858158\nMax latency(s):       0.295643\nMin latency(s):       0.00947855\n# \u968f\u673a\u8bfb\n~]# rados bench -p testpool 10 rand\nhints = 1\n  sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)\n    0       0         0         0         0         0           -           0\n    1      16       265       249   995.125       996    0.049809    0.060605\n    2      16       557       541   1081.33      1168   0.0125041   0.0563806\n    3      16       850       834   1111.23      1172   0.0137341   0.0559525\n    4      16      1146      1130   1129.21      1184   0.0858852   0.0556404\n    5      16      1449      1433   1145.54      1212     0.12376   0.0549243\n    6      16      1756      1740   1159.14      1228   0.0154079   0.0539604\n    7      15      2063      2048   1169.37      1232     0.10969   0.0539128\n    8      16      2364      2348   1173.05      1200   0.0504233    0.053643\n    9      16      2666      2650   1176.88      1208   0.0920287   0.0535442\n   10      16      2966      2950   1179.06      1200   0.0244516   0.0534336\nTotal time run:       10.0675\nTotal reads made:     2966\nRead size:            4194304\nObject size:          4194304\nBandwidth (MB/sec):   1178.44\nAverage IOPS:         294\nStddev IOPS:          17.0163\nMax IOPS:             308\nMin IOPS:             249\nAverage Latency(s):   0.053714\nMax latency(s):       0.244274\nMin latency(s):       0.00392119\n# \u6e05\u7406\u6570\u636e\n~]# rados -p testpool cleanup\nRemoved 156 objects\n</code></pre>"},{"location":"storage/cluster/#_8","title":"\u541e\u5410\u91cf\u6d4b\u8bd5","text":"<pre><code>~]# rados -p testpool load-gen --number-objects 50 --min-object-size 4M --max-object-size 4M --max-ops 16 --min-op-len 4M --max-op=len 4M --percent 5 --target-throughput 4M --run-length 10\nrun length 10 seconds\npreparing 200 objects\nload-gen will run 10 seconds\n    1: throughput=0MB/sec pending data=0\nREAD : oid=obj-Udh-Weeu_MUOLwS off=0 len=4194304\nREAD : oid=obj-w3kIYN5wOlX9pg2 off=0 len=4194304\nop 0 completed, throughput=3.92MB/sec\nop 1 completed, throughput=7.79MB/sec\n    2: throughput=3.95MB/sec pending data=0\nREAD : oid=obj-qe8nOCcIUI7EeNe off=0 len=4194304\nop 2 completed, throughput=5.86MB/sec\n    3: throughput=3.94MB/sec pending data=0\nWRITE : oid=obj-eh7WVeyYyCNSwT7 off=0 len=4194304\nop 3 completed, throughput=5.13MB/sec\n    4: throughput=3.88MB/sec pending data=0\nREAD : oid=obj-QCAn1qwZznVQ9un off=0 len=4194304\nop 4 completed, throughput=4.82MB/sec\n    5: throughput=3.88MB/sec pending data=0\nREAD : oid=obj-1DbQRDFIgSNsPrf off=0 len=4194304\nop 5 completed, throughput=4.63MB/sec\n    6: throughput=3.88MB/sec pending data=0\nREAD : oid=obj-Ramd9ZG0VCgviYr off=0 len=4194304\nop 6 completed, throughput=4.52MB/sec\n    7: throughput=3.89MB/sec pending data=0\nWRITE : oid=obj-tMeI0yyhPOqrMgo off=0 len=4194304\nop 7 completed, throughput=4.41MB/sec\n    8: throughput=3.88MB/sec pending data=0\nREAD : oid=obj-i3suoA1HJ_h-pYv off=0 len=4194304\nop 8 completed, throughput=4.35MB/sec\n    9: throughput=3.88MB/sec pending data=0\nWRITE : oid=obj-cIXMhiIlVBIY5Oh off=0 len=4194304\nop 9 completed, throughput=4.28MB/sec\nwaiting for all operations to complete\ncleaning up objects\n</code></pre>"},{"location":"storage/mon/","title":"Monitor\u76d1\u63a7\u5668","text":""},{"location":"storage/mon/#mon","title":"mon\u6dfb\u52a0","text":"<pre><code>ceph-deploy mon add mon\u8282\u70b9\u540d\u79f0\n</code></pre>"},{"location":"storage/mon/#mon_1","title":"mon\u5220\u9664","text":"<pre><code>ceph-deploy mon destroy mon\u8282\u70b9\u540d\u79f0\n</code></pre>"},{"location":"storage/osd/","title":"Object Storage \u5bf9\u8c61\u5b58\u50a8","text":"<p>OSD \u5168\u79f0 Object Storage Device\uff0c\u8d1f\u8d23\u54cd\u5e94\u5ba2\u6237\u7aef\u8bf7\u6c42\uff0c\u8fd4\u56de\u5177\u4f53\u6570\u636e\u7684\u8fdb\u7a0b\uff0cCeph\u96c6\u7fa4\u4e2d\u4e00\u822c\u901a\u8fc7\u4e13\u95e8\u7684\u4e3b\u673a\u63d0\u4f9bOSD\u4e70\u8fd9\u4e2a\u4e3b\u673a\u4e0a\u7684\u591a\u4e2a\u78c1\u76d8\u90fd\u53ef\u6dfb\u52a0\u4e3aOSD\u8bbe\u5907</p>"},{"location":"storage/osd/#_1","title":"\u547d\u4ee4\u67e5\u770b","text":""},{"location":"storage/osd/#1osdid","title":"1.\u67e5\u770b\u6240\u6709OSD\u7684ID\u503c","text":"<pre><code>ceph osd ls\n0\n1\n2\n3\n4\n5\n</code></pre>"},{"location":"storage/osd/#2osd","title":"2.\u67e5\u770bOSD\u6982\u8ff0\u4fe1\u606f","text":"<pre><code>ceph osd dump\nepoch 34\nfsid d901ea94-de1d-4814-9f97-6f7ebd4329dd\ncreated 2025-10-21 14:36:36.240960\nmodified 2025-10-21 15:00:36.907145\nflags sortbitwise,recovery_deletes,purged_snapdirs,pglog_hardlimit\ncrush_version 13\nfull_ratio 0.95\nbackfillfull_ratio 0.9\nnearfull_ratio 0.85\nrequire_min_compat_client jewel\nmin_compat_client jewel\nrequire_osd_release nautilus\npool 1 '.rgw.root' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 28 flags hashpspool stripe_width 0 application rgw\npool 2 'default.rgw.control' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 30 flags hashpspool stripe_width 0 application rgw\npool 3 'default.rgw.meta' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 32 flags hashpspool stripe_width 0 application rgw\npool 4 'default.rgw.log' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 34 flags hashpspool stripe_width 0 application rgw\nmax_osd 6\nosd.0 up   in  weight 1 up_from 5 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.101:6802/64857,v1:192.168.1.101:6803/64857] [v2:192.168.122.101:6800/64857,v1:192.168.122.101:6801/64857] exists,up d04abe1d-abb7-4ef9-8235-ffbfea2529cf\nosd.1 up   in  weight 1 up_from 9 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.101:6806/65311,v1:192.168.1.101:6807/65311] [v2:192.168.122.101:6804/65311,v1:192.168.122.101:6805/65311] exists,up c487e57b-68f3-4598-a1dd-2cd1ebb59d90\nosd.2 up   in  weight 1 up_from 13 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.102:6800/64246,v1:192.168.1.102:6801/64246] [v2:192.168.122.102:6800/64246,v1:192.168.122.102:6801/64246] exists,up 3134cec2-6f22-472e-8787-e18d7d77e641\nosd.3 up   in  weight 1 up_from 17 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.102:6804/64698,v1:192.168.1.102:6805/64698] [v2:192.168.122.102:6804/64698,v1:192.168.122.102:6805/64698] exists,up 6145a6c0-9916-4dfd-a04c-93e7cc460da4\nosd.4 up   in  weight 1 up_from 21 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.103:6800/64113,v1:192.168.1.103:6801/64113] [v2:192.168.122.103:6800/64113,v1:192.168.122.103:6801/64113] exists,up 1490030e-420b-493e-b817-515ef0aec597\nosd.5 up   in  weight 1 up_from 25 up_thru 32 down_at 0 last_clean_interval [0,0) [v2:192.168.1.103:6804/64566,v1:192.168.1.103:6805/64566] [v2:192.168.122.103:6804/64566,v1:192.168.122.103:6805/64566] exists,up a3d688ed-4432-4d0e-b12d-d75f82f70b52\n</code></pre>"},{"location":"storage/osd/#3osd","title":"3.osd\u76f8\u4fe1\u72b6\u6001\u4fe1\u606f","text":"<pre><code> ceph osd status\n+----+-------------+-------+-------+--------+---------+--------+---------+-----------+\n| id |     host    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |\n+----+-------------+-------+-------+--------+---------+--------+---------+-----------+\n| 0  | ceph-node01 | 1028M | 28.9G |    0   |     0   |    0   |     0   | exists,up |\n| 1  | ceph-node01 | 1028M | 48.9G |    0   |     0   |    0   |     0   | exists,up |\n| 2  | ceph-node02 | 1028M | 28.9G |    0   |     0   |    0   |     0   | exists,up |\n| 3  | ceph-node02 | 1028M | 48.9G |    0   |     0   |    0   |     0   | exists,up |\n| 4  | ceph-node03 | 1028M | 28.9G |    0   |     0   |    0   |     0   | exists,up |\n| 5  | ceph-node03 | 1028M | 48.9G |    0   |     0   |    0   |     0   | exists,up |\n+----+-------------+-------+-------+--------+---------+--------+---------+-----------+\nceph osd stat\n6 osds: 6 up (since 19m), 6 in (since 19m); epoch: e34\n</code></pre>"},{"location":"storage/osd/#4osd","title":"4.\u67e5\u770bosd\u5728\u4e3b\u673a\u7684\u5206\u5e03\u4fe1\u606f","text":"<pre><code>ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre>"},{"location":"storage/osd/#5osd","title":"5.osd\u5ef6\u8fdf\u7edf\u8ba1\u4fe1\u606f","text":"<pre><code>ceph osd perf\nosd commit_latency(ms) apply_latency(ms)\n  5                  0                 0\n  4                  0                 0\n  0                  0                 0\n  1                  0                 0\n  2                  0                 0\n  3                  0                 0\n</code></pre>"},{"location":"storage/osd/#5osd_1","title":"5.osd\u78c1\u76d8\u4f7f\u7528\u7387\u4fe1\u606f","text":"<pre><code>ceph osd df\nID CLASS WEIGHT  REWEIGHT SIZE    RAW USE DATA    OMAP META  AVAIL   %USE VAR  PGS STATUS\n 0   hdd 0.02930  1.00000  30 GiB 1.0 GiB 4.4 MiB  0 B 1 GiB  29 GiB 3.35 1.33  51     up\n 1   hdd 0.04880  1.00000  50 GiB 1.0 GiB 4.6 MiB  0 B 1 GiB  49 GiB 2.01 0.80  77     up\n 2   hdd 0.02930  1.00000  30 GiB 1.0 GiB 4.4 MiB  0 B 1 GiB  29 GiB 3.35 1.33  47     up\n 3   hdd 0.04880  1.00000  50 GiB 1.0 GiB 4.6 MiB  0 B 1 GiB  49 GiB 2.01 0.80  81     up\n 4   hdd 0.02930  1.00000  30 GiB 1.0 GiB 4.6 MiB  0 B 1 GiB  29 GiB 3.35 1.33  49     up\n 5   hdd 0.04880  1.00000  50 GiB 1.0 GiB 4.4 MiB  0 B 1 GiB  49 GiB 2.01 0.80  79     up\n                    TOTAL 240 GiB 6.0 GiB  27 MiB  0 B 6 GiB 234 GiB 2.51\nMIN/MAX VAR: 0.80/1.33  STDDEV: 0.69\n</code></pre>"},{"location":"storage/osd/#_2","title":"\u6682\u505c\u548c\u5f00\u542f","text":"<p>\u6682\u505c\u63a5\u53d7\u6570\u636e\uff1a<code>ceph osd pause</code> \u5f00\u59cb\u63a5\u6536\u6570\u636e\uff1a<code>ceph osd unpause</code></p> <pre><code>ceph osd pause\npauserd,pausewr is set\nceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_WARN\n            pauserd,pausewr flag(s) set\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 45m)\n    mgr: ceph-node01(active, since 38m), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 25m), 6 in (since 25m)\n         flags pauserd,pausewr\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   4 pools, 128 pgs\n    objects: 187 objects, 1.2 KiB\n    usage:   6.0 GiB used, 234 GiB / 240 GiB avail\n    pgs:     128 active+clean\n\nceph osd unpause\npauserd,pausewr is unset\nceph -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 45m)\n    mgr: ceph-node01(active, since 38m), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 26m), 6 in (since 26m)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   4 pools, 128 pgs\n    objects: 187 objects, 1.2 KiB\n    usage:   6.0 GiB used, 234 GiB / 240 GiB avail\n    pgs:     128 active+clean\n</code></pre>"},{"location":"storage/osd/#_3","title":"\u6570\u636e\u64cd\u4f5c\u6bd4\u91cd","text":"<p>\u547d\u4ee4\u683c\u5f0f\uff1a<code>ceph osd crush reweight osd.ID \u6743\u91cd</code>\uff0c\u8c03\u6574\u6743\u91cd\u4e3a0\uff0c\u5219\u8868\u793aCRUSH \u7b97\u6cd5\u4e0d\u4f1a\u518d\u9009\u62e9\u8fd9\u4e2a OSD \u5b58\u50a8\u65b0\u6570\u636e\u3002</p> <pre><code># \u67e5\u770b\u9ed8\u8ba4\u6743\u91cd\nceph osd crush tree\nID CLASS WEIGHT  TYPE NAME\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0\n 1   hdd 0.04880         osd.1\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2\n 3   hdd 0.04880         osd.3\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4\n 5   hdd 0.04880         osd.5\n # \u8c03\u6574\u6743\u91cd\u4e3a0\uff0c\u5219\u8868\u793aCRUSH \u7b97\u6cd5\u4e0d\u4f1a\u518d\u9009\u62e9\u8fd9\u4e2a OSD \u5b58\u50a8\u65b0\u6570\u636e\nceph osd crush reweight osd.4 0\nreweighted item id 4 name 'osd.4' to 0 in crush map\n[cephadm@ceph-admin ceph-cluster]$ ceph osd crush tree\nID CLASS WEIGHT  TYPE NAME\n-1       0.20499 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0\n 1   hdd 0.04880         osd.1\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2\n 3   hdd 0.04880         osd.3\n-7       0.04880     host ceph-node03\n 4   hdd       0         osd.4\n 5   hdd 0.04880         osd.5\n</code></pre>"},{"location":"storage/osd/#osd","title":"OSD\u4e0a\u4e0b\u7ebf","text":"<p>osd\u6709\u4e13\u95e8\u7684\u7ba1\u7406\u670d\u52a1\u63a7\u5236\uff0c\u4e00\u65e6\u53d1\u73b0\u88ab\u4e0b\u7ebf\uff0c\u4f1a\u5c1d\u8bd5\u542f\u52a8\u5b83 - \u4e0a\u7ebf\uff1a<code>ceph osd down ID</code> - \u4e0b\u7ebf\uff1a<code>ceph osd up ID</code></p> <pre><code># \u89c1\u78c1\u76d8\u5feb\u901f\u4e0b\u7ebf\uff0c\u7136\u540e\u67e5\u770b\u72b6\u6001\nceph osd down 0\nmarked down osd.0.\n[cephadm@ceph-admin ceph-cluster]$ ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0          down  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n#\u7b49\u5f85\u4e00\u5206\u949f\u540e\u63d2\u4ef6\u72b6\u6001\uff0c\u6307\u5b9a\u8282\u70b9\u53c8\u81ea\u52a8\u4e0a\u7ebf\u4e86\n ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre>"},{"location":"storage/osd/#osd_1","title":"\u9a71\u9010\u52a0\u5165OSD\u5bf9\u8c61","text":"<p>\u9a71\u9010\u6216\u52a0\u5165OSD\u5bf9\u8c61\uff0c\u672c\u8d28\u4e0a\u662fCeph\u96c6\u7fa4\u6570\u636e\u64cd\u4f5c\u7684\u6743\u91cd\u503c\u8c03\u6574,\u4e00\u822c\u66f4\u6362\u78c1\u76d8\u7684\u65f6\u5019\u4f1a\u4f7f\u7528\u5230</p> <ul> <li>\u9a71\u9010\uff1a<code>ceph osd out osd\u7f16\u53f7</code></li> <li>\u52a0\u5165\uff1a<code>ceph osd in osd\u7f16\u53f7</code></li> </ul>"},{"location":"storage/osd/#osd_2","title":"OSD\u8282\u70b9\u64cd\u4f5c","text":""},{"location":"storage/osd/#1osd","title":"1.OSD\u5220\u9664\u6b65\u9aa4","text":"<p>OSD\u5220\u9664\u9700\u8981\u9075\u5faa\u4e00\u5b9a\u7684\u6b65\u9aa4\uff0c\u5426\u5219\u4f1a\u5b58\u5728\u6570\u636e\u4e22\u5931\u7684\u60c5\u51b5\uff1a</p> <ol> <li>\u4fee\u6539OSD\u7684\u6570\u636e\u64cd\u4f5c\u6743\u91cd\u503c\uff0c\u8ba9\u6570\u636e\u4e0d\u5206\u5e03\u5728\u8fd9\u4e2a\u8282\u70b9\u4e0a</li> <li>\u5230\u6307\u5b9a\u8282\u70b9\uff0c\u505c\u6b62\u6307\u5b9a\u7684osd\u8fdb\u7a0b</li> <li>\u5c06\u79fb\u9664\u7684osd\u8282\u70b9\u72b6\u6001\u6807\u8bb0\u4e3aout</li> <li>\u4ececrush\u4e2d\u79fb\u9664OSD\u8282\u70b9\uff0c\u8be5\u8282\u70b9\u4e0d\u518d\u4f5c\u4e3a\u6570\u636e\u7684\u8f7d\u4f53</li> <li>\u5220\u9664OSD\u8282\u70b9</li> <li>\u5220\u9664OSD\u8282\u70b9\u7684\u8ba4\u8bc1\u4fe1\u606f</li> </ol>"},{"location":"storage/osd/#2-osd5","title":"2. \u5b9e\u8df5\uff1a\u5220\u9664osd.5","text":"<p><pre><code>ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre> <pre><code># \u4fee\u6539OSD\u7684\u6570\u636e\u64cd\u4f5c\u6743\u91cd\u503c\u4e3a0\uff0c\u8ba9\u6570\u636e\u4e0d\u5206\u5e03\u5728\u8fd9\u4e2a\u8282\u70b9\u4e0a\nceph osd crush reweight osd.5 0\nreweighted item id 5 name 'osd.5' to 0 in crush map\n[cephadm@ceph-admin ceph-cluster]$ ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.18549 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.02930     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd       0         osd.5            up  1.00000 1.00000\n # ssh ceph-node03\u505c\u6b62\u8fdb\u7a0b\nssh cephadm@ceph-node03 \"sudo systemctl stop ceph-osd@5\"\nceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.18549 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.02930     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd       0         osd.5          down  1.00000 1.00000\n#  \u5c06\u79fb\u9664\u7684osd\u8282\u70b9\u72b6\u6001\u6807\u8bb0\u4e3aout\nceph osd out 5\nmarked out osd.5.\nceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.18549 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.02930     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd       0         osd.5          down        0 1.00000\n# \u4ececrush\u4e2d\u79fb\u9664OSD\u8282\u70b9\uff0c\u8be5\u8282\u70b9\u4e0d\u518d\u4f5c\u4e3a\u6570\u636e\u7684\u8f7d\u4f53\nceph osd crush rm osd.5\nceph osd crush tree\nID CLASS WEIGHT  TYPE NAME\n-1       0.18549 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0\n 1   hdd 0.04880         osd.1\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2\n 3   hdd 0.04880         osd.3\n-7       0.02930     host ceph-node03\n 4   hdd 0.02930         osd.4\n# 5. \u5220\u9664OSD\u8282\u70b9\nceph osd rm 5\nremoved osd.5\n[cephadm@ceph-admin ceph-cluster]$ ceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.18549 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.02930     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n# \u5220\u9664OSD\u8282\u70b9\u7684\u8ba4\u8bc1\u4fe1\u606f\nceph auth rm osd.5\n</code></pre></p>"},{"location":"storage/osd/#3osd_1","title":"3.OSD\u6dfb\u52a0\u6b65\u9aa4","text":"<p>\u5c06OSD\u6dfb\u52a0\u96c6\u7fa4\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li>\u786e\u5b9aOSD\u8282\u70b9\u6ca1\u6709\u88ab\u5360\u7528</li> <li>\u683c\u5f0f\u5316\u78c1\u76d8</li> <li>ceph\u64e6\u9664\u78c1\u76d8\u4e0a\u7684\u6570\u636e</li> <li>\u6dfb\u52a0OSD\u5230\u96c6\u7fa4</li> </ol>"},{"location":"storage/osd/#4osd_1","title":"4.\u5b9e\u8df5:\u6dfb\u52a0OSD","text":"<p>\u5c06 ceph-node3\u4e0a\u7684/dev/vdc\u78c1\u76d8\u6dfb\u52a0\u6210OSD</p> <ul> <li>\u6e05\u9664\u4e4b\u524d\u7684osd\u6620\u5c04\u5173\u7cfb</li> </ul> <p><pre><code># ceph \u6570\u636e\u88ab\u4f7f\u7528\u4e2d\ndmsetup status\nceph--333eccf3--9518--4e6e--bdc6--7e1c07c0dabe-osd--block--a3d688ed--4432--4d0e--b12d--d75f82f70b52: 0 104849408 linear\nceph--cb582b60--72ad--46f5--ba21--ac756204dac0-osd--block--1490030e--420b--493e--b817--515ef0aec597: 0 62906368 linear\ncentos-swap: 0 8126464 linear\ncentos-root: 0 409190400 linear\n# \u67e5\u770bosd\u7684id\u503c\ncat /var/lib/ceph/osd/ceph-5/fsid\na3d688ed-4432-4d0e-b12d-d75f82f70b52\nlsblk\nNAME                                                                                                  MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsr0                                                                                                    11:0    1  1024M  0 rom\nvda                                                                                                   252:0    0   200G  0 disk\n\u251c\u2500vda1                                                                                                252:1    0     1G  0 part /boot\n\u2514\u2500vda2                                                                                                252:2    0   199G  0 part\n  \u251c\u2500centos-root                                                                                       253:0    0 195.1G  0 lvm  /\n  \u2514\u2500centos-swap                                                                                       253:1    0   3.9G  0 lvm  [SWAP]\nvdb                                                                                                   252:16   0    30G  0 disk\n\u2514\u2500ceph--cb582b60--72ad--46f5--ba21--ac756204dac0-osd--block--1490030e--420b--493e--b817--515ef0aec597 253:2    0    30G  0 lvm\nvdc                                                                                                   252:32   0    50G  0 disk\n\u2514\u2500ceph--333eccf3--9518--4e6e--bdc6--7e1c07c0dabe-osd--block--a3d688ed--4432--4d0e--b12d--d75f82f70b52 253:3    0    50G  0 lvm\n\n# \u5220\u9664 Ceph OSD \u5bf9\u5e94\u7684 LVM \u8bbe\u5907\u6620\u5c04\uff08device mapper\uff09\uff0c\u786e\u5b9aOSD\u8282\u70b9\u6ca1\u6709\u88ab\u5360\u7528\n~]# dmsetup remove ceph--333eccf3--9518--4e6e--bdc6--7e1c07c0dabe-osd--block--a3d688ed--4432--4d0e--b12d--d75f82f70b52\n[root@ceph-node03 ~]# lsblk\nNAME                                                                                                  MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsr0                                                                                                    11:0    1  1024M  0 rom\nvda                                                                                                   252:0    0   200G  0 disk\n\u251c\u2500vda1                                                                                                252:1    0     1G  0 part /boot\n\u2514\u2500vda2                                                                                                252:2    0   199G  0 part\n  \u251c\u2500centos-root                                                                                       253:0    0 195.1G  0 lvm  /\n  \u2514\u2500centos-swap                                                                                       253:1    0   3.9G  0 lvm  [SWAP]\nvdb                                                                                                   252:16   0    30G  0 disk\n\u2514\u2500ceph--cb582b60--72ad--46f5--ba21--ac756204dac0-osd--block--1490030e--420b--493e--b817--515ef0aec597 253:2    0    30G  0 lvm\nvdc                                                                                                   252:32   0    50G  0 disk\n</code></pre> - \u6e05\u7406\u903b\u8f91\u5377\u4fe1\u606f</p> <p><pre><code># \u505c\u6b62osd\u8fdb\u7a0b\uff0c\u5982\u679c\u8fdb\u7a0b\u8fd8\u5728\u7684\u8bdd\nsystemctl stop ceph-osd@&lt;osd-id&gt;.service\nlvs\n  LV                                             VG                                        Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  root                                           centos                                    -wi-ao---- &lt;195.12g\n  swap                                           centos                                    -wi-ao----   &lt;3.88g\n  osd-block-a3d688ed-4432-4d0e-b12d-d75f82f70b52 ceph-333eccf3-9518-4e6e-bdc6-7e1c07c0dabe -wi-------  &lt;50.00g\n  osd-block-1490030e-420b-493e-b817-515ef0aec597 ceph-cb582b60-72ad-46f5-ba21-ac756204dac0 -wi-ao----  &lt;30.00g\npvs\n  PV         VG                                        Fmt  Attr PSize    PFree\n  /dev/vda2  centos                                    lvm2 a--  &lt;199.00g 4.00m\n  /dev/vdb   ceph-cb582b60-72ad-46f5-ba21-ac756204dac0 lvm2 a--   &lt;30.00g    0\n  /dev/vdc   ceph-333eccf3-9518-4e6e-bdc6-7e1c07c0dabe lvm2 a--   &lt;50.00g    0\ndmsetup ls | grep ceph\nceph--cb582b60--72ad--46f5--ba21--ac756204dac0-osd--block--1490030e--420b--493e--b817--515ef0aec597 (253:2)\n# \u5220\u9664\u65e7 VG\uff08\u5f7b\u5e95\u91ca\u653e\u78c1\u76d8\uff09\nvgremove ceph-333eccf3-9518-4e6e-bdc6-7e1c07c0dabe\nDo you really want to remove volume group \"ceph-333eccf3-9518-4e6e-bdc6-7e1c07c0dabe\" containing 1 logical volumes? [y/n]: y\n  Logical volume \"osd-block-a3d688ed-4432-4d0e-b12d-d75f82f70b52\" successfully removed\n  Volume group \"ceph-333eccf3-9518-4e6e-bdc6-7e1c07c0dabe\" successfully removed\n# \u5220\u9664 PV \u6807\u8bc6\npvremove /dev/vdc\n  Labels on physical volume \"/dev/vdc\" successfully wiped.\n# \u786e\u8ba4\u78c1\u76d8\u5e72\u51c0\nipefs -a /dev/vdc\n</code></pre> - \u64e6\u9664\u78c1\u76d8\u6570\u636e</p> <p><pre><code>ceph-deploy disk zap ceph-node03 /dev/vdc\n</code></pre> - \u6dfb\u52a0\u4e3aosd</p> <pre><code>ceph-deploy osd create ceph-node03 --data /dev/vdc\nceph osd tree\nID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF\n-1       0.23428 root default\n-3       0.07809     host ceph-node01\n 0   hdd 0.02930         osd.0            up  1.00000 1.00000\n 1   hdd 0.04880         osd.1            up  1.00000 1.00000\n-5       0.07809     host ceph-node02\n 2   hdd 0.02930         osd.2            up  1.00000 1.00000\n 3   hdd 0.04880         osd.3            up  1.00000 1.00000\n-7       0.07809     host ceph-node03\n 4   hdd 0.02930         osd.4            up  1.00000 1.00000\n 5   hdd 0.04880         osd.5            up  1.00000 1.00000\n</code></pre>"},{"location":"storage/osd/#osd_3","title":"OSD\u6570\u636e\u5b9e\u8df5","text":""},{"location":"storage/osd/#1","title":"1.\u76f8\u5173\u672f\u8bed","text":"\u5bf9\u8c61 \u8bf4\u660e \u7528\u9014 Pool Pool \u662f Ceph \u7684\u903b\u8f91\u5b58\u50a8\u5355\u5143 \u7528\u6237\u5728 Pool \u4e2d\u521b\u5efa\u5bf9\u8c61\uff08object\uff09,\u6bcf\u4e2a Pool \u53ef\u4ee5\u8bbe\u7f6e\u4e0d\u540c\u7684\u526f\u672c\u6570\u3001\u526f\u672c\u7c7b\u578b\u3001\u5f52\u7f6e\u7b56\u7565 PG\uff08Placement Group\uff0c\u653e\u7f6e\u7ec4\uff09 Ceph \u6570\u636e\u653e\u7f6e\u548c\u590d\u5236\u7684\u6700\u5c0f\u5355\u4f4d Pool \u91cc\u7684\u5bf9\u8c61\u592a\u591a\uff08\u53ef\u80fd\u4e0a\u767e\u4e07\uff09\uff0c\u4e0d\u53ef\u80fd\u9010\u4e2a\u505a\u526f\u672c\u653e\u7f6e\uff1b\u628a\u5bf9\u8c61\u5206\u5230\u4e00\u7ec4\u4e00\u7ec4\u91cc\uff0c\u8fd9\u4e9b\u7ec4\u53eb PG,PG \u624d\u662f\u4e0e\u5177\u4f53 OSD \u7ed1\u5b9a\u7684\u5355\u4f4d\u3002 PGP\uff08Placement Group for Placement\uff09 \u7528\u4e8e CRUSH \u7b97\u6cd5\u7684 PG \u6570\u91cf\u3002 \u4e3a\u4e86\u5e73\u6ed1\u6269\u5bb9\u6216\u8fc1\u79fb\uff0cCeph \u5141\u8bb8 PG \u6570\u91cf\uff08PG_num\uff09\u548c PGP \u6570\u91cf\uff08PGP_num\uff09\u4e0d\u540c\u3002"},{"location":"storage/osd/#2","title":"2.\u521b\u5efa\u5b58\u50a8\u6c60","text":"<p><pre><code>ceph osd pool create &lt;poolname&gt; &lt;pg-num&gt; [pgp-num] [replicated] [crush-rule-name] [expected-num-object]\n</code></pre> - <code>poolname</code>: \u5b58\u50a8\u6c60\u540d\u79f0\uff0c\u5728rados\u96c6\u7fa4\u5177\u6709\u552f\u4e00\u6027 - <code>pg-num</code>:\u623f\u524d\u5b58\u50a8\u6c60\u4e2dpg\u6570\u91cf - <code>pgp-num</code>:\u7528\u4e8e\u5f52\u7f6e\u7684pg\u6570\u91cf\uff0c\u5e94\u7b49\u4e8epg-num - <code>replicated</code>\uff1a\u5b58\u50a8\u7c7b\u578b\u3002\u526f\u672c\u6c60\u9700\u8981\u66f4\u591a\u539f\u59cb\u7a7a\u95f4 - <code>crush-rule-name</code>\uff1a\u5b58\u50a8\u6c60\u9700\u8981\u7684CRUSH\u89c4\u5219\u96c6\u540d\u79f0\uff0c\u5f15\u7528\u7684\u540d\u79f0\u5fc5\u987b\u4e8b\u5148\u5b58\u5728 <pre><code> ceph osd pool create mypool 16 16\n</code></pre></p>"},{"location":"storage/osd/#3","title":"3.\u67e5\u770b\u5b58\u50a8\u6c60","text":"<pre><code>~]$ ceph osd pool ls\n.rgw.root\ndefault.rgw.control\ndefault.rgw.meta\ndefault.rgw.log\nmypool\n~]$ rados lspools\n.rgw.root\ndefault.rgw.control\ndefault.rgw.meta\ndefault.rgw.log\nmypool\n</code></pre>"},{"location":"storage/osd/#4","title":"4.\u63d0\u4ea4\u6570\u636e","text":"<pre><code>~]$ rados put myfstab /etc/fstab --pool mypool\n~]$ rados ls --pool mypool\nmyfstab\n~]$ ceph osd map --pool mypool myfstab\nosdmap e72 pool 'mypool' (5) object 'myfstab' -&gt; pg 5.1baefa14 (5.4) -&gt; up ([4,1,3], p4) acting ([4,1,3], p4)\n</code></pre>"},{"location":"storage/osd/#5","title":"5.\u5220\u9664\u6570\u636e","text":"<pre><code>~]$ rados rm myfstab --pool mypool\n</code></pre>"},{"location":"storage/osd/#6","title":"6.\u5220\u9664\u5b58\u50a8\u6c60","text":"<pre><code>~]$ ceph osd pool rm mypool mypool --yes-i-really-really-mean-it\nError EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool\n~]$ ceph config set mon   mon_allow_pool_delete true\n~]$ ceph osd pool rm mypool mypool --yes-i-really-really-mean-it\npool 'mypool' removed\n~]$ ceph config set mon   mon_allow_pool_delete false\n</code></pre>"},{"location":"storage/rbd/","title":"RADOS Block Device\u5757\u8bbe\u5907","text":"<p>RBD\uff08RADOS Block Device\uff09 \u662f Ceph \u63d0\u4f9b\u7684 \u5757\u5b58\u50a8\u63a5\u53e3\uff0c\u5b83\u57fa\u4e8e Ceph \u7684\u5206\u5e03\u5f0f\u5bf9\u8c61\u5b58\u50a8\u5c42\uff08RADOS\uff09\u5b9e\u73b0\u3002 \u7b80\u5355\u8bf4\uff0c\u5b83\u53ef\u4ee5\u628a Ceph \u7684\u5bf9\u8c61\u5b58\u50a8\u6c60\u62bd\u8c61\u6210\u300c\u5757\u8bbe\u5907\u300d\u7ed9\u4e3b\u673a\u4f7f\u7528\uff0c\u5c31\u50cf\u4e00\u5757\u865a\u62df\u78c1\u76d8\u4e00\u6837\u3002</p> <p>\u5e38\u7528\u4e8e\uff1a</p> <ul> <li>\u865a\u62df\u673a\u78c1\u76d8\uff08\u6bd4\u5982\uff1aOpenStack\u3001KVM\uff09</li> <li>\u5bb9\u5668\u5b58\u50a8\uff08Kubernetes \u7684 RBD/Ceph CSI\uff09</li> <li>\u6570\u636e\u5e93\u5b58\u50a8\u5377</li> </ul>"},{"location":"storage/rbd/#rbd","title":"RBD\u64cd\u4f5c\u903b\u8f91","text":"<p>rbd\u63a5\u53e3\u5728ceph\u73af\u5883\u521b\u5efa\u540e\uff0c\u5c31\u5728\u670d\u52a1\u7aef\u81ea\u52a8\u63d0\u4f9b\u4e86\uff0c\u5ba2\u6237\u7aef\u57fa\u4e8elibrdb\u5e93\uff0c\u53ef\u8bb2rados\u5b58\u50a8\u96c6\u7fa4\u7528\u4f5c\u5757\u8bbe\u5907\uff0c\u5177\u4f53\u4f7f\u7528\u903b\u8f91\u5982\u4e0b\uff1a</p> <ul> <li>1.\u521b\u5efa\u4e13\u7528rbd\u5b58\u50a8\u6c60</li> <li>2.\u5bf9\u5b58\u50a8\u6c60\u542f\u7528rdb\u529f\u80fd</li> <li>3.\u5bf9\u5b58\u50a8\u6c60\u6267\u884c\u73af\u5883\u521d\u59cb\u5316</li> <li>4.\u57fa\u4e8e\u5b58\u50a8\u6c60\u521b\u5efa\u78c1\u76d8\u955c\u50cf</li> </ul> <pre><code># \u521b\u5efa\u4e13\u7528rbd\u5b58\u50a8\u6c60\n~]$ ceph osd pool create rbdpool 8 8\n# \u5bf9\u5b58\u50a8\u6c60\u542f\u7528rdb\u529f\u80fd\n~]$ ceph osd pool application enable rbdpool rbd\nenabled application 'rbd' on pool 'rbdpool'\n# \u5bf9\u5b58\u50a8\u6c60\u6267\u884c\u73af\u5883\u521d\u59cb\u5316\n~]$ rbd pool init -p rbdpool\n# \u57fa\u4e8e\u5b58\u50a8\u6c60\u521b\u5efa\u78c1\u76d8\u955c\n~]$ rbd create node-img1 --size 1G --pool rbdpool\n[cephadm@ceph-admin ~]$ rbd pool stats rbdpool\nTotal Images: 1\nTotal Snapshots: 0\nProvisioned Size: 1 GiB\n# \u67e5\u770b\u955c\u50cf\u4fe1\u606f\n~]$ rbd info --image node-img1 --pool rbdpool\nrbd image 'node-img1':\n    size 1 GiB in 256 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 3b4e90cea1f4\n    block_name_prefix: rbd_data.3b4e90cea1f4\n    format: 2\n    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 18:59:13 2025\n    access_timestamp: Tue Oct 21 18:59:13 2025\n    modify_timestamp: Tue Oct 21 18:59:13 2025\n</code></pre> \u955c\u50cf\u5c5e\u6027 \u8bf4\u660e layering \u5206\u5c42\u514b\u9686\u673a\u5236 exclusive-lock \u6392\u4ed6\u9501\uff0c\u4ec5\u80fd\u4e00\u4e2a\u5ba2\u6237\u7aef\u8bbf\u95ee\u5f53\u524dimage striping \u662f\u5426\u652f\u6301\u6570\u636e\u5bf9\u8c61\u95f4\u7684\u6570\u636e\u6761\u5e26\u5316 object-map \u5bf9\u8c61\u4f4d\u56fe\uff0c\u7528\u6237\u52a0\u901f\u5bfc\u5165\u5bfc\u51fa\u5df2\u7ecf\u5bb9\u91cf\u7edf\u8ba1\u7b49\u64cd\u4f5c\uff0c\u4f9d\u8d56\u6392\u4ed6\u9501 fast-diff \u5feb\u7167\u5b9a\u5236\u673a\u5236\uff0c\u5feb\u901f\u6bd4\u5bf9\u6570\u636e\u5dee\u5f02\uff0c\u4fbf\u4e8e\u5feb\u7167\u7ba1\u7406\uff0c\u4f9d\u8d56\u5bf9\u8c61\u4f4d\u56fe deep-flatten \u6570\u636e\u5904\u7406\u673a\u5236\uff0c\u89e3\u9664\u7236\u5b50image\u4ee5\u53ca\u5feb\u7167\u7684\u4f9d\u8d56\u5173\u7cfb journaling \u78c1\u76d8\u65e5\u5fd7\u673a\u5236\uff0c\u5c06image\u7684\u6240\u6709\u4fee\u6539\u8fdb\u884c\u65e5\u5fd7\u5316\uff0c\u4fbf\u4e8e\u5f02\u5730\u5907\u4efd\uff0c\u4f9d\u8d56\u6392\u4ed6\u6240 data-pool \u662f\u5426\u652f\u6301\u5c06image\u7684\u6570\u636e\u5bf9\u8c61\u5b58\u50a8\u4e8e\u7ea0\u5220\u7801\u6c60\uff0c\u4e3b\u8981\u7528\u4e8e\u5c06\u5143\u6570\u636e\u548c\u6570\u636e\u653e\u7f6e\u4e8e\u4e0d\u540c\u7684\u5b58\u50a8\u6c60 <pre><code># \u5220\u9664\u955c\u50cf\u5c5e\u6027\n~]$ rbd feature disable rbdpool/node-img1 object-map fast-diff deep-flatten\n~]$ rbd info --image node-img1 --pool rbdpool\nrbd image 'node-img1':\n    size 1 GiB in 256 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 3b4e90cea1f4\n    block_name_prefix: rbd_data.3b4e90cea1f4\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 18:59:13 2025\n    access_timestamp: Tue Oct 21 18:59:13 2025\n    modify_timestamp: Tue Oct 21 18:59:13 2025\n~]$ rbd feature enable rbdpool/node-img1 object-map fast-diff\n~]$ rbd info --image node-img1 --pool rbdpool\nrbd image 'node-img1':\n    size 1 GiB in 256 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 3b4e90cea1f4\n    block_name_prefix: rbd_data.3b4e90cea1f4\n    format: 2\n    features: layering, exclusive-lock, object-map, fast-diff\n    op_features:\n    flags: object map invalid, fast diff invalid\n    create_timestamp: Tue Oct 21 18:59:13 2025\n    access_timestamp: Tue Oct 21 18:59:13 2025\n    modify_timestamp: Tue Oct 21 18:59:13 2025\n</code></pre>"},{"location":"storage/rbd/#_1","title":"\u955c\u50cf\u4f7f\u7528","text":"<ul> <li>\u521d\u59cb\u5316\u5b58\u50a8\u6c60\u540e\uff0c\u521b\u5efaimage</li> <li>\u6700\u597d\u7981\u7528 object-map fast-diff deep-flatten \u7b49\u5c5e\u6027</li> <li>\u9700\u8981\u76f4\u63a5\u4e8emonitor\u89d2\u8272\u8fdb\u884c\u901a\u4fe1\uff0c\u5982\u679c\u542f\u7528\u8ba4\u8bc1\uff0c\u8fd8\u9700\u8981\u6307\u5b9a\u7528\u6237\u540d\u548ckeyring</li> <li>\u4f7f\u7528rdb\u7684map\u540d\u5229\u3001\u8fdb\u884c\u78c1\u76d8\u6587\u4ef6\u7684\u6620\u5c04</li> </ul>"},{"location":"storage/rbd/#1rbd","title":"1.\u51c6\u5907rbd\u955c\u50cf\u5377","text":"<pre><code># \u51c6\u5907\u5757\u5b58\u50a8\n~]$ ceph osd pool create kube 16 16\npool 'kube' created\n~]$ ceph osd pool application enable  kube rbd\n~]$ rbd pool init -p kube\n~]$ rbd create vol-01 --size 1G --pool kube\n~]$ rbd feature disable kube/vol-01 object-map fast-diff deep-flatten\n~]$ rbd feature disable kube/vol-01 object-map fast-diff deep-flatten\n[cephadm@ceph-admin ~]$ rbd info kube/vol-01\nrbd image 'vol-01':\n    size 1 GiB in 256 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 150def6887b3\n    block_name_prefix: rbd_data.150def6887b3\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 19:58:31 2025\n    access_timestamp: Tue Oct 21 19:58:31 2025\n    modify_timestamp: Tue Oct 21 19:58:31 2025\n Warnick &lt;patience@newdream.net&gt;\nauthor:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;\nauthor:         Sage We# \u5ba2\u6237\u7aef\u4e3b\u673a\u5b89\u88c5ceph-common\n</code></pre>"},{"location":"storage/rbd/#2ceph","title":"2.\u5ba2\u6237\u7aef\u4e3b\u673a\u5b89\u88c5ceph","text":"<pre><code>~]# yum install https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/ceph-release-1-1.el7.noarch.rpm\n~]# yum install -y ceph-common\n~]# modinfo ceph\nfilename:       /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/ceph.ko.xz\nlicense:        GPL\ndescription:    Ceph filesystem for Linux\nauthor:         Patienceil &lt;sage@newdream.net&gt;\nalias:          fs-ceph\nretpoline:      Y\nrhelversion:    7.5\nsrcversion:     FD277B552FF7AC92D1E3117\ndepends:        libceph\nintree:         Y\nvermagic:       3.10.0-862.el7.x86_64 SMP mod_unload modversions\nsigner:         CentOS Linux kernel signing key\nsig_key:        3A:F3:CE:8A:74:69:6E:F1:BD:0F:37:E5:52:62:7B:71:09:E3:2B:96\nsig_hashalgo:   sha256\n</code></pre>"},{"location":"storage/rbd/#3","title":"3.\u63d0\u4f9b\u8ba4\u8bc1\u4fe1\u606f\u7ed9\u5ba2\u6237\u7aef","text":"<pre><code># \u786e\u8ba4\u7528\u6237\u6743\u9650\n~]$ ceph auth get client.kube|tee  ceph.client.kube.keyring\nexported keyring for client.kube\n[client.kube]\n    key = AQDGYfdoWGEBFxAAZCGy/1RktgAWmuyYKTkhow==\n    caps mon = \"allow r\"\n    caps osd = \"allow * pool=kube\"\n# scp \u8ba4\u8bc1\u6587\u4ef6\u5230\u5ba2\u6237\u673a\n~]$ scp ceph.client.kube.keyring ceph-cluster/ceph.conf root@192.168.1.109:/etc/ceph/\n# \u5ba2\u6237\u7aef\u9a8c\u8bc1\u901a\u8fc7\n~]# ceph -s --user kube\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 5h)\n    mgr: ceph-node01(active, since 5h), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 3h), 6 in (since 3h)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   5 pools, 144 pgs\n    objects: 191 objects, 1.2 KiB\n    usage:   6.1 GiB used, 234 GiB / 240 GiB avail\n    pgs:     144 active+clean\n</code></pre>"},{"location":"storage/rbd/#4","title":"4.\u6302\u8f7d\u4f7f\u7528","text":"<pre><code># \u6620\u5c04\u8fdc\u7a0bceph\u78c1\u76d8\u5230\u672c\u5730\n~]# rbd --user kube map kube/vol-01\n~]# lsblk\nNAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsr0              11:0    1  1024M  0 rom\nvda             252:0    0   200G  0 disk\n\u251c\u2500vda1          252:1    0     1G  0 part /boot\n\u2514\u2500vda2          252:2    0   199G  0 part\n  \u251c\u2500centos-root 253:0    0 195.1G  0 lvm  /\n  \u2514\u2500centos-swap 253:1    0   3.9G  0 lvm  [SWAP]\nrbd0            251:0    0     1G  0 disk\n~]# rbd showmapped\nid pool namespace image  snap device\n0  kube           vol-01 -    /dev/rbd0\n# \u683c\u5f0f\u5316\uff0c\u6302\u8f7d\u4f7f\u7528\n~]# mkfs.ext4 /dev/rbd0\n~]# mount /dev/rbd0 /mnt\n~]# touch /mnt/rbddata.txt\n</code></pre>"},{"location":"storage/rbd/#5","title":"5.\u5378\u8f7d\u76d8","text":"<pre><code>~]# umount /dev/rbd0\n~]# rbd --user kube unmap kube/vol-01\n~]# rbd showmapped\n</code></pre>"},{"location":"storage/rbd/#_2","title":"\u5bb9\u91cf\u7ba1\u7406","text":"<pre><code>~]$ rbd info kube/vol-01\nrbd image 'vol-01':\n    size 1 GiB in 256 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 150def6887b3\n    block_name_prefix: rbd_data.150def6887b3\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 19:58:31 2025\n    access_timestamp: Tue Oct 21 19:58:31 2025\n    modify_timestamp: Tue Oct 21 19:58:31 2025\n</code></pre>"},{"location":"storage/rbd/#1image","title":"1.\u8c03\u6574image\u5927\u5c0f","text":"<p><pre><code> ~]$ rbd resize kube/vol-01 --size 5G\nResizing image: 100% complete...done.\n[cephadm@ceph-admin ~]$ rbd info kube/vol-01\nrbd image 'vol-01':\n    size 5 GiB in 1280 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 150def6887b3\n    block_name_prefix: rbd_data.150def6887b3\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 19:58:31 2025\n    access_timestamp: Tue Oct 21 19:58:31 2025\n    modify_timestamp: Tue Oct 21 19:58:31 2025\n</code></pre> <pre><code>~]$ rbd resize kube/vol-01 --size 3G --allow-shrink\nResizing image: 100% complete...done.\n[cephadm@ceph-admin ~]$ rbd info kube/vol-01\nrbd image 'vol-01':\n    size 3 GiB in 768 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 150def6887b3\n    block_name_prefix: rbd_data.150def6887b3\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 19:58:31 2025\n    access_timestamp: Tue Oct 21 19:58:31 2025\n    modify_timestamp: Tue Oct 21 19:58:31 2025\n</code></pre></p>"},{"location":"storage/rbd/#2","title":"2.\u6269\u5bb9\u540e\u6302\u8f7d","text":"<pre><code>rbd --user kube map kube/vol-01\nmount /dev/rbd0 /mnt\nresize2fs /dev/rbd0\n</code></pre>"},{"location":"storage/rbd/#3_1","title":"3.\u5220\u9664\u955c\u50cf","text":"<p><pre><code>~]$ rbd --pool kube ls  -l\nNAME   SIZE  PARENT FMT PROT LOCK\nvol-01 3 GiB          2\n# \u5783\u573e\u6876\u529f\u80fd\uff0c\u53ef\u6062\u590d\n~]$ rbd remove kube/vol-01\n~]$ rbd trash move kube/vol-01\n~]$ rbd --pool kube ls  -l\n~]$ rbd trash ls -p kube\n150def6887b3 vol-01\n# \u6062\u590d\n~]$ rbd trash restore --pool kube --image vol-01 --image-id 150def6887b3\n[cephadm@ceph-admin ~]$ rbd --pool kube ls  -l\nNAME   SIZE  PARENT FMT PROT LOCK\nvol-01 3 GiB          2\n</code></pre> <pre><code># \u5f7b\u5e95\u5220\u9664\n~]$ rbd remove kube/vol-01\n</code></pre></p>"},{"location":"storage/rbd/#_3","title":"\u5feb\u7167\u7ba1\u7406","text":"<p>RBD\u652f\u6301\u5feb\u7167\u6280\u672f\uff0c\u501f\u52a9\u5feb\u7167\u53ef\u4ee5\u4fdd\u7559image\u7684\u72b6\u6001\u5386\u53f2\u3002Ceph\u8fd8\u652f\u6301\u5feb\u7167\u5206\u5c42\u673a\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u514b\u9686VM\u6620\u50cf\u3002</p>"},{"location":"storage/rbd/#1","title":"1.\u521b\u5efa\u5feb\u7167","text":"<p>\u521b\u5efa\u5feb\u7167\u4e4b\u524d\uff0c\u5e94\u505c\u6b62image\u4e0a\u7684IO\u64cd\u4f5c\uff0c\u4e14image\u4e0a\u5b58\u5728\u6587\u4ef6\u7cfb\u7edf\u662f\uff0c\u8fd8\u8981\u786e\u4fdd\u5176\u5904\u4e8e\u4e00\u81f4\u72b6\u6001 <pre><code>rbd snap create [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt;\nrbd snap create [&lt;pool-name&gt;/]&lt;image-name&gt;@&lt;snapshot-name&gt;\n</code></pre></p>"},{"location":"storage/rbd/#2_1","title":"2.\u5217\u51fa\u5feb\u7167","text":"<pre><code>rbd snap ls [--pool &lt;pool&gt;] --image &lt;image&gt; ...\n</code></pre>"},{"location":"storage/rbd/#3_2","title":"3.\u56de\u6eda\u5feb\u7167","text":"<p>\u4f7f\u7528\u5f53\u524d\u5feb\u7167\u4e2d\u7684\u6570\u636e\u91cd\u5199\u5f53\u524d\u7248\u672c\u7684image\uff0c\u56de\u6eda\u65f6\u95f4\u968f\u7740\u6570\u636e\u6253\u5927\u5c0f\u589e\u52a0\u800c\u5ef6\u957f <pre><code>rbd snap rollback [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt; ...\n</code></pre></p>"},{"location":"storage/rbd/#4_1","title":"4.\u9650\u5236\u5feb\u7167\u6570\u91cf","text":"<p>\u5feb\u7167\u6570\u91cf\u8fc7\u591a\u65f6\uff0c\u4f1a\u5bfc\u81f4\u955c\u50cf\u5c71\u60ef\u6709\u6570\u636e\u7684\u7b2c\u4e00\u6b21\u4fee\u6539\u65f6\u7684IO\u538b\u529b\u6076\u5316 <pre><code># \u9650\u5236\nrbd snap limit set [--pool &lt;pool&gt;] [--image &lt;image&gt;] --limit int\n# \u89e3\u9664\nrbd snap limit clear [--pool &lt;pool&gt;] [--image &lt;image&gt;]\n</code></pre></p>"},{"location":"storage/rbd/#5_1","title":"5.\u5220\u9664\u5feb\u7167","text":"<p>Ceph OSD\u4f1a\u4ee5\u4e00\u6b65\u65b9\u5f0f\u5220\u9664\u6570\u636e\uff0c\u56e0\u6b64\u5220\u9664\u5feb\u7167\u4e0d\u4f1a\u7acb\u5373\u91ca\u653e\u78c1\u76d8\u7a7a\u95f4 <pre><code>rbd snap rm [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt; ... [--force]\n</code></pre></p>"},{"location":"storage/rbd/#6","title":"6.\u6e05\u7406\u5feb\u7167","text":"<p>\u5220\u9664\u4e00\u4e2a\u955c\u50cf\u7684\u6240\u6709\u5feb\u7167\uff0c\u53ef\u4ee5\u4f7f\u7528 purge <pre><code>rbd snap purge  [--pool &lt;pool&gt;] --image &lt;image&gt;\n</code></pre></p>"},{"location":"storage/rbd/#7","title":"7.\u5feb\u7167\u5b9e\u6218","text":"<ul> <li>\u51c6\u5907\u955c\u50cf</li> </ul> <pre><code>~]$ rbd create vol01 --size 2G --pool kube\n~]$ rbd feature disable kube/vol01 object-map fast-diff deep-flatten\n~]$  rbd info kube/vol01\nrbd image 'vol01':\n    size 2 GiB in 512 objects\n    order 22 (4 MiB objects)\n    snapshot_count: 0\n    id: 15a059e5a259\n    block_name_prefix: rbd_data.15a059e5a259\n    format: 2\n    features: layering, exclusive-lock\n    op_features:\n    flags:\n    create_timestamp: Tue Oct 21 20:58:40 2025\n    access_timestamp: Tue Oct 21 20:58:40 2025\n    modify_timestamp: Tue Oct 21 20:58:40 2025\n</code></pre> <ul> <li>\u6302\u8f7d\u4ea7\u751f\u6570\u636e</li> </ul> <pre><code>~]# rbd --user kube map kube/vol01\n~]# mkfs.ext4 /dev/rbd0\n~]# mount /dev/rbd0 /mnt\n~]# cp /etc/fstab /mnt/\n~]# echo \"1111\" &gt; /mnt/test.txt\n~]# ll /mnt/\ntotal 24\n-rw-r--r-- 1 root root   465 Oct 21 21:01 fstab\ndrwx------ 2 root root 16384 Oct 21 21:00 lost+found\n-rw-r--r-- 1 root root     5 Oct 21 21:01 test.txt\n</code></pre> <ul> <li>\u521b\u5efa\u5feb\u71671</li> </ul> <pre><code>~]$ rbd snap create kube/vol01@vol01-1\n~]$ rbd snap ls kube/vol01\nSNAPID NAME    SIZE  PROTECTED TIMESTAMP\n     4 vol01-1 2 GiB           Tue Oct 21 21:03:54 2025\n</code></pre> <ul> <li>\u5220\u9664\u6570\u636e</li> </ul> <pre><code>~]# rm -rf /mnt/*\n</code></pre> <ul> <li>\u6062\u590d\u6570\u636e</li> </ul> <pre><code># \u5378\u8f7d\u76d8\n~]# umount /dev/rbd0\n~]# rbd --user kube unmap kube/vol01\n# \u6062\u590d\u5feb\u7167\n ~]$ rbd snap rollback kube/vol01@vol01-1\n# \u6302\u8f7d\u76d8\uff0c\u68c0\u67e5\u6570\u636e\u662f\u5426\u6062\u590d\n~]# rbd --user kube map kube/vol01\n/dev/rbd0\n~]# mount /dev/rbd0 /mnt\n~]# ls /mnt/\nfstab  lost+found  test.txt\n</code></pre> <ul> <li>\u5220\u9664\u5feb\u7167</li> </ul> <p><pre><code>~]$ rbd snap list --pool kube --image vol01\nSNAPID NAME    SIZE  PROTECTED TIMESTAMP\n     4 vol01-1 2 GiB           Tue Oct 21 21:03:54 2025\n     5 vol01-2 2 GiB           Tue Oct 21 21:06:08 2025\n~]$ rbd snap rm kube/vol01@vol01-2\n</code></pre> - \u5feb\u7167\u6570\u91cf\u9650\u5236</p> <pre><code>~]$ rbd snap limit set kube/vol01 --limit 5\n~]$ rbd snap create kube/vol01@vol01-2\n[cephadm@ceph-admin ~]$ rbd snap create kube/vol01@vol01-3\n[cephadm@ceph-admin ~]$ rbd snap create kube/vol01@vol01-4\n[cephadm@ceph-admin ~]$ rbd snap create kube/vol01@vol01-5\n[cephadm@ceph-admin ~]$ rbd snap create kube/vol01@vol01-6\nrbd: failed to create snapshot: (122) Disk quota exceeded\n~]$ rbd snap limit clear kube/vol01\n~]$ rbd snap create kube/vol01@vol01-6\n</code></pre> <ul> <li>\u6e05\u7406\u6240\u6709\u5feb\u7167</li> </ul> <pre><code>~]$ rbd snap purge kube/vol01\nRemoving all snapshots: 100% complete...done.\n</code></pre>"},{"location":"storage/rbd/#_4","title":"\u5feb\u7167\u5206\u5c42","text":"<p>ceph\u652f\u6301\u5728\u4e00\u4e2a\u5757\u8bbe\u5907\u5feb\u7167\u7684\u57fa\u7840\u4e0a\u521b\u5efa\u4e00\u4e2a\u6216\u8005\u591a\u4e2aCOW\u6216COR\u7c7b\u578b\u7684\u514b\u9686\uff0c\u7c7b\u4f3c\u4e8eVmware\u865a\u62df\u673a\u7684\u5feb\u901f\u514b\u9686\u6a21\u5f0f\u3002\u57fa\u4e8e\u4e00\u4e2abase\u955c\u50cf(\u4fdd\u62a4\u955c\u50cf)\u5feb\u901f\u514b\u9686\u51fa\u6765\u65b0\u7684\u955c\u50cf\u3002\u8fd9\u5c31\u662f\u5feb\u7167\u5206\u5c42\u6280\u672f\uff0c\u652f\u6301\u8de8\u5b58\u50a8\u6c60\u3002</p>"},{"location":"storage/rbd/#1protect","title":"1.\u4fdd\u62a4\u5feb\u7167(protect)","text":"<pre><code>rbd snap protect [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt; --dest-pool &lt;dest-pool&gt; ...\n</code></pre>"},{"location":"storage/rbd/#2_2","title":"2.\u514b\u9686\u5feb\u7167","text":"<pre><code>rbd clone [&lt;pool-name&gt;/]&lt;image-name&gt;@&lt;snapshot-name&gt; [&lt;pool-name&gt;/&lt;image-name&gt;]\n</code></pre>"},{"location":"storage/rbd/#3_3","title":"3.\u5217\u51fa\u5feb\u7167\u7684\u5b50\u9879","text":"<pre><code>rbd children  [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt;\n</code></pre>"},{"location":"storage/rbd/#4_2","title":"4.\u5c55\u5e73\u514b\u9686","text":"<pre><code>rbd flatten [--pool &lt;pool&gt;] --image &lt;image&gt; --snap &lt;snap&gt;\n</code></pre>"},{"location":"storage/rbd/#5_2","title":"5.\u5206\u5c42\u5b9e\u8df5","text":"<ul> <li>\u5df2\u6709rbd\u5377</li> </ul> <p><pre><code>~]$ rbd ls  --pool kube -l\nNAME  SIZE  PARENT FMT PROT LOCK\nvol01 2 GiB          2\n~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\n/dev/mapper/centos-root  196G  1.4G  194G   1% /\ndevtmpfs                 1.9G     0  1.9G   0% /dev\ntmpfs                    1.9G     0  1.9G   0% /dev/shm\ntmpfs                    1.9G   17M  1.9G   1% /run\ntmpfs                    1.9G     0  1.9G   0% /sys/fs/cgroup\n/dev/vda1               1014M  142M  873M  14% /boot\ntmpfs                    379M     0  379M   0% /run/user/0\n/dev/rbd0                2.0G  6.1M  1.8G   1% /mnt\n~]# ls /mnt/\nfstab  lost+found  test.txt\n</code></pre> - \u5b9a\u5236\u57fa\u7840\u5feb\u7167</p> <p><pre><code>~]$ rbd snap create kube/vol01@basetp01\n~]$ rbd snap ls kube/vol01\nSNAPID NAME     SIZE  PROTECTED TIMESTAMP\n    20 basetp01 2 GiB           Tue Oct 21 21:40:35 2025\n</code></pre> - \u4fdd\u62a4\u5feb\u7167</p> <pre><code>~]$ rbd snap protect kube/vol01@basetp01\n~]$ rbd snap ls kube/vol01\nSNAPID NAME     SIZE  PROTECTED TIMESTAMP\n    20 basetp01 2 GiB yes       Tue Oct 21 21:40:35 2025\n</code></pre> <ul> <li>\u57fa\u4e8e\u57fa\u7840\u5feb\u7167\u514b\u9686\u5b50\u5feb\u7167</li> </ul> <p><pre><code>~]$ rbd clone kube/vol01@basetp01 kube/image01\n~]$ rbd clone kube/vol01@basetp01 kube/image02\n~]$ rbd ls --pool kube -l\nNAME           SIZE  PARENT              FMT PROT LOCK\nimage01        2 GiB kube/vol01@basetp01   2\nimage02        2 GiB kube/vol01@basetp01   2\nvol01          2 GiB                       2\nvol01@basetp01 2 GiB                       2 yes\n</code></pre> - \u67e5\u770b\u5b50\u5feb\u7167</p> <p><pre><code>~]$ rbd children kube/vol01@basetp01\nkube/image01\nkube/image02\n</code></pre> - \u6302\u8f7d\u5b50\u5feb\u7167</p> <pre><code>~]# rbd --user kube --pool kube ls\nimage01\nimage02\nvol01\n~]# rbd --user kube map kube/image01\n/dev/rbd1\n~]# rbd --user kube map kube/image02\n/dev/rbd2\n~]# mkdir /data/image01 -pv\n~]# mkdir /data/image02 -pv\n~]# mount /dev/rdb1 /data/image01\n~]# echo \"rbd1 image01\" &gt;&gt; /data/image01/image.txt\n~]# ll /data/image01/\ntotal 28\n-rw-r--r-- 1 root root   465 Oct 21 21:01 fstab\n-rw-r--r-- 1 root root    13 Oct 21 21:50 image.txt\ndrwx------ 2 root root 16384 Oct 21 21:00 lost+found\n-rw-r--r-- 1 root root     5 Oct 21 21:01 test.txt\n~]# mount /dev/rdb2 /data/image02\n~]# echo \"rbd1 image02\" &gt;&gt; /data/image02/image02.txt\n~]# ll /data/image02\ntotal 28\n-rw-r--r-- 1 root root   465 Oct 21 21:01 fstab\n-rw-r--r-- 1 root root    13 Oct 21 21:51 image02.txt\ndrwx------ 2 root root 16384 Oct 21 21:00 lost+found\n-rw-r--r-- 1 root root     5 Oct 21 21:01 test.txt\n</code></pre> <ul> <li>\u5c55\u5e73\u5b50\u5feb\u7167</li> </ul> <p>\u514b\u9686\u7684\u955c\u50cf\u4f1a\u4fdd\u7559\u5bf9\u7236\u955c\u50cf\u7684\u5f15\u7528\uff0c\u5c55\u5e73\u5219\u4f1a\u5b8c\u6210\u590d\u5236\u7236\u5feb\u7167\u7684\u7684\u6570\u636e\uff0c\u4ece\u800c\u6210\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u4e0d\u5bf9\u7236\u955c\u50cf\u6709\u4f9d\u8d56\u7684\u955c\u50cf\u3002 <pre><code>~]$ rbd flatten kube/image01\nImage flatten: 100% complete...done.\n~]$ rbd flatten kube/image02\nImage flatten: 100% complete...done.\n</code></pre> - \u6b64\u65f6\u5220\u9664\u57fa\u7840\u5feb\u7167\u5bf9image01/image02\u65e0\u4efb\u4f55\u5f71\u54cd <pre><code>~]$ rbd snap unprotect kube/vol01@basetp01\n~]$ rbd snap rm kube/vol01@basetp01\nRemoving snap: 100% complete...done.\n~]$ rbd ls --pool kube  -l\nNAME    SIZE  PARENT FMT PROT LOCK\nimage01 2 GiB          2\nimage02 2 GiB          2\nvol01   2 GiB          2\n</code></pre></p>"},{"location":"storage/rbd/#kvmceph","title":"kvm\u96c6\u6210ceph","text":"<p>\u524d\u63d0 - \u5b89\u88c5kvm\u73af\u5883</p> <ul> <li>\u5b89\u88c5ceph\u73af\u5883</li> </ul>"},{"location":"storage/rbd/#1kvmpool","title":"1.\u51c6\u5907kvm\u4e13\u5c5epool","text":"<pre><code>~]$ ceph osd pool create kvmpool 16 16\npool 'kvmpool' created\n~]$ ceph osd pool application enable  kvmpool rbd\n~]$ rbd pool init -p kvmpool\n</code></pre>"},{"location":"storage/rbd/#2_3","title":"2.\u4e13\u5c5e\u8ba4\u8bc1\u6743\u9650\u8bbe\u5b9a","text":"<pre><code>~]$ ceph auth get-or-create client.kvmuser mon 'allow r' osd 'allow class-read object_prefix rbd_chidren, allow rwx pool=kvmpool'\n[client.kvmuser]\n    key = AQAOl/doomzVKBAAdrBMHIXbar8r4jxHyHDiTg==\n~]$ ceph auth get client.kvmuser -o ceph.client.kvmuser.keyring\nexported keyring for client.kvmuser\n~]$ cat ceph.client.kvmuser.keyring\n[client.kvmuser]\n    key = AQAOl/doomzVKBAAdrBMHIXbar8r4jxHyHDiTg==\n    caps mon = \"allow r\"\n    caps osd = \"allow class-read object_prefix rbd_chidren, allow rwx pool=kvmpool\"\n</code></pre>"},{"location":"storage/rbd/#3kvm","title":"3.\u4f20\u9012\u8ba4\u8bc1\u4fe1\u606f\u5230kvm\u4e3b\u673a","text":"<pre><code>~]$ scp ceph.client.kvmuser.keyring ceph-cluster/{ceph.conf,ceph.client.admin.keyring} root@192.168.1.109:/etc/ceph\n# kvm\u4e3b\u673a\u6267\u884c\n~]# ceph --user kvmuser -s\n  cluster:\n    id:     d901ea94-de1d-4814-9f97-6f7ebd4329dd\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 7h)\n    mgr: ceph-node01(active, since 7h), standbys: ceph-node02\n    mds:  2 up:standby\n    osd: 6 osds: 6 up (since 6h), 6 in (since 6h)\n    rgw: 1 daemon active (ceph-node03)\n\n  task status:\n\n  data:\n    pools:   6 pools, 160 pgs\n    objects: 238 objects, 135 MiB\n    usage:   6.5 GiB used, 234 GiB / 240 GiB avail\n    pgs:     160 active+clean\n</code></pre>"},{"location":"storage/rbd/#4kvmceph","title":"4.kvm\u96c6\u6210ceph","text":"<ul> <li>\u521b\u5efa\u8ba4\u8bc1\u6587\u4ef6</li> </ul> <pre><code>~]# cat &gt; ceph-client-kvmuser-secret.xml &lt;&lt; 'EOF'\n&lt;secret ephemeral='no' private='no'&gt;\n    &lt;usage type='ceph'&gt;\n        &lt;name&gt;client.kvmuser secret&lt;/name&gt;\n    &lt;/usage&gt;\n&lt;/secret&gt;\nEOF\n~]# virsh secret-define --file ceph-client-kvmuser-secret.xml\nSecret 1258b7ac-0b45-487b-8b1f-7cb3a66ab127 created\n#\n~]# virsh secret-set-value --secret 1258b7ac-0b45-487b-8b1f-7cb3a66ab127 --base64 $(ceph auth get-key client.kvmuser)\n~]# virsh secret-list\n UUID                                  Usage\n--------------------------------------------------------------------------------\n 1258b7ac-0b45-487b-8b1f-7cb3a66ab127  ceph client.kvmuser secret\n</code></pre> <ul> <li>\u521b\u5efa\u955c\u50cf</li> </ul> <pre><code>~]# qemu-img create -f rbd rbd:kvmpool/cirrors-image 2G\nFormatting 'rbd:kvmpool/cirrors-image', fmt=rbd size=2147483648 cluster_size=0\n~]# rbd --user kvmuser --pool kvmpool ls -l\nNAME          SIZE  PARENT FMT PROT LOCK\ncirrors-image 2 GiB          2\n</code></pre> <ul> <li>\u5bfc\u5165\u955c\u50cf</li> </ul> <pre><code>~# wget https://ghfast.top/https://github.com/cirros-dev/cirros/releases/download/0.4.0/cirros-0.4.0-x86_64-disk.img\n~]# qemu-img info cirros-0.4.0-x86_64-disk.img\nimage: cirros-0.4.0-x86_64-disk.img\nfile format: qcow2\nvirtual size: 44M (46137344 bytes)\ndisk size: 12M\ncluster_size: 65536\nFormat specific information:\n    compat: 1.1\n    lazy refcounts: false\n~]# qemu-img  convert -f qcow2 -O raw cirros-0.4.0-x86_64-disk.img rbd:kvmpool/cirrors-0.4.0\n~]# rbd --user kvmuser --pool kvmpool ls -l\nNAME          SIZE   PARENT FMT PROT LOCK\ncirrors-0.4.0 44 MiB          2\ncirrors-image  2 GiB          2\n</code></pre> <p>\u5f85\u8865\u5145</p>"},{"location":"storage/rgw/","title":"RADOS Gateway\u5bf9\u8c61\u5b58\u50a8\u7f51\u5173","text":"<p>RADOS Gateway\uff08RGW\uff09 \u662f Ceph \u7684\u4e00\u4e2a HTTP REST \u7f51\u5173\u670d\u52a1\uff0c \u5b83\u7684\u4f5c\u7528\u662f\u8ba9\u5916\u90e8\u5e94\u7528\u901a\u8fc7 S3 / Swift \u534f\u8bae \u8bbf\u95ee Ceph \u540e\u7aef\u7684\u5bf9\u8c61\u5b58\u50a8\u3002\u540c\u65f6RGW\u4e3a\u4e86\u5b9e\u73b0RESTful\u63a5\u53e3\u529f\u80fd\uff0c\u9ed8\u8ba4\u4f7f\u7528Civetweb\u4f5c\u4e3aWebService\uff0c\u800cCivetweb\u9ed8\u8ba4\u4f7f\u7528\u7aef\u53e3\u4e3a7448\u63d0\u4f9b\u670d\u52a1\uff0c\u5982\u679c\u60f3\u4fee\u6539\u7aef\u53e3\uff0c\u53ef\u4ee5\u5728ceph\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9e\u73b0\u3002</p>"},{"location":"storage/rgw/#_1","title":"\u4fee\u6539\u9ed8\u8ba4\u7aef\u53e3","text":"<p>http://192.168.1.103:7480/</p> <pre><code>~]#vim /etc/ceph/ceph.conf\n[client.rgw.ceph-node03]\nrgw_frontends = \"civetweb port=8080\"\n]# systemctl  restart ceph-radosgw@rgw.ceph-node03.service\n~]# ss -tnlp|grep 8080\nLISTEN     0      128          *:8080                     *:*                   users:((\"radosgw\",pid=66470,fd=48)\n</code></pre>"},{"location":"storage/rgw/#ssl","title":"ssl\u901a\u4fe1","text":"<p><pre><code>~]# mkdir /etc/ceph/ssl &amp;&amp; cd /etc/ceph/ssl\nssl]# openssl genrsa -out civetweb.key 2480\nssl]# openssl req -new -x509 -key civetweb.key -out civetweb.crt -days 3650 -subj \"/CN=ceph-node03.devops.io\"\nssl]# cat civetweb.key  civetweb.crt &gt; civetweb.pem\n</code></pre> <pre><code>~]#vim /etc/ceph/ceph.conf\n[client.rgw.ceph-node03]\nrgw_frontends = \"civetweb port=8080+8443s ssl_certificate=/etc/ceph/ssl/civetweb.pem num_threads=2000\"\n~]# systemctl  restart ceph-radosgw@rgw.ceph-node03.service\n~]# ss -tnlp|grep radosgw\nLISTEN     0      128          *:8080                     *:*                   users:((\"radosgw\",pid=67094,fd=47))\nLISTEN     0      128          *:8443                     *:*                   users:((\"radosgw\",pid=67094,fd=48))\n</code></pre></p>"},{"location":"storage/rgw/#_2","title":"\u6cdb\u57df\u540d\u5b9e\u8df5","text":"<p>radosgw \u7684S3 API \u63a5\u53e3\u529f\u80fd\u5f3a\u4f9d\u8d56DNS\u7684\u6cdb\u57df\u540d\u89e3\u6790\u670d\u52a1\u3002\u5b83\u78a7\u73ba\u80fd\u6b63\u5e38\u89e3\u6790\u4efb\u4f55<code>&lt;bucket-name&gt;/&lt;radosgw-host&gt;</code>\u683c\u5f0f\u7684\u540d\u79f0\u81f3radosgw\u4e3b\u673a\uff0c\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u914d\u7f6e\u6bcf\u4e2aradosgw\u5b88\u62a4\u8fdb\u7a0b\u7684<code>rgw_dns_name</code>\u4e3a\u5176DNS\u540d\u79f0</p>"},{"location":"storage/rgw/#1-dns","title":"1. dns\u73af\u5883\u914d\u7f6e","text":"<pre><code>~]# yum install bind bind-chroot bind-utils bind-libs -y\n~]# vim /etc/named.conf\noptions {\n        listen-on port 53 { any; };\n        allow-query     { any; };\n        ...\n        /*\n        dnssec-enable yes;\n        dnssec-validation yes;\n        */\n};\n...\n// \u5b9a\u5236\u57df\u540d\u7684zone\u914d\u7f6e\nzone \"devops.io\" IN {\n    type master;\n    file \"/var/named/devops.io.zone\";\n    allow-update { none; };\n};\n\n\ncat  &gt; /var/named/devops.io.zone &lt;&lt; 'EOF'\n$TTL 1H\n@       IN  SOA  ns1.devops.io. admin.devops.io. (\n                2025102101 ; \u5e8f\u5217\u53f7\n                1H         ; \u5237\u65b0\n                10M        ; \u91cd\u8bd5\n                1W         ; \u8fc7\u671f\n                1H )       ; \u6700\u5c0fTTL\n\n; DNS\u670d\u52a1\u5668\u81ea\u8eab\n        IN  NS   ns1.devops.io.\nns1     IN  A    192.168.1.100\n\n; \u4e3b\u57df\u540d\uff08devops.io\uff09\n@                      IN  A    192.168.1.100     ; \u53ef\u9009\uff0c\u4e5f\u53ef\u4ee5\u662f\u7f51\u7ad9IP\n\n; ceph-node03 \u4e3b\u673a\nnode3           IN  A    192.168.1.103\n\n; \u6cdb\u57df\u540d *.ceph-node03.devops.io\n*.node3           IN  A    192.168.1.103\nEOF\nsystemctl start named\ndig -t A devops.io @192.168.1.100\ndig -t A node3 .devops.io @192.168.1.100\ndig -t A img.node3 .devops.io @192.168.1.100\ndig -t A file.node3.devops.io @192.168.1.100\n</code></pre>"},{"location":"storage/rgw/#2","title":"2.\u521b\u5efa\u4e13\u5c5e\u7684\u7528\u6237\u540d","text":"<pre><code>~]# radosgw-admin user create --uid='s3user' --display-name 'S3 Testing User'\n{\n    \"user_id\": \"s3user\",\n    \"display_name\": \"S3 Testing User\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"subusers\": [],\n    \"keys\": [\n        {\n            \"user\": \"s3user\",\n            \"access_key\": \"0WGYN1LLSQ5QLU7KA581\",\n            \"secret_key\": \"vQQGkynYDEu4Nc4LQGN6SWBMo2cvBuTH6brH7Jpc\"\n        }\n    ],\n    \"swift_keys\": [],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"default_storage_class\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\",\n    \"mfa_ids\": []\n}\n</code></pre>"},{"location":"storage/rgw/#3","title":"3.\u5b89\u88c5\u4e13\u5c5e\u7684\u5ba2\u6237\u7aef\u547d\u4ee4","text":"<pre><code>~]# yum install s3cmd\n</code></pre>"},{"location":"storage/rgw/#4","title":"4.\u914d\u7f6e\u4e13\u5c5e\u7684\u8ba4\u8bc1\u914d\u7f6e\u6587\u4ef6","text":"<pre><code>~]#  s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: 0WGYN1LLSQ5QLU7KA581\nSecret Key: vQQGkynYDEu4Nc4LQGN6SWBMo2cvBuTH6brH7Jpc\nDefault Region [US]:\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: node3.devops.io:8080\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used\nif the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.node3.devops.io:8080\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program [/bin/gpg]:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: No\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: 0WGYN1LLSQ5QLU7KA581\n  Secret Key: vQQGkynYDEu4Nc4LQGN6SWBMo2cvBuTH6brH7Jpc\n  Default Region: US\n  S3 Endpoint: node3.devops.io:8080\n  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.node3.devops.io:8080\n  Encryption password:\n  Path to GPG program: /bin/gpg\n  Use HTTPS protocol: False\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n\nTest access with supplied credentials? [Y/n] y\nPlease wait, attempting to list all buckets...\nSuccess. Your access key and secret key worked fine :-)\n\nNow verifying that encryption works...\nNot configured. Never mind.\n\nSave settings? [y/N] y\nConfiguration saved to '/root/.s3cfg'\n</code></pre>"},{"location":"storage/rgw/#5s3","title":"5.\u7efc\u5408\u6d4b\u8bd5s3\u7684\u8d44\u6e90\u5bf9\u8c61\u7ba1\u7406","text":""},{"location":"storage/rgw/#51","title":"5.1.\u4e0a\u4f20\u6587\u4ef6","text":"<p><pre><code># \u9700\u8981\u914d\u7f6e/etc/resolve.conf \u4f7f\u7528\u81ea\u5efa\u7684dns\n~]# nslookup file.node3.devops.io\nServer:     192.168.1.100\nAddress:    192.168.1.100#53\n\nName:   file.ceph-node03.devops.io\nAddress: 192.168.1.103\n</code></pre> <pre><code>~]# ceph config set client.rgw.ceph-node03 rgw_dns_name node3.devops.io\n~]# s3cmd mb s3://images\nBucket 's3://images/' created\n~]# s3cmd mb s3://images1\nBucket 's3://images1/' created\n[root@ceph-node03 ~]# s3cmd mb s3://images2\nBucket 's3://images2/' created\n</code></pre> <pre><code># \u4e0a\u4f20\uff0c\u4e0a\u4f20\u76ee\u5f55\u8bf7\u6dfb\u52a0\u53c2\u6570 --recursive\n~]# s3cmd put /etc/fstab s3://images1/linux/myfs\nupload: '/etc/fstab' -&gt; 's3://images1/linux/myfs'  [1 of 1]\n 465 of 465   100% in    0s    24.11 KB/s  done\n# \u5217\u51fa\n[root@ceph-node03 ~]# s3cmd ls  s3://images1/\n                          DIR  s3://images1/linux/\n[root@ceph-node03 ~]# s3cmd ls  s3://images1/linux\n                          DIR  s3://images1/linux/\n[root@ceph-node03 ~]# s3cmd ls  s3://images1/linux/myfs\n2025-10-21 17:21          465  s3://images1/linux/myfs\n</code></pre></p>"},{"location":"storage/rgw/#52","title":"5.2.\u4e0b\u8f7d\u6587\u4ef6","text":"<pre><code>~]# s3cmd get  s3://images1/linux/myfs /tmp/myfs\ndownload: 's3://images1/linux/myfs' -&gt; '/tmp/myfs'  [1 of 1]\n 465 of 465   100% in    0s    10.69 KB/s  done\n[root@ceph-node03 ~]# ls /tmp/myfs\n/tmp/myfs\n</code></pre>"},{"location":"storage/rgw/#53","title":"5.3.\u8bbf\u95ee\u6587\u4ef6","text":"<pre><code>~]# s3cmd ls s3://images\n~]# s3cmd  put /etc/fstab s3://images/linux/fstab.txt\nupload: '/etc/fstab' -&gt; 's3://images/linux/fstab.txt'  [1 of 1]\n 465 of 465   100% in    0s    18.92 KB/s  done\n~]# s3cmd  ls s3://images/linux/\n2025-10-22 01:31          465  s3://images/linux/fstab.txt\n~]# s3cmd  ls s3://images/linux\n                          DIR  s3://images/linux/\n# \u6388\u6743 https://docs.ceph.com/en/quincy/radosgw/bucketpolicy/\n ~]# cat &gt; policy.json &lt;&lt; 'EOF'\n{\n    \"Statement\": [{\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\"s3:GetObject\"],\n            \"Resource\": \"*\"\n    }]\n}\nEOF\n~]# s3cmd setpolicy policy.json s3://images --acl-public\ns3://images/: Policy updated\n# \u5b9a\u5236cors\u7684\u7b56\u7565\u6587\u4ef6\ncat &gt; rules.xml &lt;&lt; 'EOF'\n&lt;CORSConfiguration&gt;\n  &lt;CORSRule&gt;\n    &lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt;\n    &lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;\n    &lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;\n    &lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;\n    &lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;\n    &lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;\n    &lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;\n    &lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt;\n  &lt;/CORSRule&gt;\n&lt;/CORSConfiguration&gt;\nEOF\n~]# s3cmd setcors rules.xml s3://images\n[root@ceph-node03 ~]# s3cmd info s3://images\ns3://images/ (bucket):\n   Location:  default\n   Payer:     BucketOwner\n   Expiration Rule: none\n   Policy:    {\n    \"Statement\": [{\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": [\"s3:GetObject\"],\n            \"Resource\": \"*\"\n    }]\n}\n\n   CORS:      &lt;CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;CORSRule&gt;&lt;AllowedMethod&gt;GET&lt;/AllowedMethod&gt;&lt;AllowedMethod&gt;PUT&lt;/AllowedMethod&gt;&lt;AllowedMethod&gt;DELETE&lt;/AllowedMethod&gt;&lt;AllowedMethod&gt;POST&lt;/AllowedMethod&gt;&lt;AllowedOrigin&gt;*&lt;/AllowedOrigin&gt;&lt;AllowedHeader&gt;*&lt;/AllowedHeader&gt;&lt;MaxAgeSeconds&gt;3000&lt;/MaxAgeSeconds&gt;&lt;ExposeHeader&gt;ETag&lt;/ExposeHeader&gt;&lt;/CORSRule&gt;&lt;/CORSConfiguration&gt;\n   ACL:       S3 Testing User: FULL_CONTROL\n#\ncurl https://images.node3.devops.io:8080/linux/fstab.txt\n\n#\n# /etc/fstab\n# Created by anaconda on Sun Oct 19 10:05:37 2025\n#\n# Accessible filesystems, by reference, are maintained under '/dev/disk'\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/centos-root /                       xfs     defaults        0 0\nUUID=21bb7911-72e6-4467-ac66-09f75857bc92 /boot                   xfs     defaults        0 0\n/dev/mapper/centos-swap swap                    swap    defaults        0 0\n</code></pre>"},{"location":"storage/rgw/#_3","title":"\u8fdb\u9636","text":""},{"location":"storage/rgw/#1rgw","title":"1.rgw\u5b58\u50a8\u6c60","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b RGW\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u4f1a\u9ed8\u8ba4\u751f\u6210\u4ee5\u4e0b\u5b58\u50a8\u6c60 <pre><code>~]$ ceph osd  pool ls\n# \u81ea\u52a8\u751f\u6210\u7684\u5b58\u50a8\u6c60\n.rgw.root                         # \u5305\u542bzone\u3002zonegroup\uff0crealm\u7b49\u4fe1\u606f\ndefault.rgw.control               # pool \u4e2d\u5bf9\u8c61\u7684\u63a7\u5236\u4fe1\u606f\ndefault.rgw.meta                 # \u5143\u6570\u636e\u4fe1\u606f\ndefault.rgw.log                  # \u65e5\u5fd7\u5904\u7406\u4fe1\u606f\ndefault.rgw.buckets.index        # \u5b58\u50a8\u6876\u548c\u5bf9\u8c61\u7684\u6620\u5c04\u5173\u7cfb\ndefault.rgw.buckets.data         # \u5bf9\u8c61\u6570\u636e\u4fe1\u606f\n</code></pre></p>"},{"location":"storage/rgw/#2_1","title":"2.\u5143\u6570\u636e","text":"<p><pre><code>~]$ radosgw-admin metadata list\n[\n    \"bucket\",\n    \"bucket.instance\",\n    \"otp\",\n    \"user\"\n]\n~]$ radosgw-admin metadata list user\n[\n    \"s3user\"\n]\n~]$ radosgw-admin metadata get user:s3user\n{\n    \"key\": \"user:s3user\",\n    \"ver\": {\n        \"tag\": \"_OP5oLiEF_ZURFa71qgl9xJ5\",\n        \"ver\": 1\n    },\n    \"mtime\": \"2025-10-21 16:50:44.004910Z\",\n    \"data\": {\n        \"user_id\": \"s3user\",\n        \"display_name\": \"S3 Testing User\",\n        \"email\": \"\",\n        \"suspended\": 0,\n        \"max_buckets\": 1000,\n        \"subusers\": [],\n        \"keys\": [\n            {\n                \"user\": \"s3user\",\n                \"access_key\": \"0WGYN1LLSQ5QLU7KA581\",\n                \"secret_key\": \"vQQGkynYDEu4Nc4LQGN6SWBMo2cvBuTH6brH7Jpc\"\n            }\n        ],\n        \"swift_keys\": [],\n        \"caps\": [],\n        \"op_mask\": \"read, write, delete\",\n        \"default_placement\": \"\",\n        \"default_storage_class\": \"\",\n        \"placement_tags\": [],\n        \"bucket_quota\": {\n            \"enabled\": false,\n            \"check_on_raw\": false,\n            \"max_size\": -1,\n            \"max_size_kb\": 0,\n            \"max_objects\": -1\n        },\n        \"user_quota\": {\n            \"enabled\": false,\n            \"check_on_raw\": false,\n            \"max_size\": -1,\n            \"max_size_kb\": 0,\n            \"max_objects\": -1\n        },\n        \"temp_url_keys\": [],\n        \"type\": \"rgw\",\n        \"mfa_ids\": [],\n        \"attrs\": []\n    }\n}\n~]$ radosgw-admin metadata list bucket\n[\n    \"images\",\n    \"images1\",\n    \"images2\",\n    \"imagess\"\n]\n</code></pre> <pre><code>~]$ rados -p default.rgw.meta -N users.uid ls\ns3user\ns3user.buckets\n~]$ rados -p default.rgw.meta -N users.uid listomapkeys  s3user.buckets\nimages\nimages1\nimages2\nimagess\n~]$ rados -p default.rgw.meta -N users.uid getomapval  s3user.buckets   images image_bucket\nWriting to image_bucket\n~]$ ceph-dencoder import image_bucket type cls_user_bucket_entry decode dump_json\n{\n    \"bucket\": {\n        \"name\": \"images\",\n        \"marker\": \"341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1\",\n        \"bucket_id\": \"341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1\"\n    },\n    \"size\": 465,\n    \"size_rounded\": 4096,\n    \"creation_time\": \"2025-10-21 17:14:36.068004Z\",\n    \"count\": 1,\n    \"user_stats_sync\": \"true\"\n}\n~]$ rados -p default.rgw.buckets.index ls\n.dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.4\n.dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1\n.dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3\n.dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.5\n[cephadm@ceph-admin ~]$ rados -p default.rgw.buckets.index listomapkeys .dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.4\n[cephadm@ceph-admin ~]$ rados -p default.rgw.buckets.index listomapkeys .dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.5\n[cephadm@ceph-admin ~]$ rados -p default.rgw.buckets.index listomapkeys .dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1\nlinux/fstab.txt\n~]$ rados -p default.rgw.buckets.index getomapval .dir.341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1 linux/fstab.txt object_key\n~]$ ceph-dencoder type rgw_bucket_dir_entry import object_key decode dump_json\n{\n    \"name\": \"linux/fstab.txt\",\n    \"instance\": \"\",\n    \"ver\": {\n        \"pool\": 10,\n        \"epoch\": 4\n    },\n    \"locator\": \"\",\n    \"exists\": \"true\",\n    \"meta\": {\n        \"category\": 1,\n        \"size\": 465,\n        \"mtime\": \"2025-10-22 01:31:57.034413Z\",\n        \"etag\": \"6e2d4f8e7d8bd64d7de48e79b941c2f3\",\n        \"storage_class\": \"STANDARD\",\n        \"owner\": \"s3user\",\n        \"owner_display_name\": \"S3 Testing User\",\n        \"content_type\": \"text/plain\",\n        \"accounted_size\": 465,\n        \"user_data\": \"\",\n        \"appendable\": \"false\"\n    },\n    \"tag\": \"341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.16028.73\",\n    \"flags\": 0,\n    \"pending_map\": [],\n    \"versioned_epoch\": 0\n}\n</code></pre> <pre><code>~]$ rados -p default.rgw.buckets.data ls\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.cshrc\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.tcshrc\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.rnd\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.bash_history\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.bash_profile\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_linux/myfs\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.viminfo\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.s3cfg\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1_linux/fstab.txt\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.bash_logout\n341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.3_root/.bashrc\n~]$ rados -p default.rgw.buckets.data listxattr 341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1_linux/fstab.txt\nuser.rgw.acl\nuser.rgw.content_type\nuser.rgw.etag\nuser.rgw.idtag\nuser.rgw.manifest\nuser.rgw.pg_ver\nuser.rgw.source_zone\nuser.rgw.storage_class\nuser.rgw.tail_tag\nuser.rgw.x-amz-content-sha256\nuser.rgw.x-amz-date\nuser.rgw.x-amz-meta-s3cmd-attrs\n~]$ rados -p default.rgw.buckets.data getxattr 341ed5a0-d53a-46ef-b9d0-eeb64b8bf21b.5970.1_linux/fstab.txt  user.rgw.manifest &gt; fstab.txt\n~]$ ceph-dencoder type RGWObjManifest import fstab.txt decode dump_json\n</code></pre></p>"}]}